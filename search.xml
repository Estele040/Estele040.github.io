<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>一个内存优化的小tips</title>
      <link href="/2024/03/29/%E4%B8%80%E4%B8%AA%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96%E7%9A%84%E5%B0%8Ftips/"/>
      <url>/2024/03/29/%E4%B8%80%E4%B8%AA%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96%E7%9A%84%E5%B0%8Ftips/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">#de节省内存的函数</span></span><br><span class="line"><span class="comment"># 节约内存的一个标配函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reduce_mem</span>(<span class="params">df</span>):</span><br><span class="line">    starttime = time.time()</span><br><span class="line">    numerics = [<span class="string">&#x27;int16&#x27;</span>, <span class="string">&#x27;int32&#x27;</span>, <span class="string">&#x27;int64&#x27;</span>, <span class="string">&#x27;float16&#x27;</span>, <span class="string">&#x27;float32&#x27;</span>, <span class="string">&#x27;float64&#x27;</span>]</span><br><span class="line">    start_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span>**<span class="number">2</span>   <span class="comment">#返回每列的内存使用情况，并将其相加，以MB为单位</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:      <span class="comment">#遍历df中每一列</span></span><br><span class="line">        col_type = df[col].dtypes   <span class="comment">#找出每列的数据类型</span></span><br><span class="line">        <span class="comment"># 如果该列的数据类型存在于numerics中，</span></span><br><span class="line">        <span class="keyword">if</span> col_type <span class="keyword">in</span> numerics:</span><br><span class="line">            c_min = df[col].<span class="built_in">min</span>()</span><br><span class="line">            c_max = df[col].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">if</span> pd.isnull(c_min) <span class="keyword">or</span> pd.isnull(c_max): <span class="comment">#若c_min或者c_max是空，跳出本次for循环;否则，这一部分if代码没啥用</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(col_type)[:<span class="number">3</span>] == <span class="string">&#x27;int&#x27;</span>: <span class="comment">#用来节约内存</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment">#</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float64)</span><br><span class="line"></span><br><span class="line">    end_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span>**<span class="number">2</span> <span class="comment"># 计算最终的内存占有</span></span><br><span class="line">        <span class="comment"># 打印最终使用的内存、降低的内存、程序运行时间</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-- Mem. usage decreased to &#123;:5.2f&#125; Mb (&#123;:.1f&#125;% reduction),time spend:&#123;:2.2f&#125; min&#x27;</span>.<span class="built_in">format</span>(end_mem,</span><br><span class="line">          <span class="number">100</span>*(start_mem-end_mem)/start_mem,</span><br><span class="line">          (time.time()-starttime)/<span class="number">60</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> df <span class="comment">#返回df，df指的是原本传入的参数</span></span><br></pre></td></tr></table></figure><p><strong>理解：</strong></p><p>限定两个数据类型：int &amp; float，其中，int有int8、int16、int32、int64这几种，float有float16、float32、float64这几种。判断每列数据的最大和最小数据类型，求出后将该列数据的数据类型限制为最小数据类型。最后打印数据经过优化后最终使用的内存、内存优化程序优化的内存、程序的运行时间。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>GRU</title>
      <link href="/2024/03/28/GRU/"/>
      <url>/2024/03/28/GRU/</url>
      
        <content type="html"><![CDATA[<h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><h3 id="1-什么是GRU"><a href="#1-什么是GRU" class="headerlink" title="1. 什么是GRU"></a>1. 什么是GRU</h3><p>GRU是循环神经网络的一种，为解决长期记忆和反向传播中的梯度等问题而提出的。</p><p>GRU和LSTM在很多情况下实际表现上相差无几，用GRU的原因在于：</p><blockquote><p>贫穷限制了我们的计算，so GRU is all your need!</p><p>(GRU)更容易进行训练。</p></blockquote><h3 id="2-GRU组件讲解"><a href="#2-GRU组件讲解" class="headerlink" title="2. GRU组件讲解"></a>2. GRU组件讲解</h3><h4 id="2-1-GRU的输入输出的结构"><a href="#2-1-GRU的输入输出的结构" class="headerlink" title="2.1 GRU的输入输出的结构"></a>2.1 GRU的输入输出的结构</h4><p>GRU的输入输出结构与普通的RNN是一样的。有一个当前输入$x^{t}$，和上一个节点传递下来的隐状态$h^{t-1}$，这个隐状态包含了之前节点的相关信息。</p><p>而输出$y^{t}$是由$x^{t}$和$h^{t-1}$共同决定的，同时$h^{t-1}$的信息会通过GRU传递给下一个节点隐藏状态$h^{t}$。</p><p><img src="/home/xxfs/.config/Typora/typora-user-images/image-20240327212521107.png" alt="image-20240327212521107"></p><h4 id="2-2-GRU的内部结构"><a href="#2-2-GRU的内部结构" class="headerlink" title="2.2 GRU的内部结构"></a>2.2 GRU的内部结构</h4><p>首先，我们先通过上一个传输下来的$h^{t-1}$和当前节点的$x^{t}$来获取两个门控状态。下图中，r为控制重置的门控（reset gate），z为控制更新的门控（upgrade gate）。</p><p><img src="/home/xxfs/.config/Typora/typora-user-images/image-20240327213115939.png" alt="image-20240327213115939"></p><p>得到门控信号之后，首先用重置门控来得到“<strong>重置</strong>”之后的数据$h^{t-1’}·r$，再将$h^{t-1’}$与输入$x^{t}$进行拼接，再通过一个tanh激活函数来将数据放缩到$-1 ～ 1$的范围之内。即得到如下图2-3所示的$h’$。</p><p><img src="/home/xxfs/.config/Typora/typora-user-images/image-20240327213416114.png" alt="image-20240327213416114"></p><p>这里的$h’$主要是包含了当前输入的$x’$数据。有针对性地对$h’$添加到当前的隐藏状态，相当于“记忆了当前时刻的状态”。类似于LSTM的选择记忆阶段。</p><p><img src="/home/xxfs/.config/Typora/typora-user-images/image-20240327214217417.png" alt="image-20240327214217417"></p><blockquote><p>图2-4中的 ⊙ 是Hadamard Product，也就是操作矩阵中对应的元素相乘，因此要求两个相乘矩阵是同型的。 ⊕ 则代表进行矩阵加法操作。</p></blockquote><p>最后一个RUG最关键的一个步骤，我们可以称之为“<strong>更新记忆</strong>”阶段。</p><p>在这个阶段，我们同时进行了遗忘和记忆两个步骤。我们使用了先前得到的门控$z$（upgrade gate）。</p><p>更新表达式子：$h^{t} &#x3D; (1-z)·h^{t-1} + z·h’$​。</p><p>首先再次强调一下，门控（这里的z）的范围0～1。门控信号越接近1，代表“记忆”下来的数据多；而越接近0则代表“遗忘”的越多。</p><p>我们使用了同一个门控可以进行遗忘和记忆。（LSTM则要使用多个门控）。</p><ul><li>$(1-z)·h^{t-1}$：表示对原本隐藏状态的选择性“遗忘”。这里的1-z可以想象成遗忘门（forget gate），忘记$h^{t-1}$维度中一些不重要的信息。</li><li>$z·h’$：表示对包含当前节点信息的$h’$进行“选择性”记忆。与上面类似，这里的（1-z）可以想象成遗忘门（forget gate），忘记$h^{t-1}$​维度中一些不重要的信息。</li><li>$h^{t} &#x3D; (1-z)·h^{t-1} + z·h’$：结合上述，这一步的操作就是忘记传递下来的$h^{t-1}$中的某些维度信息，并加入当前节点输入的某些维度信息。</li></ul><blockquote><p>可以看到，这里的遗忘$z$和选择$(1-z)$是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重$(z)$，我们就会包含当前输入的$h’$中所对应的权重进行弥补$(1-z)$。以保持一种”恒定“状态。</p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>2024MCM（C题）部分思路</title>
      <link href="/2024/02/07/MCM2024%E9%83%A8%E5%88%86%E6%80%9D%E8%B7%AF%EF%BC%88C%E9%A2%98%EF%BC%89/"/>
      <url>/2024/02/07/MCM2024%E9%83%A8%E5%88%86%E6%80%9D%E8%B7%AF%EF%BC%88C%E9%A2%98%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>近期更新，C题的部分思路如下：<br>针对势头的预测，有以下几个方法：</p><ol><li>使用T-sne降维 + k-means聚类。期间也尝试使用PCA进行降维，但发现效果不是很好。</li><li>在特征工程后，使用随机森林进行预测。btw，刚开始效果不是很好，acc只有28.5%，后尝试提取较重要的指标后再次进行实验。</li></ol><p>晚点更新……睡觉去……</p>]]></content>
      
      
      <categories>
          
          <category> Compititions </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MCM </tag>
            
            <tag> Study </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
