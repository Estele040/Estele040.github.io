{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"source/images/2023-09-12 23-40-39 的屏幕截图.png","path":"images/2023-09-12 23-40-39 的屏幕截图.png","modified":1,"renderable":0},{"_id":"source/images/2023-09-15 21-17-12 的屏幕截图.png","path":"images/2023-09-15 21-17-12 的屏幕截图.png","modified":1,"renderable":0},{"_id":"source/images/2023-09-15 21-39-05 的屏幕截图.png","path":"images/2023-09-15 21-39-05 的屏幕截图.png","modified":1,"renderable":0},{"_id":"source/images/avatar.png","path":"images/avatar.png","modified":1,"renderable":0},{"_id":"source/uploads/avatar.gif","path":"uploads/avatar.gif","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/background.png","path":"images/background.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/local-search.js","path":"js/local-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/anime.min.js","path":"lib/anime.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","path":"lib/font-awesome/css/all.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","path":"lib/font-awesome/webfonts/fa-brands-400.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","path":"lib/font-awesome/webfonts/fa-regular-400.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","path":"lib/font-awesome/webfonts/fa-solid-900.woff2","modified":1,"renderable":1}],"Cache":[{"_id":"source/_posts/C++Primer.md","hash":"9139c0534b539efb75019fdd4bb4f214045a717a","modified":1693580104393},{"_id":"source/_posts/DeepLearning.md","hash":"3b17c30a8c05c2d2c5cca10b9c9bb23a3f0da4d1","modified":1694964272033},{"_id":"source/_posts/SummerRecord2023.md","hash":"ec56e1b1fc2f5601ee78189ca3c463ae9e6e36f9","modified":1692616035088},{"_id":"source/_posts/python.md","hash":"744d781521e884fb088154b1e14cc5054bb83321","modified":1691395602391},{"_id":"source/_posts/softmax回归.md","hash":"f86a8546ce461764324eaa3d5e4102a469883fb7","modified":1694761999298},{"_id":"source/about/index.md","hash":"7beb9a6f5c596f6d06a452e44f095c3e0463cf55","modified":1691302804195},{"_id":"source/categories/index.md","hash":"b3920485fda8cdd562ab47aa5daa4ceb62f48277","modified":1691301911060},{"_id":"source/images/2023-09-15 21-39-05 的屏幕截图.png","hash":"422c7ba102b37cd3a147e3c4c0fab592aaa523d6","modified":1694845814369},{"_id":"source/images/avatar.png","hash":"74ce5da46d280a5d6041ee3f58be6a6783a26206","modified":1694845814369},{"_id":"source/links/index.md","hash":"1da067d5ac9236da556eb9aa424fdbb69fb9e3e3","modified":1691946234539},{"_id":"source/tags/index.md","hash":"222e14e9b69acda589f106b831235d9f202d7b95","modified":1691301939240},{"_id":"source/uploads/avatar.gif","hash":"179f17063869a91bd6af1c4dd0ee7d1b24cde2fa","modified":1691339426370},{"_id":"source/uploads/index.md","hash":"5e7af999abf59308384887cbf0035df2571ad2a2","modified":1691339353237},{"_id":"source/images/2023-09-12 23-40-39 的屏幕截图.png","hash":"1a0295c9de22951bd8923d77d83361a583f2c5dd","modified":1694845814369},{"_id":"source/images/2023-09-15 21-17-12 的屏幕截图.png","hash":"8d0d408246ba4b5b621c79241861eff57c160899","modified":1694845814369},{"_id":"themes/next/.editorconfig","hash":"8570735a8d8d034a3a175afd1dd40b39140b3e6a","modified":1691294135131},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1691294135131},{"_id":"themes/next/.gitattributes","hash":"a54f902957d49356376b59287b894b1a3d7a003f","modified":1691294135131},{"_id":"themes/next/.stylintrc","hash":"2cf4d637b56d8eb423f59656a11f6403aa90f550","modified":1691294135131},{"_id":"themes/next/.gitignore","hash":"56f3470755c20311ddd30d421b377697a6e5e68b","modified":1691294135131},{"_id":"themes/next/.travis.yml","hash":"ecca3b919a5b15886e3eca58aa84aafc395590da","modified":1691294135131},{"_id":"themes/next/LICENSE.md","hash":"18144d8ed58c75af66cb419d54f3f63374cd5c5b","modified":1691294135131},{"_id":"themes/next/README.md","hash":"9b4b7d66aca47f9c65d6321b14eef48d95c4dff1","modified":1691294135131},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1691294135135},{"_id":"themes/next/gulpfile.js","hash":"1b4fc262b89948937b9e3794de812a7c1f2f3592","modified":1691294135135},{"_id":"themes/next/_config.yml","hash":"437ccd42c615a4b1cd2b910511de7c56b556d143","modified":1692115095816},{"_id":"themes/next/package.json","hash":"f7147252461c8a72b685639697f2f0d7c7a0b3f4","modified":1691336609962},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"aa4cb7aff595ca628cb58160ee1eee117989ec4e","modified":1691294135131},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"1a435c20ae8fa183d49bbf96ac956f7c6c25c8af","modified":1691294135131},{"_id":"themes/next/.github/config.yml","hash":"1d3f4e8794986817c0fead095c74f756d45f91ed","modified":1691294135131},{"_id":"themes/next/.github/issue-close-app.yml","hash":"7cba457eec47dbfcfd4086acd1c69eaafca2f0cd","modified":1691294135131},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"fca600ddef6f80c5e61aeed21722d191e5606e5b","modified":1691294135131},{"_id":"themes/next/.github/lock.yml","hash":"61173b9522ebac13db2c544e138808295624f7fd","modified":1691294135131},{"_id":"themes/next/.github/mergeable.yml","hash":"0ee56e23bbc71e1e76427d2bd255a9879bd36e22","modified":1691294135131},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"e554931b98f251fd49ff1d2443006d9ea2c20461","modified":1691294135131},{"_id":"themes/next/.github/release-drafter.yml","hash":"3cc10ce75ecc03a5ce86b00363e2a17eb65d15ea","modified":1691294135131},{"_id":"themes/next/.github/support.yml","hash":"d75db6ffa7b4ca3b865a925f9de9aef3fc51925c","modified":1691294135131},{"_id":"themes/next/.github/stale.yml","hash":"fdf82de9284f8bc8e0b0712b4cc1cb081a94de59","modified":1691294135131},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"c7a994b9542040317d8f99affa1405c143a94a38","modified":1691294135135},{"_id":"themes/next/docs/DATA-FILES.md","hash":"cddbdc91ee9e65c37a50bec12194f93d36161616","modified":1691294135135},{"_id":"themes/next/docs/INSTALLATION.md","hash":"af88bcce035780aaa061261ed9d0d6c697678618","modified":1691294135135},{"_id":"themes/next/docs/AUTHORS.md","hash":"10135a2f78ac40e9f46b3add3e360c025400752f","modified":1691294135135},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"94dc3404ccb0e5f663af2aa883c1af1d6eae553d","modified":1691294135135},{"_id":"themes/next/docs/LICENSE.txt","hash":"368bf2c29d70f27d8726dd914f1b3211cae4bbab","modified":1691294135135},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"8b6e4b2c9cfcb969833092bdeaed78534082e3e6","modified":1691294135135},{"_id":"themes/next/docs/MATH.md","hash":"d645b025ec7fb9fbf799b9bb76af33b9f5b9ed93","modified":1691294135135},{"_id":"themes/next/languages/ar.yml","hash":"9815e84e53d750c8bcbd9193c2d44d8d910e3444","modified":1691294135135},{"_id":"themes/next/languages/de.yml","hash":"74c59f2744217003b717b59d96e275b54635abf5","modified":1691294135135},{"_id":"themes/next/languages/en.yml","hash":"45bc5118828bdc72dcaa25282cd367c8622758cb","modified":1691294135135},{"_id":"themes/next/languages/default.yml","hash":"45bc5118828bdc72dcaa25282cd367c8622758cb","modified":1691294135135},{"_id":"themes/next/languages/es.yml","hash":"c64cf05f356096f1464b4b1439da3c6c9b941062","modified":1691294135135},{"_id":"themes/next/languages/fa.yml","hash":"3676b32fda37e122f3c1a655085a1868fb6ad66b","modified":1691294135135},{"_id":"themes/next/languages/fr.yml","hash":"752bf309f46a2cd43890b82300b342d7218d625f","modified":1691294135135},{"_id":"themes/next/languages/hu.yml","hash":"b1ebb77a5fd101195b79f94de293bcf9001d996f","modified":1691294135135},{"_id":"themes/next/languages/id.yml","hash":"572ed855d47aafe26f58c73b1394530754881ec2","modified":1691294135135},{"_id":"themes/next/languages/it.yml","hash":"44759f779ce9c260b895532de1d209ad4bd144bf","modified":1691294135135},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1691294135135},{"_id":"themes/next/languages/ja.yml","hash":"0cf0baa663d530f22ff380a051881216d6adcdd8","modified":1691294135135},{"_id":"themes/next/languages/ko.yml","hash":"0feea9e43cd399f3610b94d755a39fff1d371e97","modified":1691294135135},{"_id":"themes/next/languages/pt-BR.yml","hash":"67555b1ba31a0242b12fc6ce3add28531160e35b","modified":1691294135135},{"_id":"themes/next/languages/nl.yml","hash":"5af3473d9f22897204afabc08bb984b247493330","modified":1691294135135},{"_id":"themes/next/languages/pt.yml","hash":"718d131f42f214842337776e1eaddd1e9a584054","modified":1691294135135},{"_id":"themes/next/languages/ru.yml","hash":"e993d5ca072f7f6887e30fc0c19b4da791ca7a88","modified":1691294135135},{"_id":"themes/next/languages/tr.yml","hash":"2b041eeb8bd096f549464f191cfc1ea0181daca4","modified":1691294135135},{"_id":"themes/next/languages/vi.yml","hash":"93393b01df148dcbf0863f6eee8e404e2d94ef9e","modified":1691294135135},{"_id":"themes/next/languages/uk.yml","hash":"3a6d635b1035423b22fc86d9455dba9003724de9","modified":1691294135135},{"_id":"themes/next/languages/zh-HK.yml","hash":"3789f94010f948e9f23e21235ef422a191753c65","modified":1691294135135},{"_id":"themes/next/languages/zh-CN.yml","hash":"a1f15571ee7e1e84e3cc0985c3ec4ba1a113f6f8","modified":1691294135135},{"_id":"themes/next/languages/zh-TW.yml","hash":"8c09da7c4ec3fca2c6ee897b2eea260596a2baa1","modified":1691294135135},{"_id":"themes/next/layout/_layout.swig","hash":"6a6e92a4664cdb981890a27ac11fd057f44de1d5","modified":1691294135135},{"_id":"themes/next/layout/category.swig","hash":"1bde61cf4d2d171647311a0ac2c5c7933f6a53b0","modified":1691294135139},{"_id":"themes/next/layout/archive.swig","hash":"e4e31317a8df68f23156cfc49e9b1aa9a12ad2ed","modified":1691294135139},{"_id":"themes/next/layout/index.swig","hash":"7f403a18a68e6d662ae3e154b2c1d3bbe0801a23","modified":1691294135139},{"_id":"themes/next/layout/post.swig","hash":"2f6d992ced7e067521fdce05ffe4fd75481f41c5","modified":1691294135139},{"_id":"themes/next/layout/page.swig","hash":"db581bdeac5c75fabb0f17d7c5e746e47f2a9168","modified":1691307997116},{"_id":"themes/next/layout/tag.swig","hash":"0dfb653bd5de980426d55a0606d1ab122bd8c017","modified":1691294135139},{"_id":"themes/next/scripts/renderer.js","hash":"49a65df2028a1bc24814dc72fa50d52231ca4f05","modified":1691294135139},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"c3e6b8196c983c40fd140bdeca012d03e6e86967","modified":1691294135131},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"12d99fb8b62bd9e34d9672f306c9ae4ace7e053e","modified":1691294135131},{"_id":"themes/next/.github/ISSUE_TEMPLATE/other.md","hash":"d3efc0df0275c98440e69476f733097916a2d579","modified":1691294135131},{"_id":"themes/next/.github/ISSUE_TEMPLATE/question.md","hash":"53df7d537e26aaf062d70d86835c5fd8f81412f3","modified":1691294135131},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"5237a368ab99123749d724b6c379415f2c142a96","modified":1691294135135},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"0bd2d696f62a997a11a7d84fec0130122234174e","modified":1691294135135},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"9c4fe2873123bf9ceacab5c50d17d8a0f1baef27","modified":1691294135135},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"34b88784ec120dfdc20fa82aadeb5f64ef614d14","modified":1691294135135},{"_id":"themes/next/docs/ru/README.md","hash":"85dd68ed1250897a8e4a444a53a68c1d49eb7e11","modified":1691294135135},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"ca1030efdfca5e20f9db2e7a428998e66a24c0d0","modified":1691294135135},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"579c7bd8341873fb8be4732476d412814f1a3df7","modified":1691294135135},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"d3f03be036b75dc71cf3c366cd75aee7c127c874","modified":1691294135135},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"8b18f84503a361fc712b0fe4d4568e2f086ca97d","modified":1691294135135},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"b92585d251f1f9ebe401abb5d932cb920f9b8b10","modified":1691294135135},{"_id":"themes/next/layout/_macro/passage-end-tag.swig","hash":"8b77c4ce34f16c711b5c0b98a64be7a714bfe27f","modified":1691299016000},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"d9ce7331c1236bbe0a551d56cef2405e47e65325","modified":1691294135135},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"9c8dc0b8170679cdc1ee9ee8dbcbaebf3f42897b","modified":1691294135135},{"_id":"themes/next/docs/zh-CN/README.md","hash":"c038629ff8f3f24e8593c4c8ecf0bef3a35c750d","modified":1691294135135},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"fb23b85db6f7d8279d73ae1f41631f92f64fc864","modified":1691294135135},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"71655ca21907e9061b6e8ac52d0d8fbf54d0062b","modified":1691294135135},{"_id":"themes/next/layout/_partials/languages.swig","hash":"ba9e272f1065b8f0e8848648caa7dea3f02c6be1","modified":1691294135135},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9876dbfc15713c7a47d4bcaa301f4757bd978269","modified":1691294135135},{"_id":"themes/next/layout/_partials/comments.swig","hash":"db6ab5421b5f4b7cb32ac73ad0e053fdf065f83e","modified":1691294135135},{"_id":"themes/next/layout/_scripts/index.swig","hash":"cea942b450bcb0f352da78d76dc6d6f1d23d5029","modified":1691294135135},{"_id":"themes/next/layout/_partials/widgets.swig","hash":"83a40ce83dfd5cada417444fb2d6f5470aae6bb0","modified":1691294135135},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"d1f2bfde6f1da51a2b35a7ab9e7e8eb6eefd1c6b","modified":1691294135135},{"_id":"themes/next/layout/_macro/post.swig","hash":"bcd3a68c56df3402e5d1f099e026c998450c1b7a","modified":1691299154294},{"_id":"themes/next/layout/_scripts/pjax.swig","hash":"4d2c93c66e069852bb0e3ea2e268d213d07bfa3f","modified":1691294135135},{"_id":"themes/next/layout/_partials/footer.swig","hash":"b5e5f323c72efcde2772a82a79aed10e78fe6ab2","modified":1691946774078},{"_id":"themes/next/layout/_scripts/three.swig","hash":"a4f42f2301866bd25a784a2281069d8b66836d0b","modified":1691294135135},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"b782eb2e34c0c15440837040b5d65b093ab6ec04","modified":1691294135135},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"ef38c213679e7b6d2a4116f56c9e55d678446069","modified":1691294135135},{"_id":"themes/next/layout/_third-party/index.swig","hash":"70c3c01dd181de81270c57f3d99b6d8f4c723404","modified":1691294135135},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"311e5eceec9e949f1ea8d623b083cec0b8700ff2","modified":1691294135135},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"2731e262a6b88eaee2a3ca61e6a3583a7f594702","modified":1691294135135},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"aec50ed57b9d5d3faf2db3c88374f107203617e0","modified":1691294135139},{"_id":"themes/next/scripts/filters/front-matter.js","hash":"703bdd142a671b4b67d3d9dfb4a19d1dd7e7e8f7","modified":1691294135139},{"_id":"themes/next/scripts/filters/locals.js","hash":"b193a936ee63451f09f8886343dcfdca577c0141","modified":1691294135139},{"_id":"themes/next/scripts/filters/minify.js","hash":"19985723b9f677ff775f3b17dcebf314819a76ac","modified":1691294135139},{"_id":"themes/next/scripts/filters/post.js","hash":"44ba9b1c0bdda57590b53141306bb90adf0678db","modified":1691294135139},{"_id":"themes/next/scripts/helpers/font.js","hash":"40cf00e9f2b7aa6e5f33d412e03ed10304b15fd7","modified":1691294135139},{"_id":"themes/next/scripts/events/index.js","hash":"5743cde07f3d2aa11532a168a652e52ec28514fd","modified":1691294135139},{"_id":"themes/next/scripts/helpers/next-config.js","hash":"5e11f30ddb5093a88a687446617a46b048fa02e5","modified":1691294135139},{"_id":"themes/next/scripts/tags/button.js","hash":"8c6b45f36e324820c919a822674703769e6da32c","modified":1691294135139},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"958e86b2bd24e4fdfcbf9ce73e998efe3491a71f","modified":1691294135139},{"_id":"themes/next/scripts/helpers/engine.js","hash":"bdb424c3cc0d145bd0c6015bb1d2443c8a9c6cda","modified":1691294135139},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"94e0bbc7999b359baa42fa3731bdcf89c79ae2b3","modified":1691294135139},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"f1826ade2d135e2f60e2d95cb035383685b3370c","modified":1691294135139},{"_id":"themes/next/scripts/tags/label.js","hash":"fc5b267d903facb7a35001792db28b801cccb1f8","modified":1691294135139},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"983c6c4adea86160ecc0ba2204bc312aa338121d","modified":1691294135139},{"_id":"themes/next/scripts/tags/note.js","hash":"0a02bb4c15aec41f6d5f1271cdb5c65889e265d9","modified":1691294135139},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"d902fd313e8d35c3cc36f237607c2a0536c9edf1","modified":1691294135139},{"_id":"themes/next/scripts/tags/pdf.js","hash":"8c613b39e7bff735473e35244b5629d02ee20618","modified":1691294135139},{"_id":"themes/next/source/css/_colors.styl","hash":"a8442520f719d3d7a19811cb3b85bcfd4a596e1f","modified":1691294135139},{"_id":"themes/next/scripts/tags/tabs.js","hash":"93d8a734a3035c1d3f04933167b500517557ba3e","modified":1691294135139},{"_id":"themes/next/scripts/tags/video.js","hash":"e5ff4c44faee604dd3ea9db6b222828c4750c227","modified":1691294135139},{"_id":"themes/next/source/css/_mixins.styl","hash":"e31a557f8879c2f4d8d5567ee1800b3e03f91f6e","modified":1691294135139},{"_id":"themes/next/source/css/main.styl","hash":"a3a3bbb5a973052f0186b3523911cb2539ff7b88","modified":1691294135139},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1691294135139},{"_id":"themes/next/source/images/background.png","hash":"87dad7eb0ca2c805e5f5539c5c1fec566093b9d9","modified":1691397745081},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1691294135139},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1691294135139},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1691294135139},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1691294135139},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1691294135139},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1691294135139},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1691294135139},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1691294135139},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1691294135139},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1691294135139},{"_id":"themes/next/source/js/algolia-search.js","hash":"498d233eb5c7af6940baf94c1a1c36fdf1dd2636","modified":1691294135139},{"_id":"themes/next/source/js/bookmark.js","hash":"9734ebcb9b83489686f5c2da67dc9e6157e988ad","modified":1691294135139},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1691294135139},{"_id":"themes/next/source/js/next-boot.js","hash":"a1b0636423009d4a4e4cea97bcbf1842bfab582c","modified":1691294135139},{"_id":"themes/next/source/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1691294135139},{"_id":"themes/next/source/images/avatar.gif","hash":"179f17063869a91bd6af1c4dd0ee7d1b24cde2fa","modified":1691226375900},{"_id":"themes/next/source/js/local-search.js","hash":"35ccf100d8f9c0fd6bfbb7fa88c2a76c42a69110","modified":1691294135139},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"000bad572d76ee95d9c0a78f9ccdc8d97cc7d4b4","modified":1691294135135},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"c70f8e71e026e878a4e9d5ab3bbbf9b0b23c240c","modified":1691294135135},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"7dbe93b8297b746afb89700b4d29289556e85267","modified":1691294135135},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"810d544019e4a8651b756dd23e5592ee851eda71","modified":1691294135135},{"_id":"themes/next/layout/_partials/header/menu-item.swig","hash":"9440d8a3a181698b80e1fa47f5104f4565d8cdf3","modified":1691294135135},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"d31f896680a6c2f2c3f5128b4d4dd46c87ce2130","modified":1691294135135},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"c851717497ca64789f2176c9ecd1dedab237b752","modified":1691294135135},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"9b7a66791d7822c52117fe167612265356512477","modified":1691294135135},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"954ad71536b6eb08bd1f30ac6e2f5493b69d1c04","modified":1691294135135},{"_id":"themes/next/layout/_partials/post/post-footer.swig","hash":"8f14f3f8a1b2998d5114cc56b680fb5c419a6b07","modified":1691294135135},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"ae2261bea836581918a1c2b0d1028a78718434e0","modified":1691294135135},{"_id":"themes/next/layout/_partials/post/post-followme.swig","hash":"ceba16b9bd3a0c5c8811af7e7e49d0f9dcb2f41e","modified":1691294135135},{"_id":"themes/next/source/js/utils.js","hash":"730cca7f164eaf258661a61ff3f769851ff1e5da","modified":1691294135139},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"f79c44692451db26efce704813f7a8872b7e63a0","modified":1691294135135},{"_id":"themes/next/layout/_partials/post/post-reward.swig","hash":"2b1a73556595c37951e39574df5a3f20b2edeaef","modified":1691294135135},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"48430bd03b8f19c9b8cdb2642005ed67d56c6e0b","modified":1691294135135},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"2be50f9bfb1c56b85b3b6910a7df27f51143632c","modified":1691294135135},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"f48a6a8eba04eb962470ce76dd731e13074d4c45","modified":1691294135135},{"_id":"themes/next/source/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1691294135139},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1691294135135},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1691294135135},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1691294135135},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"4790058691b7d36cf6d2d6b4e93795a7b8d608ad","modified":1691294135135},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1691294135135},{"_id":"themes/next/layout/_partials/sidebar/site-overview.swig","hash":"c46849e0af8f8fb78baccd40d2af14df04a074af","modified":1691294135135},{"_id":"themes/next/layout/_scripts/pages/schedule.swig","hash":"077b5d66f6309f2e7dcf08645058ff2e03143e6c","modified":1691294135135},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"4decb10268a24a17715032ca1e2421fabfc09595","modified":1691337542681},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"2fa2b51d56bfac6a1ea76d651c93b9c20b01c09b","modified":1691294135135},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"5adea065641e8c55994dd2328ddae53215604928","modified":1691294135135},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"1472cabb0181f60a6a0b7fec8899a4d03dfb2040","modified":1691294135135},{"_id":"themes/next/layout/_third-party/chat/tidio.swig","hash":"cba0e6e0fad08568a9e74ba9a5bee5341cfc04c1","modified":1691294135135},{"_id":"themes/next/layout/_third-party/chat/chatra.swig","hash":"f910618292c63871ca2e6c6e66c491f344fa7b1f","modified":1691294135135},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b14908644225d78c864cd0a9b60c52407de56183","modified":1691294135135},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"82f5b6822aa5ec958aa987b101ef860494c6cf1f","modified":1691294135135},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"d6ceb70648555338a80ae5724b778c8c58d7060d","modified":1691294135135},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"f39a5bf3ce9ee9adad282501235e0c588e4356ec","modified":1691294135135},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"f7a9eca599a682479e8ca863db59be7c9c7508c8","modified":1691294135135},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"be0a8eccf1f6dc21154af297fc79555343031277","modified":1691294135135},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"6c5976621efd5db5f7c4c6b4f11bc79d6554885f","modified":1691294135135},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"4791c977a730f29c846efcf6c9c15131b9400ead","modified":1691294135135},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"767b6c714c22588bcd26ba70b0fc19b6810cbacd","modified":1691294135135},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"d35a999d67f4c302f76fdf13744ceef3c6506481","modified":1691294135135},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.swig","hash":"4b1986e43d6abce13450d2b41a736dd6a5620a10","modified":1691294135135},{"_id":"themes/next/layout/_third-party/statistics/cnzz-analytics.swig","hash":"a17ace37876822327a2f9306a472974442c9005d","modified":1691294135135},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"ecf751321e799f0fb3bf94d049e535130e2547aa","modified":1691294135135},{"_id":"themes/next/layout/_third-party/statistics/index.swig","hash":"5f6a966c509680dbfa70433f9d658cee59c304d7","modified":1691294135135},{"_id":"themes/next/layout/_third-party/search/swiftype.swig","hash":"ba0dbc06b9d244073a1c681ff7a722dcbf920b51","modified":1691294135135},{"_id":"themes/next/layout/_third-party/tags/mermaid.swig","hash":"f3c43664a071ff3c0b28bd7e59b5523446829576","modified":1691294135139},{"_id":"themes/next/layout/_third-party/tags/pdf.swig","hash":"d30b0e255a8092043bac46441243f943ed6fb09b","modified":1691294135139},{"_id":"themes/next/scripts/events/lib/injects-point.js","hash":"6661c1c91c7cbdefc6a5e6a034b443b8811235a1","modified":1691294135139},{"_id":"themes/next/layout/_third-party/statistics/firestore.swig","hash":"b26ac2bfbe91dd88267f8b96aee6bb222b265b7a","modified":1691294135135},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.swig","hash":"d56d5af427cdfecc33a0f62ee62c056b4e33d095","modified":1691294135135},{"_id":"themes/next/scripts/events/lib/config.js","hash":"d34c6040b13649714939f59be5175e137de65ede","modified":1691294135139},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"2486f3e0150c753e5f3af1a3665d074704b8ee2c","modified":1691294135139},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"7f2d93af012c1e14b8596fecbfc7febb43d9b7f5","modified":1691294135139},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"f233d8d0103ae7f9b861344aa65c1a3c1de8a845","modified":1691294135139},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"a54708fd9309b4357c423a3730eb67f395344a5e","modified":1691294135139},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"7f8b92913d21070b489457fa5ed996d2a55f2c32","modified":1691294135139},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"e51dc3072c1ba0ea3008f09ecae8b46242ec6021","modified":1691294135139},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"d5fefc31fba4ab0188305b1af1feb61da49fdeb0","modified":1691294135139},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"f4e694e5db81e57442c7e34505a416d818b3044a","modified":1691294135139},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"4c0c99c7e0f00849003dfce02a131104fb671137","modified":1691294135139},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"f70be8e229da7e1715c11dd0e975a2e71e453ac8","modified":1691294135139},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"62df49459d552bbf73841753da8011a1f5e875c8","modified":1691294135139},{"_id":"themes/next/scripts/filters/comment/valine.js","hash":"6cbd85f9433c06bae22225ccf75ac55e04f2d106","modified":1691294135139},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"612ec843372dae709acb17112c1145a53450cc59","modified":1691294135139},{"_id":"themes/next/source/js/schemes/muse.js","hash":"1eb9b88103ddcf8827b1a7cbc56471a9c5592d53","modified":1691294135139},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"0ac5ce155bc58c972fe21c4c447f85e6f8755c62","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"ca5e70662dcfb261c25191cc5db5084dcf661c76","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"8e7b57a72e757cf95278239641726bb2d5b869d1","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"a47725574e1bee3bc3b63b0ff2039cc982b17eff","modified":1691294135139},{"_id":"themes/next/source/css/_variables/base.styl","hash":"818508748b7a62e02035e87fe58e75b603ed56dc","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"2e3bf7baf383c9073ec5e67f157d3cb3823c0957","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"a1690e035b505d28bdef2b4424c13fc6312ab049","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"681d33e3bc85bdca407d93b134c089264837378c","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"7785bd756e0c4acede3a47fec1ed7b55988385a5","modified":1691294135139},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1691294135143},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"a2e9e00962e43e98ec2614d6d248ef1773bb9b78","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"0b2c4b78eead410020d7c4ded59c75592a648df8","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"b1f0fab7344a20ed6748b04065b141ad423cf4d9","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"8f58570a1bbc34c4989a47a1b7d42a8030f38b06","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"18ce72d90459c9aa66910ac64eae115f2dde3767","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"f6516d0f7d89dc7b6c6e143a5af54b926f585d82","modified":1691294135139},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1691294135143},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"179e33b8ac7f4d8a8e76736a7e4f965fe9ab8b42","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"bb7ace23345364eb14983e860a7172e1683a4c94","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"7104b9cef90ca3b140d7a7afcf15540a250218fc","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"6136da4bbb7e70cec99f5c7ae8c7e74f5e7c261a","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"a717969829fa6ef88225095737df3f8ee86c286b","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"f0131db6275ceaecae7e1a6a3798b8f89f6c850d","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"4d1c17345d2d39ef7698f7acf82dfc0f59308c34","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"93db5dafe9294542a6b5f647643cb9deaced8e06","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"2b2e7b5cea7783c9c8bb92655e26a67c266886f0","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"e282df938bd029f391c466168d0e68389978f120","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"70a4324b70501132855b5e59029acfc5d3da1ebd","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"85da2f3006f4bef9a2199416ecfab4d288f848c4","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"44f47c88c06d89d06f220f102649057118715828","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"e740deadcfc4f29c5cb01e40f9df6277262ba4e3","modified":1691294135139},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1691294135139},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","hash":"0038dc97c79451578b7bd48af60ba62282b4082b","modified":1691294135143},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1691294135143},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fafc96c86926b22afba8bb9418c05e6afbc05a57","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"2bd0eb1512415325653b26d62a4463e6de83c5ac","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"e771dcb0b4673e063c0f3e2d73e7336ac05bcd57","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"d21d4ac1982c13d02f125a67c065412085a92ff2","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"e75693f33dbc92afc55489438267869ae2f3db54","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f49ca072b5a800f735e8f01fc3518f885951dd8e","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"902569a9dea90548bec21a823dd3efd94ff7c133","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"ded41fd9d20a5e8db66aaff7cc50f105f5ef2952","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/post/post-followme.styl","hash":"1e4190c10c9e0c9ce92653b0dbcec21754b0b69d","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"72d495a88f7d6515af425c12cbc67308a57d88ea","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"65cb6edb69e94e70e3291e9132408361148d41d5","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"6a97bcfa635d637dc59005be3b931109e0d1ead5","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"d114b2a531129e739a27ba6271cfe6857aa9a865","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"f5c2788a78790aca1a2f37f7149d6058afb539e0","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"99e12c9ce3d14d4837e3d3f12fc867ba9c565317","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"5b5649b9749e3fd8b63aef22ceeece0a6e1df605","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"a760ee83ba6216871a9f14c5e56dc9bd0d9e2103","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"8a7fc03a568b95be8d3337195e38bc7ec5ba2b23","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"b49e9fbd3c182b8fc066b8c2caf248e3eb748619","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"e2992846b39bf3857b5104675af02ba73e72eed5","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"9f0b93d109c9aec79450c8a0cf4a4eab717d674d","modified":1691294135139},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9a878d0119785a2316f42aebcceaa05a120b9a7a","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"e2d606f1ac343e9be4f15dbbaf3464bc4df8bf81","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"e7a9fdb6478b8674b1cdf94de4f8052843fb71d9","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/header/header.styl","hash":"a793cfff86ad4af818faef04c18013077873f8f0","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/header/headerband.styl","hash":"0caf32492692ba8e854da43697a2ec8a41612194","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"5f432a6ed9ca80a413c68b00e93d4a411abf280a","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"45a239edca44acecf971d99b04f30a1aafbf6906","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"b2fc519828fe89a1f8f03ff7b809ad68cd46f3d7","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/footer/footer.styl","hash":"454a4aebfabb4469b92a8cbb49f46c49ac9bf165","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"2cb1876e9e0c9ac32160888af27b1178dbcb0616","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"fa0222197b5eee47e18ac864cdc6eac75678b8fe","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"44487d9ab290dc97871fa8dd4487016deb56e123","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"1f0e7fbe80956f47087c2458ea880acf7a83078b","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"9b479c2f9a9bfed77885e5093b8245cc5d768ec7","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"a960a2dd587b15d3b3fe1b59525d6fa971c6a6ec","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"a05a4031e799bc864a4536f9ef61fe643cd421af","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"b3220db827e1adbca7880c2bb23e78fa7cbe95cb","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar.styl","hash":"a9cd93c36bae5af9223e7804963096274e8a4f03","modified":1691294135139},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"2a47f8a6bb589c2fb635e6c1e4a2563c7f63c407","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"f71a3e86c05ea668b008cf05a81f67d92b6d65e4","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/highlight/diff.styl","hash":"d3f73688bb7423e3ab0de1efdf6db46db5e34f80","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/highlight/highlight.styl","hash":"35c871a809afa8306c8cde13651010e282548bc6","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/highlight/theme.styl","hash":"3b3acc5caa0b95a2598bef4eeacb21bab21bea56","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"1d2778ca5aeeeafaa690dc2766b01b352ab76a02","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b49c64f8e9a6ca1c45c0ba98febf1974fdd03616","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"f23670f1d8e749f3e83766d446790d8fd9620278","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/tags/tags.styl","hash":"9e4c0653cfd3cc6908fa0d97581bcf80861fb1e7","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"709d10f763e357e1472d6471f8be384ec9e2d983","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"e4d9a77ffe98e851c1202676940097ba28253313","modified":1691294135139},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"d7fce4b51b5f4b7c31d93a9edb6c6ce740aa0d6b","modified":1691294135139},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1691294135143},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1691294135143},{"_id":"themes/next/package-lock.json","hash":"b6598e49310869a234c351ba93dee8381d351190","modified":1691336610002},{"_id":"public/atom.xml","hash":"98eb372b41a2586edfe78219ca53f85e83f9e97a","modified":1694964283114},{"_id":"public/search.xml","hash":"95e975b4b48102404849162f005ac795f3f3fb72","modified":1694964283114},{"_id":"public/tags/index.html","hash":"58a7005c63a02978492dcc96884612c433a6d90b","modified":1694964283114},{"_id":"public/uploads/index.html","hash":"e14762675bb8a63091b8c6010790ade26b66c6fc","modified":1694964283114},{"_id":"public/archives/2023/09/index.html","hash":"29c834b4b1e1c056565c5ed18e933fc09d17a0e2","modified":1694964283114},{"_id":"public/categories/C/index.html","hash":"96706e80dad884c18dc26ad62dcaee31872841f6","modified":1694964283114},{"_id":"public/categories/deep-learning/index.html","hash":"ff42a01c78c7782271f87b44abe12709fa9de658","modified":1694964283114},{"_id":"public/categories/Recording/index.html","hash":"91bc300065781816990d947aaf516155f32e7d0a","modified":1694964283114},{"_id":"public/categories/python/index.html","hash":"fbfd790aef7b71d360303053fdbaeed9c878d0fe","modified":1694964283114},{"_id":"public/tags/python/index.html","hash":"df2d7364d897a00da5a3b41f741c35505c392f98","modified":1694964283114},{"_id":"public/about/index.html","hash":"0251979b0049646660c1b3b63bd596d5493bfa63","modified":1694964283114},{"_id":"public/links/index.html","hash":"911a7dc1cd78357485f98c6b121a2b354178ccac","modified":1694964283114},{"_id":"public/categories/index.html","hash":"c6fb1938184f2fca89097bf3319358596643f5cf","modified":1694964283114},{"_id":"public/2023/09/01/C++Primer/index.html","hash":"647c50f881eb2655b53423d645fcaf98d7f6c5cc","modified":1694964283114},{"_id":"public/2023/08/07/DeepLearning/index.html","hash":"eeaea2139a36e927f3f6ff3b8ad7abea2d5315c1","modified":1694964283114},{"_id":"public/2023/08/07/softmax回归/index.html","hash":"3719bcdb5eb37d104acc17b506c1cab4614d7b75","modified":1694964283114},{"_id":"public/2023/08/07/SummerRecord2023/index.html","hash":"2f79f6c7540469cd4e7834faa566069525b5ef1e","modified":1694964283114},{"_id":"public/2023/08/06/python/index.html","hash":"f632c97e07b00d733626c6b8946d76eb97943dd0","modified":1694964283114},{"_id":"public/archives/index.html","hash":"bf7c4935524a95635293bcb6664f347ee0950851","modified":1694964283114},{"_id":"public/archives/2023/index.html","hash":"38f6eed922ec2a6750e5960e3bda3f2a1ce7bd2e","modified":1694964283114},{"_id":"public/archives/2023/08/index.html","hash":"fdef63cb9999854515f04e056af0b14753850b42","modified":1694964283114},{"_id":"public/index.html","hash":"f32c2d78ee6a7567bf326fa341a1a3594e655919","modified":1694964283114},{"_id":"public/images/2023-09-15 21-39-05 的屏幕截图.png","hash":"422c7ba102b37cd3a147e3c4c0fab592aaa523d6","modified":1694964283114},{"_id":"public/images/avatar.png","hash":"74ce5da46d280a5d6041ee3f58be6a6783a26206","modified":1694964283114},{"_id":"public/uploads/avatar.gif","hash":"179f17063869a91bd6af1c4dd0ee7d1b24cde2fa","modified":1694964283114},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1694964283114},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1694964283114},{"_id":"public/images/avatar.gif","hash":"179f17063869a91bd6af1c4dd0ee7d1b24cde2fa","modified":1694964283114},{"_id":"public/images/background.png","hash":"87dad7eb0ca2c805e5f5539c5c1fec566093b9d9","modified":1694964283114},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1694964283114},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1694964283114},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1694964283114},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1694964283114},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1694964283114},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1694964283114},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1694964283114},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1694964283114},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1694964283114},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1694964283114},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1694964283114},{"_id":"public/images/2023-09-12 23-40-39 的屏幕截图.png","hash":"1a0295c9de22951bd8923d77d83361a583f2c5dd","modified":1694964283114},{"_id":"public/images/2023-09-15 21-17-12 的屏幕截图.png","hash":"8d0d408246ba4b5b621c79241861eff57c160899","modified":1694964283114},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1694964283114},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1694964283114},{"_id":"public/js/bookmark.js","hash":"9734ebcb9b83489686f5c2da67dc9e6157e988ad","modified":1694964283114},{"_id":"public/js/algolia-search.js","hash":"498d233eb5c7af6940baf94c1a1c36fdf1dd2636","modified":1694964283114},{"_id":"public/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1694964283114},{"_id":"public/js/next-boot.js","hash":"a1b0636423009d4a4e4cea97bcbf1842bfab582c","modified":1694964283114},{"_id":"public/js/local-search.js","hash":"35ccf100d8f9c0fd6bfbb7fa88c2a76c42a69110","modified":1694964283114},{"_id":"public/js/utils.js","hash":"730cca7f164eaf258661a61ff3f769851ff1e5da","modified":1694964283114},{"_id":"public/js/schemes/pisces.js","hash":"0ac5ce155bc58c972fe21c4c447f85e6f8755c62","modified":1694964283114},{"_id":"public/js/schemes/muse.js","hash":"1eb9b88103ddcf8827b1a7cbc56471a9c5592d53","modified":1694964283114},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1694964283114},{"_id":"public/css/main.css","hash":"243552848c81bc56f48d78c099b66d7c826868ae","modified":1694964283114},{"_id":"public/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1694964283114},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1694964283114},{"_id":"public/lib/font-awesome/css/all.min.css","hash":"0038dc97c79451578b7bd48af60ba62282b4082b","modified":1694964283114}],"Category":[{"name":"C++","_id":"clmnlyz6o0004myqbcrno9q5t"},{"name":"deep learning","_id":"clmnlyz6r000bmyqbar1317uo"},{"name":"Recording","_id":"clmnlyz6s000emyqb72hc0f8k"},{"name":"python","_id":"clmnlyz6s000hmyqbglgg6mpe"}],"Data":[],"Page":[{"title":"about","date":"2023-08-06T06:03:47.000Z","type":"about","_content":"本科在读，软件工程专业。\n\n写博客的目的：\n    一、总结学习过程，强迫自己以更加系统和严谨的态度梳理知识；\n    二、记录学习过程中遇到的问题，以及解决问题的经验；\n    三、希望自己的记录会在将来给他人一些帮助\n\n联系方式\nQQ: 2863353792\nGithub: https://github.com/xxfs040\n\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2023-08-06 14:03:47\ntype: \"about\"\n---\n本科在读，软件工程专业。\n\n写博客的目的：\n    一、总结学习过程，强迫自己以更加系统和严谨的态度梳理知识；\n    二、记录学习过程中遇到的问题，以及解决问题的经验；\n    三、希望自己的记录会在将来给他人一些帮助\n\n联系方式\nQQ: 2863353792\nGithub: https://github.com/xxfs040\n\n","updated":"2023-08-06T06:20:04.195Z","path":"about/index.html","comments":1,"layout":"page","_id":"clmnlyz6k0000myqbek262czz","content":"<p>本科在读，软件工程专业。</p>\n<p>写博客的目的：<br>    一、总结学习过程，强迫自己以更加系统和严谨的态度梳理知识；<br>    二、记录学习过程中遇到的问题，以及解决问题的经验；<br>    三、希望自己的记录会在将来给他人一些帮助</p>\n<p>联系方式<br>QQ: 2863353792<br>Github: <a href=\"https://github.com/xxfs040\">https://github.com/xxfs040</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>本科在读，软件工程专业。</p>\n<p>写博客的目的：<br>    一、总结学习过程，强迫自己以更加系统和严谨的态度梳理知识；<br>    二、记录学习过程中遇到的问题，以及解决问题的经验；<br>    三、希望自己的记录会在将来给他人一些帮助</p>\n<p>联系方式<br>QQ: 2863353792<br>Github: <a href=\"https://github.com/xxfs040\">https://github.com/xxfs040</a></p>\n"},{"title":"友链","date":"2023-08-06T07:36:47.000Z","type":"友链","_content":"\n\n<div class=\"post-body\">\n   <div id=\"links\">\n      <style>\n         .links-content{\n         margin-top:1rem;\n         }\n         .link-navigation::after {\n         content: \" \";\n         display: block;\n         clear: both;\n         }\n         .card {\n         width: 45%;\n         font-size: 1rem;\n         padding: 10px 20px;\n         border-radius: 4px;\n         transition-duration: 0.15s;\n         margin-bottom: 1rem;\n         display:flex;\n         }\n         .card:nth-child(odd) {\n         float: left;\n         }\n         .card:nth-child(even) {\n         float: right;\n         }\n         .card:hover {\n         transform: scale(1.1);\n         box-shadow: 0 2px 6px 0 rgba(0, 0, 0, 0.12), 0 0 6px 0 rgba(0, 0, 0, 0.04);\n         }\n         .card a {\n         border:none;\n         }\n         .card .ava {\n         width: 3rem!important;\n         height: 3rem!important;\n         margin:0!important;\n         margin-right: 1em!important;\n         border-radius:4px;\n         }\n         .card .card-header {\n         font-style: italic;\n         overflow: hidden;\n         width: 100%;\n         }\n         .card .card-header a {\n         font-style: normal;\n         color: #2bbc8a;\n         font-weight: bold;\n         text-decoration: none;\n         }\n         .card .card-header a:hover {\n         color: #d480aa;\n         text-decoration: none;\n         }\n         .card .card-header .info {\n         font-style:normal;\n         color:#a3a3a3;\n         font-size:14px;\n         min-width: 0;\n         overflow: hidden;\n         white-space: nowrap;\n         }\n      </style>\n      <div class=\"links-content\">\n         <div class=\"link-navigation\">\n            <div class=\"card\">\n               <img class=\"ava\" src=\"https://avatars.githubusercontent.com/u/110778575?v=4\" />\n               <div class=\"card-header\">\n                  <div>\n                     <a href=\"https://starrysky1004.github.io\">starrysky</a>\n                  </div>\n                  <div class=\"info\">超可爱的匡师傅，匡师傅红烧牛肉面，超棒！！！</div>\n               </div>\n            </div>\n         </div>\n      </div>\n   </div>\n</div>\n\n","source":"links/index.md","raw":"---\ntitle: 友链\ndate: 2023-08-06 15:36:47\ntype: \"友链\"\n---\n\n\n<div class=\"post-body\">\n   <div id=\"links\">\n      <style>\n         .links-content{\n         margin-top:1rem;\n         }\n         .link-navigation::after {\n         content: \" \";\n         display: block;\n         clear: both;\n         }\n         .card {\n         width: 45%;\n         font-size: 1rem;\n         padding: 10px 20px;\n         border-radius: 4px;\n         transition-duration: 0.15s;\n         margin-bottom: 1rem;\n         display:flex;\n         }\n         .card:nth-child(odd) {\n         float: left;\n         }\n         .card:nth-child(even) {\n         float: right;\n         }\n         .card:hover {\n         transform: scale(1.1);\n         box-shadow: 0 2px 6px 0 rgba(0, 0, 0, 0.12), 0 0 6px 0 rgba(0, 0, 0, 0.04);\n         }\n         .card a {\n         border:none;\n         }\n         .card .ava {\n         width: 3rem!important;\n         height: 3rem!important;\n         margin:0!important;\n         margin-right: 1em!important;\n         border-radius:4px;\n         }\n         .card .card-header {\n         font-style: italic;\n         overflow: hidden;\n         width: 100%;\n         }\n         .card .card-header a {\n         font-style: normal;\n         color: #2bbc8a;\n         font-weight: bold;\n         text-decoration: none;\n         }\n         .card .card-header a:hover {\n         color: #d480aa;\n         text-decoration: none;\n         }\n         .card .card-header .info {\n         font-style:normal;\n         color:#a3a3a3;\n         font-size:14px;\n         min-width: 0;\n         overflow: hidden;\n         white-space: nowrap;\n         }\n      </style>\n      <div class=\"links-content\">\n         <div class=\"link-navigation\">\n            <div class=\"card\">\n               <img class=\"ava\" src=\"https://avatars.githubusercontent.com/u/110778575?v=4\" />\n               <div class=\"card-header\">\n                  <div>\n                     <a href=\"https://starrysky1004.github.io\">starrysky</a>\n                  </div>\n                  <div class=\"info\">超可爱的匡师傅，匡师傅红烧牛肉面，超棒！！！</div>\n               </div>\n            </div>\n         </div>\n      </div>\n   </div>\n</div>\n\n","updated":"2023-08-13T17:03:54.539Z","path":"links/index.html","comments":1,"layout":"page","_id":"clmnlyz6n0002myqba6i28kww","content":"<div class=\"post-body\">\n   <div id=\"links\">\n      <style>\n         .links-content{\n         margin-top:1rem;\n         }\n         .link-navigation::after {\n         content: \" \";\n         display: block;\n         clear: both;\n         }\n         .card {\n         width: 45%;\n         font-size: 1rem;\n         padding: 10px 20px;\n         border-radius: 4px;\n         transition-duration: 0.15s;\n         margin-bottom: 1rem;\n         display:flex;\n         }\n         .card:nth-child(odd) {\n         float: left;\n         }\n         .card:nth-child(even) {\n         float: right;\n         }\n         .card:hover {\n         transform: scale(1.1);\n         box-shadow: 0 2px 6px 0 rgba(0, 0, 0, 0.12), 0 0 6px 0 rgba(0, 0, 0, 0.04);\n         }\n         .card a {\n         border:none;\n         }\n         .card .ava {\n         width: 3rem!important;\n         height: 3rem!important;\n         margin:0!important;\n         margin-right: 1em!important;\n         border-radius:4px;\n         }\n         .card .card-header {\n         font-style: italic;\n         overflow: hidden;\n         width: 100%;\n         }\n         .card .card-header a {\n         font-style: normal;\n         color: #2bbc8a;\n         font-weight: bold;\n         text-decoration: none;\n         }\n         .card .card-header a:hover {\n         color: #d480aa;\n         text-decoration: none;\n         }\n         .card .card-header .info {\n         font-style:normal;\n         color:#a3a3a3;\n         font-size:14px;\n         min-width: 0;\n         overflow: hidden;\n         white-space: nowrap;\n         }\n      </style>\n      <div class=\"links-content\">\n         <div class=\"link-navigation\">\n            <div class=\"card\">\n               <img class=\"ava\" src=\"https://avatars.githubusercontent.com/u/110778575?v=4\" />\n               <div class=\"card-header\">\n                  <div>\n                     <a href=\"https://starrysky1004.github.io\">starrysky</a>\n                  </div>\n                  <div class=\"info\">超可爱的匡师傅，匡师傅红烧牛肉面，超棒！！！</div>\n               </div>\n            </div>\n         </div>\n      </div>\n   </div>\n</div>\n\n","site":{"data":{}},"excerpt":"","more":"<div class=\"post-body\">\n   <div id=\"links\">\n      <style>\n         .links-content{\n         margin-top:1rem;\n         }\n         .link-navigation::after {\n         content: \" \";\n         display: block;\n         clear: both;\n         }\n         .card {\n         width: 45%;\n         font-size: 1rem;\n         padding: 10px 20px;\n         border-radius: 4px;\n         transition-duration: 0.15s;\n         margin-bottom: 1rem;\n         display:flex;\n         }\n         .card:nth-child(odd) {\n         float: left;\n         }\n         .card:nth-child(even) {\n         float: right;\n         }\n         .card:hover {\n         transform: scale(1.1);\n         box-shadow: 0 2px 6px 0 rgba(0, 0, 0, 0.12), 0 0 6px 0 rgba(0, 0, 0, 0.04);\n         }\n         .card a {\n         border:none;\n         }\n         .card .ava {\n         width: 3rem!important;\n         height: 3rem!important;\n         margin:0!important;\n         margin-right: 1em!important;\n         border-radius:4px;\n         }\n         .card .card-header {\n         font-style: italic;\n         overflow: hidden;\n         width: 100%;\n         }\n         .card .card-header a {\n         font-style: normal;\n         color: #2bbc8a;\n         font-weight: bold;\n         text-decoration: none;\n         }\n         .card .card-header a:hover {\n         color: #d480aa;\n         text-decoration: none;\n         }\n         .card .card-header .info {\n         font-style:normal;\n         color:#a3a3a3;\n         font-size:14px;\n         min-width: 0;\n         overflow: hidden;\n         white-space: nowrap;\n         }\n      </style>\n      <div class=\"links-content\">\n         <div class=\"link-navigation\">\n            <div class=\"card\">\n               <img class=\"ava\" src=\"https://avatars.githubusercontent.com/u/110778575?v=4\" />\n               <div class=\"card-header\">\n                  <div>\n                     <a href=\"https://starrysky1004.github.io\">starrysky</a>\n                  </div>\n                  <div class=\"info\">超可爱的匡师傅，匡师傅红烧牛肉面，超棒！！！</div>\n               </div>\n            </div>\n         </div>\n      </div>\n   </div>\n</div>\n\n"},{"title":"tags","date":"2023-08-06T06:03:53.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2023-08-06 14:03:53\ntype: \"tags\"\n---\n","updated":"2023-08-06T06:05:39.240Z","path":"tags/index.html","comments":1,"layout":"page","_id":"clmnlyz6p0005myqb8jr731oh","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2023-08-06T06:04:04.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2023-08-06 14:04:04\ntype: \"categories\"\n---\n","updated":"2023-08-06T06:05:11.060Z","path":"categories/index.html","comments":1,"layout":"page","_id":"clmnlyz6q0007myqb9wxb35b2","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"uploads","date":"2023-08-06T16:29:13.000Z","_content":"","source":"uploads/index.md","raw":"---\ntitle: uploads\ndate: 2023-08-07 00:29:13\n---\n","updated":"2023-08-06T16:29:13.237Z","path":"uploads/index.html","comments":1,"layout":"page","_id":"clmnlyz6q0009myqb30nb8t0o","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"C++Priemr.md","date":"2023-09-01T13:12:34.000Z","_content":"# C++ Primer\n\n<!--more-->\n\n\n## 1. 开始\n\n### 输入输出\n\n函数的定义包含四个部分：1. 返回类型；2.函数名；3.一个括号包含的形参类表；4.函数体。\n\n编写程序之后，我们需要编译它。很多PC机上的编译器都具备集成开发环境（IDE）将编译器于其他程序创建的分析工具包装在一起。\n\n在C++中包含了一个全面的标准库来提供IO机制，比如**iostream**库。iostream库包含了两个基础类型的istream和ostream，分别表示输入流和输出流。一个流就是一个字符序列，从IO设备里读取或写入IO设备的。\n\n| 对象 |            |\n| ---- | ---------- |\n| cin  | 标准输入   |\n| cout | 标准输出   |\n| cerr | 标准错误   |\n| clog | 一般性信息 |\n| <<   | 输出运算符 |\n| >>   | 输入运算符 |\n\n程序示例：\n\n```c++\n#include<iostream>\nint main()\n{\n    std::cout << \"Enter two numbers:\" << std::endl;\n    int v1 = 0, v2 = 0;\n    std::cin >> v1 >>v2;\n    std::cout << \"The sum of\" << v1 << \"and\" << v2\n              << \"is\" << v1 + v2 << std:endl;\n    return 0;\n}\n```\n\n程序中的`std::`指出了名字cout和endl是定义在名为std命名空间中的。\n\nendl（操纵符），结束当前行，并将与设备关联的缓冲区（buffer）中的内容刷到设备中。\n\n\n\n### 注释\n\n//：单行注释\n\n/**/：多行注释\n\n\n\n### 读取不定的输入数据\n\n```c++\n#include<iostream>\nint main()\n{\n    int sum = 0,value = 0;\n    //读取数据直到文件尾，计算所有读入的值的和\n    while(std::cin >> value)\n        sum += value;\n    std::cout << \"Sum is:\" << sum <<std::endl;\n    return 0;\n}\n```\n\n\n\n## 2 变量和基本类型\n\n### 2.5处理类型\n\n#### `typedef`\n\n```c++\ntypedef double wages; //wages是double的同义词\ntypedef wages base, *p; //base是double的同义词，p是double* 的同义词\n```\n\n\n\n#### `别名声明`\n\n```c++\nusing SI = Sales_item; //SI是sales_item的同义词\n```\n\n\n\n#### `auto`类型说明符\n\nauto类型说明符号的作用是让编译器去分析表达式所属的类型。那么显然，auto定义的变量必须有初始值。\n\n```c++\nauto item = val1 + val2;\n```\n\n如果val1和val2是类Sales_item的对象，则item的类型就是Sales_item；如果这两个变量的类型是double，则auto的类型就是double，以此类推。\n\n使用auto也能在一条语句上声明多个变量，但条件是所有变量的初始数据类型都一样。\n\n#### auto的使用\n\n首先，我们所熟知的，使用引用其实是使用引用的对象，特别是当引用被用作初始值时，真正参与初始化的其实是引用对象的值。此时编译器以引用对象的类型作为auto的类型：\n\n```c++\nint i = 0, &r = i;\nauto a = r; //a是一个整数（r是i的别名，而i是一个整数）\n```\n\n其次，auto一般会忽略掉顶层const，同时底层const则会保留下来，比如当初始值是一个指向常量的指针时：\n\n```c++\nconst int ci = i,&cr = ci;\nauto b = ci;\t//b是一个整数(ci的顶层const特性被忽略掉了)\nauto c = cr;\t//c是一个整数(cr是ci的别名，ci本身是一个顶层const)\nauto d = &i;\t//d是一个指向整形的指针(整数的地址就是指向整数的指针)\nauto r = &ci;\t//e是一个指向整数常量的指针(对常量对象取缔值是一种底层const)\n```\n\n如果希望推断出的auto类型是一个顶层const，需要说明指出：\n\n```c++\nconst auto f = ci;\t//ci的推演类型是int，f是const int\n```\n\n还可以将引用类型设为auto，此时原来的初始化规则仍然适用：\n\n```c++\nauto &g = ci;\t//g是一个整形常量引用，绑定到ci\nauto &h = 42;\t//错误：不能为非常量引用绑定字面值\nconst auto &j = 42; //正确：可以为常量引用绑定字面值\n```\n\n\n\n\n\n\n","source":"_posts/C++Primer.md","raw":"---\ntitle: C++Priemr.md\ndate: 2023-09-01 21:12:34\ntags:\ncategories:\n- C++\n---\n# C++ Primer\n\n<!--more-->\n\n\n## 1. 开始\n\n### 输入输出\n\n函数的定义包含四个部分：1. 返回类型；2.函数名；3.一个括号包含的形参类表；4.函数体。\n\n编写程序之后，我们需要编译它。很多PC机上的编译器都具备集成开发环境（IDE）将编译器于其他程序创建的分析工具包装在一起。\n\n在C++中包含了一个全面的标准库来提供IO机制，比如**iostream**库。iostream库包含了两个基础类型的istream和ostream，分别表示输入流和输出流。一个流就是一个字符序列，从IO设备里读取或写入IO设备的。\n\n| 对象 |            |\n| ---- | ---------- |\n| cin  | 标准输入   |\n| cout | 标准输出   |\n| cerr | 标准错误   |\n| clog | 一般性信息 |\n| <<   | 输出运算符 |\n| >>   | 输入运算符 |\n\n程序示例：\n\n```c++\n#include<iostream>\nint main()\n{\n    std::cout << \"Enter two numbers:\" << std::endl;\n    int v1 = 0, v2 = 0;\n    std::cin >> v1 >>v2;\n    std::cout << \"The sum of\" << v1 << \"and\" << v2\n              << \"is\" << v1 + v2 << std:endl;\n    return 0;\n}\n```\n\n程序中的`std::`指出了名字cout和endl是定义在名为std命名空间中的。\n\nendl（操纵符），结束当前行，并将与设备关联的缓冲区（buffer）中的内容刷到设备中。\n\n\n\n### 注释\n\n//：单行注释\n\n/**/：多行注释\n\n\n\n### 读取不定的输入数据\n\n```c++\n#include<iostream>\nint main()\n{\n    int sum = 0,value = 0;\n    //读取数据直到文件尾，计算所有读入的值的和\n    while(std::cin >> value)\n        sum += value;\n    std::cout << \"Sum is:\" << sum <<std::endl;\n    return 0;\n}\n```\n\n\n\n## 2 变量和基本类型\n\n### 2.5处理类型\n\n#### `typedef`\n\n```c++\ntypedef double wages; //wages是double的同义词\ntypedef wages base, *p; //base是double的同义词，p是double* 的同义词\n```\n\n\n\n#### `别名声明`\n\n```c++\nusing SI = Sales_item; //SI是sales_item的同义词\n```\n\n\n\n#### `auto`类型说明符\n\nauto类型说明符号的作用是让编译器去分析表达式所属的类型。那么显然，auto定义的变量必须有初始值。\n\n```c++\nauto item = val1 + val2;\n```\n\n如果val1和val2是类Sales_item的对象，则item的类型就是Sales_item；如果这两个变量的类型是double，则auto的类型就是double，以此类推。\n\n使用auto也能在一条语句上声明多个变量，但条件是所有变量的初始数据类型都一样。\n\n#### auto的使用\n\n首先，我们所熟知的，使用引用其实是使用引用的对象，特别是当引用被用作初始值时，真正参与初始化的其实是引用对象的值。此时编译器以引用对象的类型作为auto的类型：\n\n```c++\nint i = 0, &r = i;\nauto a = r; //a是一个整数（r是i的别名，而i是一个整数）\n```\n\n其次，auto一般会忽略掉顶层const，同时底层const则会保留下来，比如当初始值是一个指向常量的指针时：\n\n```c++\nconst int ci = i,&cr = ci;\nauto b = ci;\t//b是一个整数(ci的顶层const特性被忽略掉了)\nauto c = cr;\t//c是一个整数(cr是ci的别名，ci本身是一个顶层const)\nauto d = &i;\t//d是一个指向整形的指针(整数的地址就是指向整数的指针)\nauto r = &ci;\t//e是一个指向整数常量的指针(对常量对象取缔值是一种底层const)\n```\n\n如果希望推断出的auto类型是一个顶层const，需要说明指出：\n\n```c++\nconst auto f = ci;\t//ci的推演类型是int，f是const int\n```\n\n还可以将引用类型设为auto，此时原来的初始化规则仍然适用：\n\n```c++\nauto &g = ci;\t//g是一个整形常量引用，绑定到ci\nauto &h = 42;\t//错误：不能为非常量引用绑定字面值\nconst auto &j = 42; //正确：可以为常量引用绑定字面值\n```\n\n\n\n\n\n\n","slug":"C++Primer","published":1,"updated":"2023-09-01T14:55:04.393Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clmnlyz6m0001myqbffcif0wn","content":"<h1 id=\"C-Primer\"><a href=\"#C-Primer\" class=\"headerlink\" title=\"C++ Primer\"></a>C++ Primer</h1><span id=\"more\"></span>\n<h2 id=\"1-开始\"><a href=\"#1-开始\" class=\"headerlink\" title=\"1. 开始\"></a>1. 开始</h2><h3 id=\"输入输出\"><a href=\"#输入输出\" class=\"headerlink\" title=\"输入输出\"></a>输入输出</h3><p>函数的定义包含四个部分：1. 返回类型；2.函数名；3.一个括号包含的形参类表；4.函数体。</p>\n<p>编写程序之后，我们需要编译它。很多PC机上的编译器都具备集成开发环境（IDE）将编译器于其他程序创建的分析工具包装在一起。</p>\n<p>在C++中包含了一个全面的标准库来提供IO机制，比如<strong>iostream</strong>库。iostream库包含了两个基础类型的istream和ostream，分别表示输入流和输出流。一个流就是一个字符序列，从IO设备里读取或写入IO设备的。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>对象</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>cin</td>\n<td>标准输入</td>\n</tr>\n<tr>\n<td>cout</td>\n<td>标准输出</td>\n</tr>\n<tr>\n<td>cerr</td>\n<td>标准错误</td>\n</tr>\n<tr>\n<td>clog</td>\n<td>一般性信息</td>\n</tr>\n<tr>\n<td>&lt;&lt;</td>\n<td>输出运算符</td>\n</tr>\n<tr>\n<td>&gt;&gt;</td>\n<td>输入运算符</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>程序示例：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    std::cout &lt;&lt; <span class=\"string\">&quot;Enter two numbers:&quot;</span> &lt;&lt; std::endl;</span><br><span class=\"line\">    <span class=\"type\">int</span> v1 = <span class=\"number\">0</span>, v2 = <span class=\"number\">0</span>;</span><br><span class=\"line\">    std::cin &gt;&gt; v1 &gt;&gt;v2;</span><br><span class=\"line\">    std::cout &lt;&lt; <span class=\"string\">&quot;The sum of&quot;</span> &lt;&lt; v1 &lt;&lt; <span class=\"string\">&quot;and&quot;</span> &lt;&lt; v2</span><br><span class=\"line\">              &lt;&lt; <span class=\"string\">&quot;is&quot;</span> &lt;&lt; v1 + v2 &lt;&lt; std:endl;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>程序中的<code>std::</code>指出了名字cout和endl是定义在名为std命名空间中的。</p>\n<p>endl（操纵符），结束当前行，并将与设备关联的缓冲区（buffer）中的内容刷到设备中。</p>\n<h3 id=\"注释\"><a href=\"#注释\" class=\"headerlink\" title=\"注释\"></a>注释</h3><p>//：单行注释</p>\n<p>/**/：多行注释</p>\n<h3 id=\"读取不定的输入数据\"><a href=\"#读取不定的输入数据\" class=\"headerlink\" title=\"读取不定的输入数据\"></a>读取不定的输入数据</h3><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> sum = <span class=\"number\">0</span>,value = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"comment\">//读取数据直到文件尾，计算所有读入的值的和</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span>(std::cin &gt;&gt; value)</span><br><span class=\"line\">        sum += value;</span><br><span class=\"line\">    std::cout &lt;&lt; <span class=\"string\">&quot;Sum is:&quot;</span> &lt;&lt; sum &lt;&lt;std::endl;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-变量和基本类型\"><a href=\"#2-变量和基本类型\" class=\"headerlink\" title=\"2 变量和基本类型\"></a>2 变量和基本类型</h2><h3 id=\"2-5处理类型\"><a href=\"#2-5处理类型\" class=\"headerlink\" title=\"2.5处理类型\"></a>2.5处理类型</h3><h4 id=\"typedef\"><a href=\"#typedef\" class=\"headerlink\" title=\"typedef\"></a><code>typedef</code></h4><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"type\">double</span> wages; <span class=\"comment\">//wages是double的同义词</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> wages base, *p; <span class=\"comment\">//base是double的同义词，p是double* 的同义词</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"别名声明\"><a href=\"#别名声明\" class=\"headerlink\" title=\"别名声明\"></a><code>别名声明</code></h4><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">using</span> SI = Sales_item; <span class=\"comment\">//SI是sales_item的同义词</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"auto类型说明符\"><a href=\"#auto类型说明符\" class=\"headerlink\" title=\"auto类型说明符\"></a><code>auto</code>类型说明符</h4><p>auto类型说明符号的作用是让编译器去分析表达式所属的类型。那么显然，auto定义的变量必须有初始值。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">auto</span> item = val1 + val2;</span><br></pre></td></tr></table></figure>\n<p>如果val1和val2是类Sales_item的对象，则item的类型就是Sales_item；如果这两个变量的类型是double，则auto的类型就是double，以此类推。</p>\n<p>使用auto也能在一条语句上声明多个变量，但条件是所有变量的初始数据类型都一样。</p>\n<h4 id=\"auto的使用\"><a href=\"#auto的使用\" class=\"headerlink\" title=\"auto的使用\"></a>auto的使用</h4><p>首先，我们所熟知的，使用引用其实是使用引用的对象，特别是当引用被用作初始值时，真正参与初始化的其实是引用对象的值。此时编译器以引用对象的类型作为auto的类型：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> i = <span class=\"number\">0</span>, &amp;r = i;</span><br><span class=\"line\"><span class=\"keyword\">auto</span> a = r; <span class=\"comment\">//a是一个整数（r是i的别名，而i是一个整数）</span></span><br></pre></td></tr></table></figure>\n<p>其次，auto一般会忽略掉顶层const，同时底层const则会保留下来，比如当初始值是一个指向常量的指针时：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">const</span> <span class=\"type\">int</span> ci = i,&amp;cr = ci;</span><br><span class=\"line\"><span class=\"keyword\">auto</span> b = ci;\t<span class=\"comment\">//b是一个整数(ci的顶层const特性被忽略掉了)</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> c = cr;\t<span class=\"comment\">//c是一个整数(cr是ci的别名，ci本身是一个顶层const)</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> d = &amp;i;\t<span class=\"comment\">//d是一个指向整形的指针(整数的地址就是指向整数的指针)</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> r = &amp;ci;\t<span class=\"comment\">//e是一个指向整数常量的指针(对常量对象取缔值是一种底层const)</span></span><br></pre></td></tr></table></figure>\n<p>如果希望推断出的auto类型是一个顶层const，需要说明指出：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">const</span> <span class=\"keyword\">auto</span> f = ci;\t<span class=\"comment\">//ci的推演类型是int，f是const int</span></span><br></pre></td></tr></table></figure>\n<p>还可以将引用类型设为auto，此时原来的初始化规则仍然适用：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">auto</span> &amp;g = ci;\t<span class=\"comment\">//g是一个整形常量引用，绑定到ci</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> &amp;h = <span class=\"number\">42</span>;\t<span class=\"comment\">//错误：不能为非常量引用绑定字面值</span></span><br><span class=\"line\"><span class=\"type\">const</span> <span class=\"keyword\">auto</span> &amp;j = <span class=\"number\">42</span>; <span class=\"comment\">//正确：可以为常量引用绑定字面值</span></span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<h1 id=\"C-Primer\"><a href=\"#C-Primer\" class=\"headerlink\" title=\"C++ Primer\"></a>C++ Primer</h1>","more":"<h2 id=\"1-开始\"><a href=\"#1-开始\" class=\"headerlink\" title=\"1. 开始\"></a>1. 开始</h2><h3 id=\"输入输出\"><a href=\"#输入输出\" class=\"headerlink\" title=\"输入输出\"></a>输入输出</h3><p>函数的定义包含四个部分：1. 返回类型；2.函数名；3.一个括号包含的形参类表；4.函数体。</p>\n<p>编写程序之后，我们需要编译它。很多PC机上的编译器都具备集成开发环境（IDE）将编译器于其他程序创建的分析工具包装在一起。</p>\n<p>在C++中包含了一个全面的标准库来提供IO机制，比如<strong>iostream</strong>库。iostream库包含了两个基础类型的istream和ostream，分别表示输入流和输出流。一个流就是一个字符序列，从IO设备里读取或写入IO设备的。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>对象</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>cin</td>\n<td>标准输入</td>\n</tr>\n<tr>\n<td>cout</td>\n<td>标准输出</td>\n</tr>\n<tr>\n<td>cerr</td>\n<td>标准错误</td>\n</tr>\n<tr>\n<td>clog</td>\n<td>一般性信息</td>\n</tr>\n<tr>\n<td>&lt;&lt;</td>\n<td>输出运算符</td>\n</tr>\n<tr>\n<td>&gt;&gt;</td>\n<td>输入运算符</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>程序示例：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    std::cout &lt;&lt; <span class=\"string\">&quot;Enter two numbers:&quot;</span> &lt;&lt; std::endl;</span><br><span class=\"line\">    <span class=\"type\">int</span> v1 = <span class=\"number\">0</span>, v2 = <span class=\"number\">0</span>;</span><br><span class=\"line\">    std::cin &gt;&gt; v1 &gt;&gt;v2;</span><br><span class=\"line\">    std::cout &lt;&lt; <span class=\"string\">&quot;The sum of&quot;</span> &lt;&lt; v1 &lt;&lt; <span class=\"string\">&quot;and&quot;</span> &lt;&lt; v2</span><br><span class=\"line\">              &lt;&lt; <span class=\"string\">&quot;is&quot;</span> &lt;&lt; v1 + v2 &lt;&lt; std:endl;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>程序中的<code>std::</code>指出了名字cout和endl是定义在名为std命名空间中的。</p>\n<p>endl（操纵符），结束当前行，并将与设备关联的缓冲区（buffer）中的内容刷到设备中。</p>\n<h3 id=\"注释\"><a href=\"#注释\" class=\"headerlink\" title=\"注释\"></a>注释</h3><p>//：单行注释</p>\n<p>/**/：多行注释</p>\n<h3 id=\"读取不定的输入数据\"><a href=\"#读取不定的输入数据\" class=\"headerlink\" title=\"读取不定的输入数据\"></a>读取不定的输入数据</h3><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> sum = <span class=\"number\">0</span>,value = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"comment\">//读取数据直到文件尾，计算所有读入的值的和</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span>(std::cin &gt;&gt; value)</span><br><span class=\"line\">        sum += value;</span><br><span class=\"line\">    std::cout &lt;&lt; <span class=\"string\">&quot;Sum is:&quot;</span> &lt;&lt; sum &lt;&lt;std::endl;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-变量和基本类型\"><a href=\"#2-变量和基本类型\" class=\"headerlink\" title=\"2 变量和基本类型\"></a>2 变量和基本类型</h2><h3 id=\"2-5处理类型\"><a href=\"#2-5处理类型\" class=\"headerlink\" title=\"2.5处理类型\"></a>2.5处理类型</h3><h4 id=\"typedef\"><a href=\"#typedef\" class=\"headerlink\" title=\"typedef\"></a><code>typedef</code></h4><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"type\">double</span> wages; <span class=\"comment\">//wages是double的同义词</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> wages base, *p; <span class=\"comment\">//base是double的同义词，p是double* 的同义词</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"别名声明\"><a href=\"#别名声明\" class=\"headerlink\" title=\"别名声明\"></a><code>别名声明</code></h4><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">using</span> SI = Sales_item; <span class=\"comment\">//SI是sales_item的同义词</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"auto类型说明符\"><a href=\"#auto类型说明符\" class=\"headerlink\" title=\"auto类型说明符\"></a><code>auto</code>类型说明符</h4><p>auto类型说明符号的作用是让编译器去分析表达式所属的类型。那么显然，auto定义的变量必须有初始值。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">auto</span> item = val1 + val2;</span><br></pre></td></tr></table></figure>\n<p>如果val1和val2是类Sales_item的对象，则item的类型就是Sales_item；如果这两个变量的类型是double，则auto的类型就是double，以此类推。</p>\n<p>使用auto也能在一条语句上声明多个变量，但条件是所有变量的初始数据类型都一样。</p>\n<h4 id=\"auto的使用\"><a href=\"#auto的使用\" class=\"headerlink\" title=\"auto的使用\"></a>auto的使用</h4><p>首先，我们所熟知的，使用引用其实是使用引用的对象，特别是当引用被用作初始值时，真正参与初始化的其实是引用对象的值。此时编译器以引用对象的类型作为auto的类型：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> i = <span class=\"number\">0</span>, &amp;r = i;</span><br><span class=\"line\"><span class=\"keyword\">auto</span> a = r; <span class=\"comment\">//a是一个整数（r是i的别名，而i是一个整数）</span></span><br></pre></td></tr></table></figure>\n<p>其次，auto一般会忽略掉顶层const，同时底层const则会保留下来，比如当初始值是一个指向常量的指针时：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">const</span> <span class=\"type\">int</span> ci = i,&amp;cr = ci;</span><br><span class=\"line\"><span class=\"keyword\">auto</span> b = ci;\t<span class=\"comment\">//b是一个整数(ci的顶层const特性被忽略掉了)</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> c = cr;\t<span class=\"comment\">//c是一个整数(cr是ci的别名，ci本身是一个顶层const)</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> d = &amp;i;\t<span class=\"comment\">//d是一个指向整形的指针(整数的地址就是指向整数的指针)</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> r = &amp;ci;\t<span class=\"comment\">//e是一个指向整数常量的指针(对常量对象取缔值是一种底层const)</span></span><br></pre></td></tr></table></figure>\n<p>如果希望推断出的auto类型是一个顶层const，需要说明指出：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">const</span> <span class=\"keyword\">auto</span> f = ci;\t<span class=\"comment\">//ci的推演类型是int，f是const int</span></span><br></pre></td></tr></table></figure>\n<p>还可以将引用类型设为auto，此时原来的初始化规则仍然适用：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">auto</span> &amp;g = ci;\t<span class=\"comment\">//g是一个整形常量引用，绑定到ci</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> &amp;h = <span class=\"number\">42</span>;\t<span class=\"comment\">//错误：不能为非常量引用绑定字面值</span></span><br><span class=\"line\"><span class=\"type\">const</span> <span class=\"keyword\">auto</span> &amp;j = <span class=\"number\">42</span>; <span class=\"comment\">//正确：可以为常量引用绑定字面值</span></span><br></pre></td></tr></table></figure>"},{"title":"DeepLearning","date":"2023-08-07T15:19:01.000Z","mathjax":true,"_content":"\n# DeepLearning\n\n关于 dl 一些笔记，或是不懂的问题的记录……:happy:\n\n<!--more-->\n\n\n\n## 多层感知机\n\n\n\n### 1.1 线性模型可能会出错\n\n例如，线性意味着单调假设：任何特征的增大都有可能导致模型输出的增大（如果相应的权重为正）；或导致模型权重的减小（如果相应的权重为负）。\n\n此外，数据的表示可能考虑到特征之间的相关交互作用。在此表示的基础上建立一个线性模型可能会是合适的， 但我们不知道如何手动计算这么一种表示。 对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。\n\n### 1.2 在网络中加入隐藏层\n\n我们可以考虑在网络中加入一个或多个隐藏层来克服线性模型的限制，能使其处理更普遍函数之间的关系。要做到这一点，最简单的方法就是将许多全连接层堆叠到一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前面的$L-1$层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机。\n\n### 1.3 从线性到非线性\n\n我们通过$X \\in R^{n×h}$来表示n个样本的小批量，其中每个样本具有d个输入特征。对于具有$h$个隐藏单元的单层隐藏多层感知机，用$H\\in R^{n*h}$表示隐藏层的输出，称为隐藏表示。在数学或代码中，$H$也被称为隐藏层变量（hidden-layer variable）或隐藏变量（hidden variable）。因为隐藏层和输出层都是全连接的，所以我们具有隐藏层权重$W \\in R^{d×h}$和隐藏层偏置$b^{(1)} \\in R^{1*h}$以及输出层权重$W^{(2)} \\in R^{h×q}$和输出层偏置$b^{(2)} \\in R^{1×q}$。形式上我们按如下方式计算单隐藏层多层感知机的输出$O \\in R^{n×q}$：\n$$\nH = XW^{ (1) } + b^{ (1) }，\nO = HW^{ (2) }  + b^{ (2) },\n\\tag{1}\n$$\n注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。 可我们能从中得到什么好处呢？在上面定义的模型里，我们没有好处！ 原因很简单：上面的隐藏单元由输入的仿射函数给出， 而输出（softmax操作前）只是隐藏单元的仿射函数。 仿射函数的仿射函数本身就是仿射函数， 但是我们之前的线性模型已经能够表示任何仿射函数。\n\n为了发挥多层架构的潜力，我们还需要一个额外的关键因素：在仿射变换之后对每个隐藏单元应用非线性激活函数（activation function）$\\sigma$。激活函数的输出（例如，$\\sigma(.)$）被称为活性值（activation）。一般来说，有了激活函数，就不可能再将我们的多层感知机退化成现行模型：\n$$\nH = \\sigma(XW^{(1)} + b^{ (1) } )\nO = HW^{(2)} + b^{ (2) } \\tag{2}\n$$\n由于$X$中的每一行都对应于小批量中的一个样本，处于记号习惯的考量，我们定义非线性函数$\\sigma$也以按行的方式作用于其输入，即一次计算一个样本。但是本节应用于隐藏层的激活函数通常不按行进行操作，也按元素操作。\n\n这意味着，在计算每一层的线性部分之后，我们可以计算每个活性值，而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。\n\n### 1.4 通用近似定理\n\n多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算。例如，在一对输入上进行基本的逻辑操作，多层感知机是通用近似器。即使网络只有一个隐藏层，给足够的神经元和足够的权重，我们可以对任意函数建模，尽管实际中学习该函数是很困难的神经网络有点像C语言。 C语言和任何其他现代编程语言一样，能够表达任何可计算的程序。 但实际上，想出一个符合规范的程序才是最困难的部分。\n\n而且，虽然一个单隐层网络能学习任何函数， 但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。 我们将在后面的章节中进行更细致的讨论。\n\n### 1.5 激活函数\n\n激活函数（activate function）通过计算加权和并加上偏置来确定神经元是否应该被激活，他们将输入信号转换为输出的可微运算，大多数激活函数都是非线性的。由于激活函数是深度学习的基础，下面介绍一些简单的激活函数。\n\n### 1.6 ReLU函数\n\n最受欢迎的激活函数是线性修正单元，因为它实现简单，同时在各种预测任务中表现良好。ReLU提供了一种非常简单的线性变换。给定元素$x$，ReLU函数被定义为该元素于0的最大值：\n$$\nReLU(x) = max(x,0)\n$$\n通俗地说， ReLU 函数通过将相应的活性值设置为0，仅保留正元素，并丢弃所有负元素。为了直观的感受一下，我们可以画出函数的曲线图。正如从图中所看到，激活函数是分段线性的。\n\n```python\ny = torch.arange(-8.0, 8.0, 0.1, requires_grad = True)\ny = torch.relu(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))\n#返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。\n```\n\n> 这样我们就会继续使用这个新的`tensor进行计算，后面当我们进行`反向传播时，到该调用detach()的`tensor`就会停止，不能再继续向前进行传播。\n>\n> 注意：使用detach()返回的Tensor和原始的tensor共用一个内存，即一个修改另一个也会跟着改变。\n>\n> 当使用detach()分离tensor但是没有更改这个tensor时，并不会影响backward()。\n>\n> 当使用detach()分离tensor，然后用这个分离出来的tensor去求导数，会影响backward()，会出现错误。\n>\n> 当使用detach()分离tensor并且更改这个tensor时，即使再对原来的out求导数，会影响backward()，会出现错误。\n\n![8](/home/xxfs/study/recording/deep_learning/photos/2023-08-14 20-48-23 的屏幕截图.png)\n\n当输入为负数时，ReLU导数为0，当输入为正数时，ReLU函数的导数为1。注意，输入值精确等于0时，ReLU函数不可导。在此时，我们默认使用左边导数，即当输入0的导数为0。我们可以忽略这种情况，因为输入可能永远都不会是0。\n\n```python\ny.backward(torch.ones_like(x), retrain_graph=True)\nd2l.plot(x.detach(), x.grad(), 'x', 'grad of relu', figsize = (5, 2.5))\n```\n\n\n\n下面我们绘制ReLU函数的导数。\n\n![7](/home/xxfs/study/recording/deep_learning/photos/2023-08-14 20-53-33 的屏幕截图.png)下面我们绘制ReLU函数的导数。\n\n使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且**ReLU减轻了困扰以往神经网络的梯度消失问题。**\n\n注意，ReLU函数有很多变体，包括参数化ReLU函数。改变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：\n$$\npReLU(x) = max(0,x) + \\alpha min(0, x)\n$$\n\n### 1.7 sigmoid函数\n\n对一个定义域在$R$上的输入，sigmoid函数将输入变换为区间$(0,1)$上的输出。因此，sigmoid函数通常称为挤压函数：它将范围$(-inf, inf)$中的任意输入压缩到区间$(0,1)$中的某个值：\n$$\nsigmoid(x) = \\frac{1} {1 + exp(-x)}\n$$\n 注意，当输入接近0时，sigmoid函数接近线性变换。\n\n```python\ny = torch.sigmoid(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize = (5, 2.5))\n```\n\nsigmoid的导数为以下公式$\\frac{d} {dx}sigmoid(x) = \\frac{exp(-x)}{ { (1+exp(-x)) }^2} = sigmoid(x)(1-sigmoid(x))$ \n\nsigmoid函数的导数图像如下。注意，当输入为0时，sigmoid函数的导数最大可以达到0.25；而输入在任意方向上越远离0时，导数越接近0。\n\n```python\n# 清除以前的梯度\nx.grad.data.zero_()\ny.backward(torch.ones_like(x),retain_graph=True)\nd2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))\n```\n\n![6](/home/xxfs/study/recording/deep_learning/photos/2023-08-14 21-18-27 的屏幕截图.png)\n\n### 1.8 tanh 函数\n\n与sigmoid函数类似，tanh （双曲正切）函数也能将其输入压缩转换到区间$(-1,1)$上。tanh 函数的公式如下：\n$$\ntanh(x) = \\frac{1 - exp(-2x)}{1 + exp(-2x)}\n$$\n下面我们绘制tanh 函数。注意，当输入在0附近时，tanh函数接近线性变换。函数的形状类似于sigmoid函数，不同的是tanh函数关于坐标系远点中心对称。\n\n```python\ny = torch.tanh(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))\n```\n\n![5](/home/xxfs/study/recording/deep_learning/photos/2023-08-14 21-52-57 的屏幕截图.png)\n\ntanh 的物理导数是：\n$$\n\\frac{d}{dx}tanh(x) = 1 - tanh^2(x)\n$$\ntanh 函数的导数图像如下所示。 当输入接近0时，tanh函数的导数接近最大值1。 与我们在sigmoid函数图像中看到的类似， 输入在任一方向上越远离0点，导数越接近0。\n\n```python\n# 清除以前的梯度\nx.grad.data.zero_()\ny.backward(torch.ones_like(x),retain_graph=True)\nd2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))\n```\n\n![4](/home/xxfs/study/recording/deep_learning/photos/2023-08-14 22-07-56 的屏幕截图.png)\n\n\n\n\n\n## 模型选择，过拟合和欠拟合\n\n### 误差\n\n#### 训练误差\n\n训练误差是指模型在训练集上的错分样本比率，说白了就是在训练集上训练完毕后在训练集本身上进行预测得到了错分率\n\n#### 泛化误差\n\n*泛化误差*（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。\n\n问题是我们永远不能准确的计算出泛化误差。这是因为无限多的数据样本是一个虚构的对象。在实际中，我们只能通过模型应用于一个独立的测试集来估计泛化误差，该测试集由随机选取的，未曾在训练集中出现的样本构成。\n\n泛化误差的意义，其实就是在模型训练后查看模型是否具有代表性。\n\n泛化误差的公式为:$E_G(\\omega) = \\sum_{x \\in X}p(x)(\\hat f (x|\\omega) - f(x))^2$，即全集X中x出现的概率乘以其相对应的训练误差。\n\n但是过分追求低训练误差会使得模型过拟合于训练集反而不使用于其他数据。\n\n因此在样本集划分时，如果得到的训练集与测试集的数据没有交集，此时测试误差基本等同于泛化误差。\n\n\n\n### 系统学习理论\n\n#### 独立同分布\n\n假设训练数据和测试数据都是从相同的分布中独立提取的，这通常被称为独立同分布假设，这意味这对数据进行采样的过程没有进行\"记忆\"。\n\n影响模型泛化的因素：\n\n1. 可调整参数的数量。当可调整参数的数量（有时称为*自由度*）很大时，模型往往更容易过拟合。\n2. 参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。\n3. 训练样本的值。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。\n\n#### 模型选择\n\n在机器学习中，在我们确定所有超参数之前，我们不希望用到测试集。如果我们在模型选择过程中使用测试数据，有可能会过拟合测试数据的风险，那就麻烦大了。如果我们过拟合来训练数据，还可以在测试数据上的评估来判断过拟合。但是如果我们拟合了测试数据集，我们又该怎么知道呢?\n\n因此，我们决不能靠测试数据进行模型的选择。然而，我们也不能依靠训练模型来选择模型，因为我们无法估计训练数据的泛化误差。\n\n在实际应用中，情况变得更加复杂。虽然理想情况下，我们只会使用测试数据一次，以评估最好的模型或比较一些模型的效果，但现实是测试数据很少在使用一次后被丢弃。我们很少能有充足的实验来对每一轮实验才用全新的测试集。\n\n解决此问题的常见做法是将我们的数据分成三份，除了训练集和测试集外，还增加依一个验证数据集，也叫验证集（validation dataset）。但现实是验证数据和测试数据之间模糊地令人担忧。除非另有明确说明，否则在本书的实验中，我们实际上实在使用应该被正确地称为训练数据和验证数据的数据集，并没有真正的测试数据集。因此，文中每次实验报告的准确度都是验证集准确度，而不是测试集准确度。\n\n#### K折交叉验证\n\n当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个适合的验证集。这个问题的一个流行解决方案是采用K折交叉验证。这里，原始训练数据被分成K个不重叠的子集。然后执行K次模型训练和验证，每次在$K-1$个子集上进行训练，并在剩余一个子集（该轮中没有用于训练的子集）进行验证。最后，通过对K次实验的结果取平均值来估计训练和验证的误差。\n\n\n\n### 欠拟合&&过拟合\n\n#### 欠拟合\n\n欠拟合是指模型不能在训练集上获得足够低的误差。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律。\n\n当我们比较训练和验证误差时，我们要注意两种常见的情况。\n\n#### 如何解决欠拟合\n\n欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过**增加网络复杂度**或者在模型中**增加特征**，这些都是很好解决欠拟合的方法。\n\n\n\n#### 过拟合\n\n过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，**模型在训练集上表现很好，但在测试集上却表现很差**。模型对训练集\"死记硬背\"（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，**泛化能力差**。\n\n\n\n#### **为什么会出现过拟合现象？**\n\n造成原因主要有以下几种：\n1、**训练数据集样本单一，样本不足**。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。\n2、**训练数据中噪声干扰过大**。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。\n3、**模型过于复杂。**模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。\n\n\n\n#### 如何防止过拟合？\n\n通过正则化：修改学习算法，使其降低泛化误差而非训练误差。\n\n常用的正则化方法根据具体的使用策略不同可以分为：\n\n1. 直接提供正则化约束的参数正则化方法，如$L1/L2$正则化；\n2. 通过工程上的技巧来实现更低泛化误差的方法，如提前终止（early stopping）和（Drop）\n3. 不直接提供约束的隐式正则化方法，如数据增强等等。\n\n\n\n**1. 获取和使用更多的数据（数据集增强） -----解决过拟合的根本性方法**\n\n让机器学习或深度学习模型泛化能力更好的办法就是使用更多的数据进行训练。但是，在实践中，我们拥有的数据量是有限的。解决这个问题的一种方法就是**创建“假数据”并添加到训练集中——数据集增强**。通过增加训练集的额外副本来增加训练集的大小，进而改进模型的泛化能力。\n\n我们以图像数据集举例，能够做：旋转图像、缩放图像、随机裁剪、加入随机噪声、平移、镜像等方式来增加数据量。另外补充一句，在物体分类问题里，**CNN在图像识别的过程中有强大的“不变性”规则，即待辨识的物体在图像中的形状、姿势、位置、图像整体明暗度都不会影响分类结果**。我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充。\n\n\n\n**2. 采用适合的模型（控制模型的复杂度）**\n\n对于过于复杂的模型会带来过拟合问题1。对于模型的设计，目前公认的一个深度学习的规律是\"deeper is better\"。比如许多大牛通过实验和竞赛发现，对于CNN来说，层数越多，效果越好，但也更容易产生过拟合，并且计算所耗费的时间也越长。\n\n**对于模型的设计而言，我们应该选择简单、合适的模型解决复杂的问题。**\n\n\n\n**3.降低特征的数量**\n\n对于一些特征工程而言，可以降低特征的数量——删除冗余特征，人工选择保留哪些特征。这种方法也可以解决过拟合问题。\n\n\n\n**4. L1/L2正则化**\n\n**(1) L1正则化**\n\n在原始的损失函数后面加上一个L1正则化项\n\n首先，我们要注意这样的情况：\n\n1. 训练误差和验证误差都很严重；\n\n2. 训练误差和验证误差之间仅有一点差距。\n\n如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式。此外，由于我们的训练和验证误差之间的泛化误差很小，我们有理由相信可以用一个更复杂的模型降低训练误差。这种现象被称为欠拟合（underfitting）。\n\n另一方方面，当我们的训练误差明显小于验证误差时要小心，这表明严重的过拟合（overfitting）。 注意，*过拟合*并不总是一件坏事。 特别是在深度学习领域，众所周知， 最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。 最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。\n\n**过拟合或欠拟合的因素：**\n\n1. 模型的复杂性；\n2. 训练数据集的大小。\n\n**模型的复杂性**\n\n为了说明一些关于过拟合和模型复杂性的经典直觉，我们给出一个多项式的例子。给定由单个特征$x$和和对应实数标签$y$组成的训练数据，我们试图找到下面的$d$阶多项式来估计标签$y$。\n$$\n\\hat{y} = \\sum \\limits_{i=0}^d x^i \\omega_i\n$$\n由于这是一个线性回归问题，我们可以用平方误差作为我们的损失函数。\n\n高阶函数比低阶函数复杂得多，高阶函数的参数较多，模型的选择范围较广。因此在固定训练数据集的情况下，高阶多项式函数相对于低阶多项式的的训练误差应该始终更低（最坏也是相等）。事实上，当数据样本包含了$x$的不同值时，函数阶数等于样本数据量的多项式函数可以完美拟合训练集。下图中我们直观描述了过拟合和欠拟合的关系。\n\n![3](/home/xxfs/study/recording/deep_learning/photos/2023-08-18 10-37-40 的屏幕截图.png)\n\n**数据集大小**\n\n另一个重要因素是数据集的大小。 训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。 此外，一般来说，更多的数据不会有什么坏处。 对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系。 给出更多的数据，我们可能会尝试拟合一个更复杂的模型。 能够拟合更复杂的模型可能是有益的。 如果没有足够的数据，简单的模型可能更有用。 对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。 从一定程度上来说，深度学习目前的生机要归功于 廉价存储、互联设备以及数字化经济带来的海量数据集。\n\n\n\n### 多项式回归\n\n我们现在可以通过多项式拟合来探索这些概念。\n\n```python\nimport math\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n```\n\n\n\n#### 生成数据集\n\n给定$x$，我们将使用以下三阶多项式来生成训练和测试数据的标签：\n$$\ny = 5 + 1.2x -3.4\\frac{x^2}{2!} + 5.6\\frac{x^3}{3!} + \\epsilon \\quad where \\quad \\epsilon ～ N(0,0.1^2).\n$$\n 噪声$\\epsilon$服从均值为0，标准差为1的正太分布。在优化的过程中，我们通常希望避免非常大的梯度值或损失值。这就是我们将特征从$x^i$调整为$\\frac{x^i}{i!}$的原因，这样可以避免很大的$i$带来特别大的指数值。我们将训练集和测试集各生成100个样本。\n\n```python\nmax_degree = 20 #多项式的最大阶数\nn_train, n_test = 100 #训练和测试数据集将大小\ntrue_w = np.zeros(max_degree) # 分配大量的空间\ntrue_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\n\nfeatures = np.random.normal(size=(n_train + n_test, 1))\nnp.random.shuffle(features)\npoly_features = np.power(features, np.arange(max_degree).reshape(1,-1))\nfor i in range(max_degree):\n    ploy_features[:,i] /= math.gamma(i+1) #gamma(n) = (n-1)!\n# labels的维度（n_train + n_test,)\nlabels = np.dot(poly_features, true_w)\nlabels += np.random.normal(scale = 0.1, size = labels.shape)\n```\n\n同样，存储在ploy_features中的单项式由gamma函数重新缩放，其中$\\Gamma(n) = (n-1)!$。从生成的数据集中查看一下前两个样本，第一个值是与偏置相对应的常量特征。\n\n```python\n# NumPy ndarray转换为tensor\ntrue_w, features, poly_features, labels = [torch.tensor(x, dtype=\n    torch.float32) for x in [true_w, features, poly_features, labels]]\n\nfeatures[:2], poly_features[:2, :], labels[:2]\n```\n\n#### 对模型进行训练和测试\n\n```python\ndef train(train_features, test_features, train_labels, test_labels\n         num_epochs = 400):\n    loss = nn.MESLoss(reduction='none')\n    input_shape = train_features.shape[-1]\n    #不设置偏置，因为我们已经在多项式中实现了它\n    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\n    batch_size = min(10, train_labels.shape[0])\n    train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),\n                               batch_size)\n    test_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),\n                              batch_size, is_train = False)\n    trainer = torch.optim.SGD(net.parameters(), lr=0.01)\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss', y.scale='log',\n                           xlim=[1,num_epochs], ylim = [1e-3, 1e2],\n                           legend = ['train', 'test'])\n    for epoch in range(num_epochs):\n        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\n        if epoch==0 or (epoch + 1)%20 == 0:\n            animator.add(epoch + 1, (evaluate_loss(net, train_iter,loss),\n                                    evaluate_loss(net, test_iter,loss)))\n            \n    print('weight:', net[0].weight.data.numpy())\n```\n\n\n\n#### 三阶多项式函数拟合\n\n我们将首先使用三阶多项式函数，它与数据生成函数的阶数相同。 结果表明，该模型能有效降低训练损失和测试损失。学习到的模型参数也接近真实值$\\omega=[5, 1.2, -3.4, 5.6]$。\n\n```python\n#从多项式特征中选取前四个维度，即1, x, x^2/2!, x^3/3!\ntrain(poly_features[:n_train, :4], ploy_features[n_train:, :4],\n      labels[:n_train], labels[n_train:])\n```\n\n```python\nweight: [[ 4.993645   1.2287872 -3.3972282  5.559377 ]]\n```\n\n![2](/home/xxfs/study/recording/deep_learning/photos/2023-08-18 14-51-23 的屏幕截图.png)\n\n\n\n#### 线性函数拟合（欠拟合）\n\n让我们再看看线性函数拟合，减少该模型的训练损失相对困难。 在最后一个迭代周期完成后，训练损失仍然很高。 当用来拟合非线性模式（如这里的三阶多项式函数）时，线性模型容易欠拟合。\n\n```python\n# 从多项式特征中选择前2个维度，即1和x\ntrain(poly_features[:n_train, :2], poly_features[n_train:, :2],\n      labels[:n_train], labels[n_train:])\n```\n\n```python\nweight: [[2.5148914 4.2223625]]\n```\n\n![2023-08-18 14-54-34 的屏幕截图](/home/xxfs/study/recording/deep_learning/photos/2023-08-18 14-54-34 的屏幕截图.png)\n\n#### 高阶多项式拟合（过拟合）\n\n现在，让我们尝试使用一个阶数过高的多项式来训练模型。 在这种情况下，没有足够的数据用于学到高阶系数应该具有接近于零的值。 因此，这个过于复杂的模型会轻易受到训练数据中噪声的影响。 虽然训练损失可以有效地降低，但测试损失仍然很高。 结果表明，复杂模型对数据造成了过拟合。\n\n```python\n# 从多项式特征中选取所有维度\ntrain(poly_features[:n_train, :], poly_features[n_train:, :],\n      labels[:n_train], labels[n_train:], num_epochs=1500)\n```\n\n![1](/home/xxfs/study/recording/deep_learning/photos/2023-08-18 14-56-01 的屏幕截图.png)\n\n\n\n\n\n## 权重衰减\n\n### L1/L2正则化和权重衰减\n\nL2范数也被称为欧几里得范数，可以简单理解为向模长。\n\n范数定义的公式如下：\n$$\n||x||_p :=  (\\sum_{i = 1}^{n}|x_i|^p)^{\\frac{1}{p}}\n$$\n\n#### L1范数\n\n$p= 1$时称为$L1$范数(L1-norm)：\n$$\n||x||_1 := \\sum^n_{i = 1}|x_i|\n$$\n$L1$范数是一组数的绝对值累加和。\n\n#### L2范数\n\n$p = 2$时，称为$L2$范数：\n$$\n||x||_2 := (\\sum_{i =1}^n x^{(i)})^{\\frac{1}{2}}\n$$\n可以理解为空间或平面内某一点到原点的距离。\n\n\n\n#### L1/L2正则化和权重衰减\n\n通过在loss上增加了$L1$或$L2$范数项，达到参数惩罚的作用，即实现了正则化效果，从而称为$L1/L2$正则化。\n\n{% asset_img example.jpg This is an example image %}\n\n![2023-09-15 21-17-12 的屏幕截图](/source/images/2023-09-15 21-17-12 的屏幕截图.png)\n\n由于其高次项参数的使用，使得模型对训练数据过分拟合，导致对未来更一般的数据预测性大大下降，为了缓解这种过拟合的现象，我们可以采用L2正则化。 使用$L2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。具体来说就是在原有的损失函数上添加L2正则化项(l2-norm的平方)：\n\n原来的损失：\n$$\nQ(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y} - y)^2\n$$\n加上$L2$正则化项后的损失：\n$$\nJ(\\theta) = Q(x) + \\frac{1}{2n} \\lambda \\sum_{j=1}^{n} \\theta_j^2\n$$\n这里，通过正则化系数$\\lambda$可以较好地惩罚高次项的特征，从而起到降低过拟合，正则化的效果。\n\n添加$L2$正则化修正以后的模型：\n\n![2023-09-15 21-39-05 的屏幕截图](source/images/2023-09-15 21-39-05 的屏幕截图.png)\n\n### 权重衰减\n\n权重衰减weight decay，并不是一个规范的定义，而只是俗称而已，可以理解为削减/惩罚权重。在大多数情况下weight dacay 可以等价为L2正则化。L2正则化的作用就在于削减权重，降低模型过拟合，其行为即直接导致每轮迭代过程中的权重weight参数被削减/惩罚了一部分，故也称为权重衰减weight decay。从这个角度看，不论你用L1正则化还是L2正则化，亦或是其他的正则化方法，只要是削减了权重，那都可以称为weight dacay。从这个角度看，不论你用$L1$正则化还是$L2$正则化，亦或是其他的正则化方法，只要是削减了权重，那都可以称为weight dacay。\n\n设：\n\n- 参数矩阵为p（包括weight和bias）；\n- 模型训练迭代过程中计算出的loss对参数梯度为d_p；\n- 学习率lr；\n- 权重衰减参数为decay\n\n则不设dacay时，迭代时参数的更新过程可以表示为：\n$$\np = p - lr × d\\_p\n$$\n增加weight_dacay参数后更新过程可以表示为：\n$$\np = p - lr × （d\\_p + p × dacay)\n$$\n\n### 代码实现\n\n在深度学习框架的实现中，可以通过设置weight_decay参数，直接对weight矩阵中的数值进行削减（而不是像L2正则一样，通过修改loss函数）起到正则化的参数惩罚作用。二者通过不同方式，同样起到了对权重参数削减/惩罚的作用，实际上在通常的随机梯度下降算法(SGD)中，通过数学计算L2正则化完全可以等价于直接权重衰减。（少数情况除外，譬如使用Adam优化器时，可以参考：[L2正则=Weight Decay？并不是这样](https://zhuanlan.zhihu.com/p/40814046)）\n\n正因如此，深度学习框架通常实现weight dacay/L2正则化的方式很简单，直接指定weight_dacay参数即可。\n\n在pytorch/tensorflow等框架中，我们可以方便地指定weight_dacay参数，来达到正则化的效果，譬如在pytorch的sgd优化器中，直接指定weight_decay = 0.0001：\n\n```python\noptimizer = torch.optim.SGD(net.parameters(), lr=0.001, weight_decay=0.0001)\n```\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n#0.05 + 0.01X + e where e \\in N(0, 0.01^2)\n\nn_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\ntrue_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05\ntrain_data = d2l.synthetic_data(true_w, true_b, n_train)\ntrain_iter = d2l.load_array(train_data, batch_size)\ntest_data = d2l.synthetic_data(true_w, true_b, n_test)\ntest_iter = d2l.load_array(test_data, batch_size, is_train=False)\n\n#定义一个函数来随机初始化参数模型\ndef init_params():\n    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)\n    b = torch.zeros(1, requires_grad=True)\n    return [w,b]\n\ndef l2_penalty(w):\n    return torch.sum(w.pow(2)) / 2\n\n#定义训练代码的实现\ndef train(lambd):\n    w, b = init_params()\n    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n    num_epochs, lr = 100, 0.003\n    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',\n                            xlim=[5, num_epochs], legend=['train', 'test'])\n    for epoch in range(num_epochs):\n        for X, y in train_iter:\n            # 增加了L2范数惩罚项，\n            # 广播机制使l2_penalty(w)成为一个长度为batch_size的向量\n            l = loss(net(X), y) + lambd * l2_penalty(w)\n            l.sum().backward()\n            d2l.sgd([w, b], lr, batch_size)\n        if (epoch + 1) % 5 == 0:\n            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\n                                     d2l.evaluate_loss(net, test_iter, loss)))\n    print('w的L2范数是：', torch.norm(w).item())\n    \n\n#忽略正则化直接进行训练\ntrain(lambd=0)\n\n#使用权重衰减\ntrain(lambd=3)\n```\n\n\n\n### 简洁实现\n\n由于权重衰减在神经网络优化中很常用， 深度学习框架为了便于我们使用权重衰减， 将权重衰减集成到优化算法中，以便与任何损失函数结合使用。 此外，这种集成还有计算上的好处， 允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。 由于更新的权重衰减部分仅依赖于每个参数的当前值， 因此优化器必须至少接触每个参数一次。\n\n```python\ndef train_concise(wd):\n    net = nn.Sequential(nn.Linear(num_inputs, 1))\n    for param in net.parameters():\n        param.data.normal_()\n    loss = nn.MSELoss(reduction='none')\n    num_epochs, lr = 100, 0.003\n    # 偏置参数没有衰减\n    trainer = torch.optim.SGD([\n        {\"params\":net[0].weight,'weight_decay': wd},\n        {\"params\":net[0].bias}], lr=lr)\n    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',\n                            xlim=[5, num_epochs], legend=['train', 'test'])\n    for epoch in range(num_epochs):\n        for X, y in train_iter:\n            trainer.zero_grad()\n            l = loss(net(X), y)\n            l.mean().backward()\n            trainer.step()\n        if (epoch + 1) % 5 == 0:\n            animator.add(epoch + 1,\n                         (d2l.evaluate_loss(net, train_iter, loss),\n                          d2l.evaluate_loss(net, test_iter, loss)))\n    print('w的L2范数：', net[0].weight.norm().item())\n```\n\n\n\n\n\n## 暂退法 Dropout\n\n#### 重新审视过拟合\n\n当面对更多的特征而样本不足时，线性模型往往会过拟合。相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。\n\n泛化性和灵活性之间的权衡被描述为**偏差-方差权衡**。线性模型有很高的偏差：它们只能表示一小类函数。然而，这些模型的方差很低：它们在不同的随机数据样本上可以得出相似的结果。\n\n深度学习网络位于偏差-方差谱的另一端。于线性模型不同，神经网络并不局限于查看每个特征，而是学习特征之间的交互。\n\n在探究泛化之前，我们先来定义以下什么是“好”的预测模型？我们期待好的预测模型能在未知的数据上有很好的表现， 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。\n\n简单性的另一个度量角度是平滑性，即函数不应该对其输入的微小变化而敏感 例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。在2014年，斯里瓦斯塔瓦等人就如何将毕晓普的想法应用于网络的内部层提出了一个想法： 在训练过程中他们建议在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。\n\n这个想法被称为暂退法。暂退法在前向传播过程中，计算每一层内部的同时注入噪音，这已经成为训练神经网络的常用技术。这种方法之所以被称为暂退法，因为我们表面上看是在训练过程中丢弃的一些神经元。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。\n\n需要说明的是，暂退法的原始论文提到了一个关于有性繁殖的类比： 神经网络过拟合与每一层都依赖于前一层激活值相关，称这种情况为“共适应性”。 作者认为，暂退法会破坏共适应性，就像有性生殖会破坏共适应的基因一样。\n\n那么关键的挑战就是如何注入这种噪声。 一种想法是以一种*无偏向*（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。\n\n可以考虑将高斯噪声加入到线性模型的输入中。在没次训练中，他将从均值为零的分布$\\epsilon ～ N（0,\\sigma)$采样噪声添加到输入$x$，从而产生扰动点$x' = x + \\epsilon$，期望是$E[x'] = x$。\n\n在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值ℎ以*暂退概率$p$由随机变量$ℎ′$替换，如下所示：\n$$\nh' = \n\\left\\{\n\\begin{array}{**lr**}  \n0 \\quad 概率为0\n\\\\\n\\frac{h}{1-p} \\quad 其他情况\n\\end{array}  \n\\right.  \n$$\n根据此模型的设计，其期望值保持不变，即$E[x'] = x$。\n\n#### 总结\n\ndropout相当于给出一个概率$p$，比方说$p=40\\%$，那么就是说有$40\\%$的文件要被删除，只留下$60%$的神经元，那么这就是我们的表面理解。对于程序来说，就是将这40%的神经元赋值0，那么可以想一下一个神经元等于0了，那么他对下一层还能产出结果吗，0乘多少权重都是0，相当于这个被dropout选中的神经元没价值了，那他就相当于被删了。\n\n\n\n#### 实践中的暂退法\n\n带有1个隐藏层和5个隐藏单元的多层感知机。 当我们将暂退法应用到隐藏层，以$p$的概率将隐藏单元置为零时， 结果可以看作一个只包含原始神经元子集的网络。假设隐藏单元为$h1,h2,h3,h4,h5$，我们删除了$h2,h5$，因此输出的计算不依赖$h2,h5$并且它们各自的梯度在之执行反向传播也会消失。这样，输出层的计算不能过度依赖$h1,...,h5$中的任意一个元素。\n\n#### 从零开始实现\n\n要实现单层的暂退法函数，我们从均匀分布$U[0,1]$中抽取样本，样本数于这层神经网络的维度一致。然后我们保留那些对应样本大于$p$的节点，把剩下的丢弃。\n\n在下面的代码中，我们实现dropout_layer函数，该函数以dropout的概率丢弃丢弃张量输入X中的元素，如上述重新缩放剩余部分：将剩余部分除以1.0-dropout。\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n\ndef dropout_layer(X, dropout):\n    assert 0 <= dropout <= 1\n    # 在本情况中，所有元素都被丢弃\n    if dropout == 1:\n        return torch.zeros_like(X)\n    # 在本情况中，所有元素都被保留\n    if dropout == 0:\n        return X\n    mask = (torch.rand(X.shape) > dropout).float()\n    return mask * X / (1.0 - dropout)\n```\n\n#### 定义模型参数\n\n引入Fashion-MNIST数据集。我们定义具有两个隐藏层的多层感知机，每个隐藏层包含256个单元。\n\n```python\nnum_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n```\n\n#### 定义模型\n\n我们可以将暂退法应用于每个隐藏层的输出（在激活函数之后）， 并且可以为每一层分别设置暂退概率： 常见的技巧是在靠近输入层的地方设置较低的暂退概率。 下面的模型将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5， 并且暂退法只在训练期间有效。\n\n```python\ndropout1, dropout2 = 0.2, 0.5\n\nclass Net(nn.Module):\n    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,\n                 is_training = True):\n        super(Net, self).__init__()\n        self.num_inputs = num_inputs\n        self.training = is_training\n        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\n        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\n        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\n        # 只有在训练模型时才使用dropout\n        if self.training == True:\n            # 在第一个全连接层之后添加一个dropout层\n            H1 = dropout_layer(H1, dropout1)\n        H2 = self.relu(self.lin2(H1))\n        if self.training == True:\n            # 在第二个全连接层之后添加一个dropout层\n            H2 = dropout_layer(H2, dropout2)\n        out = self.lin3(H2)\n        return out\n\n\nnet = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)\n```\n\n#### 训练和测试\n\n```python\nnum_epochs, lr, batch_size = 10, 0.5, 256\nloss = nn.CrossEntropyLoss(reduction='none')\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n#### 简洁实现\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n\ndef dropout_layer(X, dropout):\n    assert 0 <= dropout <= 1\n    # 在本情况中，所有元素都被丢弃\n    if dropout == 1:\n        return torch.zeros_like(X)\n    # 在本情况中，所有元素都被保留\n    if dropout == 0:\n        return X\n    mask = (torch.rand(X.shape) > dropout).float()\n    return mask * X / (1.0 - dropout)\n\nX= torch.arange(16, dtype = torch.float32).reshape((2, 8))\nprint(X)\nprint(dropout_layer(X, 0.))\nprint(dropout_layer(X, 0.5))\nprint(dropout_layer(X, 1.))\n\nnum_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n\ndropout1, dropout2 = 0.2, 0.5\n\nnet = nn.Sequential(nn.Flatten(),\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        # 在第一个全连接层之后添加一个dropout层\n        nn.Dropout(dropout1),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n        # 在第二个全连接层之后添加一个dropout层\n        nn.Dropout(dropout2),\n        nn.Linear(256, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights);\n\nnum_epochs, lr, batch_size = 10, 0.5, 256\nloss = nn.CrossEntropyLoss()\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\n\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n\n\n## 前向传播、反向传播和计算图\n\n我们已经学习了如何用小批量随机梯度下降训练模型。 然而当实现该算法时，我们只考虑了通过*前向传播*（forward propagation）所涉及的计算。 在计算梯度时，我们只调用了深度学习框架提供的反向传播函数，而不知其所以然。\n\n梯度的自动计算（自动微分）大大简化了深度学习算法的实现。 在自动微分之前，即使是对复杂模型的微小调整也需要手工重新计算复杂的导数， 学术论文也不得不分配大量页面来推导更新规则。 本节将通过一些基本的数学和计算图， 深入探讨*反向传播*的细节。 首先，我们将重点放在带权重衰减（ $L2$ 正则化）的单隐藏层多层感知机上。\n\n#### 前向传播\n\n前向传播指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。\n\n我们将一步一步研究单隐藏层神经网络的机制，为简单起见，我们假设输入样本是$x \\in R^d$，并且我们的隐藏层不包括偏置项。这里的中间变量是：\n$$\nz = W^{(1)}x\n$$\n其中$W^{(1)} \\in R^{h*d}$是隐藏层的权重参数。将中间变量$z \\in R^h$通过激活函数$\\phi$，我们得到长度为$h$的隐藏激活向量：\n$$\nh = \\phi(z)\n$$\n隐藏变量$h$也是一个中间变量。假设输出层的参数只有权重$W^{(2)} \\in R^{q*h}$，我们可以得到输出层的变量，它是一个长度为$q$的向量：\n$$\no = W^{(2)}h\n$$\n假设损失函数为$l$，样本标签为$y$，我们可以单个计算数据样本的损失项，$L = l(o,y)$\n\n根据$L2$正则化的定义，给定超参数$\\lambda$，正则化项为\n$$\ns = \\frac{\\lambda}{2}(||W||_F^2 + ||W||_F^2)\n$$\n其中矩阵的Frobenius范数是将矩阵展平为向量后应用的$L2$范数。最后，模型在给定数据样本上的正则化损失为：\n$$\nJ = L + s\n$$\n在下面讨论中，我们将$J$称为目标函数。\n\n#### 反向传播\n\n反向传播指的是计算神经网络参数梯度的方法。简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。假设我们有函数$Y=f(X)$和$Z = g(X)$，其中输入和输出为$X,Y,Z$是任意形状的张量。利用链式法则，我们可以计算$Z$关于$X$的导数\n$$\n\\frac{\\partial Z}{\\partial L} = prod(\\frac{\\partial Z}{\\partial Y}, \\frac{\\partial{Y}}{\\partial X})\n$$\n这里我们使用prod运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。对于向量，这很简单，它只是矩阵-矩阵乘法。对于高维向量，我们使用适当的对因项。运算符prod代指了所有的这些符号。\n\n回想以下，在计算图中的单隐藏层简单网络的参数是$W^{(1)}$和$W^{(2)}$。反向传播的目的是计算度$\\partial J/\\partial W^{(1)}$和$\\partial J/ \\partial W^{(2)}$。为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。第一步是计算目标函数$J = L + s$相对于损失项L和正则项$s$的梯度。\n$$\n\\frac{\\partial J}{\\partial o}= prod(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial o}) = \\frac{\\partial L}{\\partial o} \\in R^q\n$$\n接下来，我们计算正则化项两个参数的梯度：\n\n$\\frac{\\partial s}{\\partial W^{(1)}} = \\lambda W^{(1)} and \\frac{\\partial s}{\\partial W^{(2)}} = \\lambda W^{(2)}$\n\n现在我们可以计算最接近输出层的模型的梯度$\\frac{\\partial J}{\\partial W^{(2)}} \\in R^{q*h}$。使用链式法则得出：\n$$\n\\frac{\\partial J}{\\partial W^{(2)}} = prod(\\frac{\\partial J}{\\partial o}, \\frac{\\partial o}{\\partial W^{(2)}}) + prod(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial W^{(2)}}) = \\lambda W^{(2)}\n$$\n为了获得关于$W^{(1)}$的梯度，我们需要继续沿着输出层到隐藏层反向传播。关于隐藏层输出的梯度$\\partial J/ \\partial h \\in R^h$由下式给出：\n$$\n\\frac{\\partial J}{\\partial h} = prod(\\frac{\\partial J}{\\partial o}) = W^{(2)^T} \\frac{\\partial J}{\\partial o}\n$$\n由于激活函数$\\phi$是按元素计算的，计算中间变量$z$的梯度$\\partial J/ \\partial z \\in R^n$需要使用按元素乘法运算符，我们用$\\odot$来表示：\n$$\n\\frac{\\partial J}{\\partial z} = prod(\\frac{\\partial J}{\\partial h}, \\frac{\\partial h}{\\partial z}) = \\frac{\\partial J}{\\partial h } \\odot \\phi'(z)\n$$\n最后，我们可以得到最接近输入层的的模型参数的梯度$\\partial J / \\partial W^{(1)} \\in R^{h*d}$。根据链式法则，我们得到：\n$$\n\\frac{\\partial J}{\\partial W^{(1)}} = prod(\\frac{\\partial J}{\\partial z},\\frac{\\partial z}{\\partial W^{(1)}}) + prod(\\frac{\\partial J}{\\partial s}, \\frac{s}{W^{(1)}}) = \\frac{\\partial J}{\\partial z}x^T + \\lambda W^{(1)}\n$$\n\n#### 训练神经网络\n\n在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。\n\n以上述简单网络为例：一方面，在前向传播期间计算正则项取决于模型参数$W^{(1)}$和$W^{(2)}$的当前值。它们是由优化算法根据最近迭代的反向传播给出的。另一方面，反向传播期间参数的梯度计算，取决于由前向传播给出的隐藏层变量$h$的当前值。\n\n因此，在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致*内存不足*（out of memory）错误。\n\n\n\n## 数值稳定和模型初始化\n\n### part1：为什么要用梯度更新\n\n","source":"_posts/DeepLearning.md","raw":"---\ntitle: DeepLearning\ndate: 2023-08-07 23:19:01\ntags:\ncategories:\n- deep learning\nmathjax: true\n---\n\n# DeepLearning\n\n关于 dl 一些笔记，或是不懂的问题的记录……:happy:\n\n<!--more-->\n\n\n\n## 多层感知机\n\n\n\n### 1.1 线性模型可能会出错\n\n例如，线性意味着单调假设：任何特征的增大都有可能导致模型输出的增大（如果相应的权重为正）；或导致模型权重的减小（如果相应的权重为负）。\n\n此外，数据的表示可能考虑到特征之间的相关交互作用。在此表示的基础上建立一个线性模型可能会是合适的， 但我们不知道如何手动计算这么一种表示。 对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。\n\n### 1.2 在网络中加入隐藏层\n\n我们可以考虑在网络中加入一个或多个隐藏层来克服线性模型的限制，能使其处理更普遍函数之间的关系。要做到这一点，最简单的方法就是将许多全连接层堆叠到一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前面的$L-1$层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机。\n\n### 1.3 从线性到非线性\n\n我们通过$X \\in R^{n×h}$来表示n个样本的小批量，其中每个样本具有d个输入特征。对于具有$h$个隐藏单元的单层隐藏多层感知机，用$H\\in R^{n*h}$表示隐藏层的输出，称为隐藏表示。在数学或代码中，$H$也被称为隐藏层变量（hidden-layer variable）或隐藏变量（hidden variable）。因为隐藏层和输出层都是全连接的，所以我们具有隐藏层权重$W \\in R^{d×h}$和隐藏层偏置$b^{(1)} \\in R^{1*h}$以及输出层权重$W^{(2)} \\in R^{h×q}$和输出层偏置$b^{(2)} \\in R^{1×q}$。形式上我们按如下方式计算单隐藏层多层感知机的输出$O \\in R^{n×q}$：\n$$\nH = XW^{ (1) } + b^{ (1) }，\nO = HW^{ (2) }  + b^{ (2) },\n\\tag{1}\n$$\n注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。 可我们能从中得到什么好处呢？在上面定义的模型里，我们没有好处！ 原因很简单：上面的隐藏单元由输入的仿射函数给出， 而输出（softmax操作前）只是隐藏单元的仿射函数。 仿射函数的仿射函数本身就是仿射函数， 但是我们之前的线性模型已经能够表示任何仿射函数。\n\n为了发挥多层架构的潜力，我们还需要一个额外的关键因素：在仿射变换之后对每个隐藏单元应用非线性激活函数（activation function）$\\sigma$。激活函数的输出（例如，$\\sigma(.)$）被称为活性值（activation）。一般来说，有了激活函数，就不可能再将我们的多层感知机退化成现行模型：\n$$\nH = \\sigma(XW^{(1)} + b^{ (1) } )\nO = HW^{(2)} + b^{ (2) } \\tag{2}\n$$\n由于$X$中的每一行都对应于小批量中的一个样本，处于记号习惯的考量，我们定义非线性函数$\\sigma$也以按行的方式作用于其输入，即一次计算一个样本。但是本节应用于隐藏层的激活函数通常不按行进行操作，也按元素操作。\n\n这意味着，在计算每一层的线性部分之后，我们可以计算每个活性值，而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。\n\n### 1.4 通用近似定理\n\n多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算。例如，在一对输入上进行基本的逻辑操作，多层感知机是通用近似器。即使网络只有一个隐藏层，给足够的神经元和足够的权重，我们可以对任意函数建模，尽管实际中学习该函数是很困难的神经网络有点像C语言。 C语言和任何其他现代编程语言一样，能够表达任何可计算的程序。 但实际上，想出一个符合规范的程序才是最困难的部分。\n\n而且，虽然一个单隐层网络能学习任何函数， 但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。 我们将在后面的章节中进行更细致的讨论。\n\n### 1.5 激活函数\n\n激活函数（activate function）通过计算加权和并加上偏置来确定神经元是否应该被激活，他们将输入信号转换为输出的可微运算，大多数激活函数都是非线性的。由于激活函数是深度学习的基础，下面介绍一些简单的激活函数。\n\n### 1.6 ReLU函数\n\n最受欢迎的激活函数是线性修正单元，因为它实现简单，同时在各种预测任务中表现良好。ReLU提供了一种非常简单的线性变换。给定元素$x$，ReLU函数被定义为该元素于0的最大值：\n$$\nReLU(x) = max(x,0)\n$$\n通俗地说， ReLU 函数通过将相应的活性值设置为0，仅保留正元素，并丢弃所有负元素。为了直观的感受一下，我们可以画出函数的曲线图。正如从图中所看到，激活函数是分段线性的。\n\n```python\ny = torch.arange(-8.0, 8.0, 0.1, requires_grad = True)\ny = torch.relu(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))\n#返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。\n```\n\n> 这样我们就会继续使用这个新的`tensor进行计算，后面当我们进行`反向传播时，到该调用detach()的`tensor`就会停止，不能再继续向前进行传播。\n>\n> 注意：使用detach()返回的Tensor和原始的tensor共用一个内存，即一个修改另一个也会跟着改变。\n>\n> 当使用detach()分离tensor但是没有更改这个tensor时，并不会影响backward()。\n>\n> 当使用detach()分离tensor，然后用这个分离出来的tensor去求导数，会影响backward()，会出现错误。\n>\n> 当使用detach()分离tensor并且更改这个tensor时，即使再对原来的out求导数，会影响backward()，会出现错误。\n\n![8](/home/xxfs/study/recording/deep_learning/photos/2023-08-14 20-48-23 的屏幕截图.png)\n\n当输入为负数时，ReLU导数为0，当输入为正数时，ReLU函数的导数为1。注意，输入值精确等于0时，ReLU函数不可导。在此时，我们默认使用左边导数，即当输入0的导数为0。我们可以忽略这种情况，因为输入可能永远都不会是0。\n\n```python\ny.backward(torch.ones_like(x), retrain_graph=True)\nd2l.plot(x.detach(), x.grad(), 'x', 'grad of relu', figsize = (5, 2.5))\n```\n\n\n\n下面我们绘制ReLU函数的导数。\n\n![7](/home/xxfs/study/recording/deep_learning/photos/2023-08-14 20-53-33 的屏幕截图.png)下面我们绘制ReLU函数的导数。\n\n使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且**ReLU减轻了困扰以往神经网络的梯度消失问题。**\n\n注意，ReLU函数有很多变体，包括参数化ReLU函数。改变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：\n$$\npReLU(x) = max(0,x) + \\alpha min(0, x)\n$$\n\n### 1.7 sigmoid函数\n\n对一个定义域在$R$上的输入，sigmoid函数将输入变换为区间$(0,1)$上的输出。因此，sigmoid函数通常称为挤压函数：它将范围$(-inf, inf)$中的任意输入压缩到区间$(0,1)$中的某个值：\n$$\nsigmoid(x) = \\frac{1} {1 + exp(-x)}\n$$\n 注意，当输入接近0时，sigmoid函数接近线性变换。\n\n```python\ny = torch.sigmoid(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize = (5, 2.5))\n```\n\nsigmoid的导数为以下公式$\\frac{d} {dx}sigmoid(x) = \\frac{exp(-x)}{ { (1+exp(-x)) }^2} = sigmoid(x)(1-sigmoid(x))$ \n\nsigmoid函数的导数图像如下。注意，当输入为0时，sigmoid函数的导数最大可以达到0.25；而输入在任意方向上越远离0时，导数越接近0。\n\n```python\n# 清除以前的梯度\nx.grad.data.zero_()\ny.backward(torch.ones_like(x),retain_graph=True)\nd2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))\n```\n\n![6](/home/xxfs/study/recording/deep_learning/photos/2023-08-14 21-18-27 的屏幕截图.png)\n\n### 1.8 tanh 函数\n\n与sigmoid函数类似，tanh （双曲正切）函数也能将其输入压缩转换到区间$(-1,1)$上。tanh 函数的公式如下：\n$$\ntanh(x) = \\frac{1 - exp(-2x)}{1 + exp(-2x)}\n$$\n下面我们绘制tanh 函数。注意，当输入在0附近时，tanh函数接近线性变换。函数的形状类似于sigmoid函数，不同的是tanh函数关于坐标系远点中心对称。\n\n```python\ny = torch.tanh(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))\n```\n\n![5](/home/xxfs/study/recording/deep_learning/photos/2023-08-14 21-52-57 的屏幕截图.png)\n\ntanh 的物理导数是：\n$$\n\\frac{d}{dx}tanh(x) = 1 - tanh^2(x)\n$$\ntanh 函数的导数图像如下所示。 当输入接近0时，tanh函数的导数接近最大值1。 与我们在sigmoid函数图像中看到的类似， 输入在任一方向上越远离0点，导数越接近0。\n\n```python\n# 清除以前的梯度\nx.grad.data.zero_()\ny.backward(torch.ones_like(x),retain_graph=True)\nd2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))\n```\n\n![4](/home/xxfs/study/recording/deep_learning/photos/2023-08-14 22-07-56 的屏幕截图.png)\n\n\n\n\n\n## 模型选择，过拟合和欠拟合\n\n### 误差\n\n#### 训练误差\n\n训练误差是指模型在训练集上的错分样本比率，说白了就是在训练集上训练完毕后在训练集本身上进行预测得到了错分率\n\n#### 泛化误差\n\n*泛化误差*（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。\n\n问题是我们永远不能准确的计算出泛化误差。这是因为无限多的数据样本是一个虚构的对象。在实际中，我们只能通过模型应用于一个独立的测试集来估计泛化误差，该测试集由随机选取的，未曾在训练集中出现的样本构成。\n\n泛化误差的意义，其实就是在模型训练后查看模型是否具有代表性。\n\n泛化误差的公式为:$E_G(\\omega) = \\sum_{x \\in X}p(x)(\\hat f (x|\\omega) - f(x))^2$，即全集X中x出现的概率乘以其相对应的训练误差。\n\n但是过分追求低训练误差会使得模型过拟合于训练集反而不使用于其他数据。\n\n因此在样本集划分时，如果得到的训练集与测试集的数据没有交集，此时测试误差基本等同于泛化误差。\n\n\n\n### 系统学习理论\n\n#### 独立同分布\n\n假设训练数据和测试数据都是从相同的分布中独立提取的，这通常被称为独立同分布假设，这意味这对数据进行采样的过程没有进行\"记忆\"。\n\n影响模型泛化的因素：\n\n1. 可调整参数的数量。当可调整参数的数量（有时称为*自由度*）很大时，模型往往更容易过拟合。\n2. 参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。\n3. 训练样本的值。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。\n\n#### 模型选择\n\n在机器学习中，在我们确定所有超参数之前，我们不希望用到测试集。如果我们在模型选择过程中使用测试数据，有可能会过拟合测试数据的风险，那就麻烦大了。如果我们过拟合来训练数据，还可以在测试数据上的评估来判断过拟合。但是如果我们拟合了测试数据集，我们又该怎么知道呢?\n\n因此，我们决不能靠测试数据进行模型的选择。然而，我们也不能依靠训练模型来选择模型，因为我们无法估计训练数据的泛化误差。\n\n在实际应用中，情况变得更加复杂。虽然理想情况下，我们只会使用测试数据一次，以评估最好的模型或比较一些模型的效果，但现实是测试数据很少在使用一次后被丢弃。我们很少能有充足的实验来对每一轮实验才用全新的测试集。\n\n解决此问题的常见做法是将我们的数据分成三份，除了训练集和测试集外，还增加依一个验证数据集，也叫验证集（validation dataset）。但现实是验证数据和测试数据之间模糊地令人担忧。除非另有明确说明，否则在本书的实验中，我们实际上实在使用应该被正确地称为训练数据和验证数据的数据集，并没有真正的测试数据集。因此，文中每次实验报告的准确度都是验证集准确度，而不是测试集准确度。\n\n#### K折交叉验证\n\n当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个适合的验证集。这个问题的一个流行解决方案是采用K折交叉验证。这里，原始训练数据被分成K个不重叠的子集。然后执行K次模型训练和验证，每次在$K-1$个子集上进行训练，并在剩余一个子集（该轮中没有用于训练的子集）进行验证。最后，通过对K次实验的结果取平均值来估计训练和验证的误差。\n\n\n\n### 欠拟合&&过拟合\n\n#### 欠拟合\n\n欠拟合是指模型不能在训练集上获得足够低的误差。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律。\n\n当我们比较训练和验证误差时，我们要注意两种常见的情况。\n\n#### 如何解决欠拟合\n\n欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过**增加网络复杂度**或者在模型中**增加特征**，这些都是很好解决欠拟合的方法。\n\n\n\n#### 过拟合\n\n过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，**模型在训练集上表现很好，但在测试集上却表现很差**。模型对训练集\"死记硬背\"（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，**泛化能力差**。\n\n\n\n#### **为什么会出现过拟合现象？**\n\n造成原因主要有以下几种：\n1、**训练数据集样本单一，样本不足**。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。\n2、**训练数据中噪声干扰过大**。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。\n3、**模型过于复杂。**模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。\n\n\n\n#### 如何防止过拟合？\n\n通过正则化：修改学习算法，使其降低泛化误差而非训练误差。\n\n常用的正则化方法根据具体的使用策略不同可以分为：\n\n1. 直接提供正则化约束的参数正则化方法，如$L1/L2$正则化；\n2. 通过工程上的技巧来实现更低泛化误差的方法，如提前终止（early stopping）和（Drop）\n3. 不直接提供约束的隐式正则化方法，如数据增强等等。\n\n\n\n**1. 获取和使用更多的数据（数据集增强） -----解决过拟合的根本性方法**\n\n让机器学习或深度学习模型泛化能力更好的办法就是使用更多的数据进行训练。但是，在实践中，我们拥有的数据量是有限的。解决这个问题的一种方法就是**创建“假数据”并添加到训练集中——数据集增强**。通过增加训练集的额外副本来增加训练集的大小，进而改进模型的泛化能力。\n\n我们以图像数据集举例，能够做：旋转图像、缩放图像、随机裁剪、加入随机噪声、平移、镜像等方式来增加数据量。另外补充一句，在物体分类问题里，**CNN在图像识别的过程中有强大的“不变性”规则，即待辨识的物体在图像中的形状、姿势、位置、图像整体明暗度都不会影响分类结果**。我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充。\n\n\n\n**2. 采用适合的模型（控制模型的复杂度）**\n\n对于过于复杂的模型会带来过拟合问题1。对于模型的设计，目前公认的一个深度学习的规律是\"deeper is better\"。比如许多大牛通过实验和竞赛发现，对于CNN来说，层数越多，效果越好，但也更容易产生过拟合，并且计算所耗费的时间也越长。\n\n**对于模型的设计而言，我们应该选择简单、合适的模型解决复杂的问题。**\n\n\n\n**3.降低特征的数量**\n\n对于一些特征工程而言，可以降低特征的数量——删除冗余特征，人工选择保留哪些特征。这种方法也可以解决过拟合问题。\n\n\n\n**4. L1/L2正则化**\n\n**(1) L1正则化**\n\n在原始的损失函数后面加上一个L1正则化项\n\n首先，我们要注意这样的情况：\n\n1. 训练误差和验证误差都很严重；\n\n2. 训练误差和验证误差之间仅有一点差距。\n\n如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式。此外，由于我们的训练和验证误差之间的泛化误差很小，我们有理由相信可以用一个更复杂的模型降低训练误差。这种现象被称为欠拟合（underfitting）。\n\n另一方方面，当我们的训练误差明显小于验证误差时要小心，这表明严重的过拟合（overfitting）。 注意，*过拟合*并不总是一件坏事。 特别是在深度学习领域，众所周知， 最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。 最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。\n\n**过拟合或欠拟合的因素：**\n\n1. 模型的复杂性；\n2. 训练数据集的大小。\n\n**模型的复杂性**\n\n为了说明一些关于过拟合和模型复杂性的经典直觉，我们给出一个多项式的例子。给定由单个特征$x$和和对应实数标签$y$组成的训练数据，我们试图找到下面的$d$阶多项式来估计标签$y$。\n$$\n\\hat{y} = \\sum \\limits_{i=0}^d x^i \\omega_i\n$$\n由于这是一个线性回归问题，我们可以用平方误差作为我们的损失函数。\n\n高阶函数比低阶函数复杂得多，高阶函数的参数较多，模型的选择范围较广。因此在固定训练数据集的情况下，高阶多项式函数相对于低阶多项式的的训练误差应该始终更低（最坏也是相等）。事实上，当数据样本包含了$x$的不同值时，函数阶数等于样本数据量的多项式函数可以完美拟合训练集。下图中我们直观描述了过拟合和欠拟合的关系。\n\n![3](/home/xxfs/study/recording/deep_learning/photos/2023-08-18 10-37-40 的屏幕截图.png)\n\n**数据集大小**\n\n另一个重要因素是数据集的大小。 训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。 此外，一般来说，更多的数据不会有什么坏处。 对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系。 给出更多的数据，我们可能会尝试拟合一个更复杂的模型。 能够拟合更复杂的模型可能是有益的。 如果没有足够的数据，简单的模型可能更有用。 对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。 从一定程度上来说，深度学习目前的生机要归功于 廉价存储、互联设备以及数字化经济带来的海量数据集。\n\n\n\n### 多项式回归\n\n我们现在可以通过多项式拟合来探索这些概念。\n\n```python\nimport math\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n```\n\n\n\n#### 生成数据集\n\n给定$x$，我们将使用以下三阶多项式来生成训练和测试数据的标签：\n$$\ny = 5 + 1.2x -3.4\\frac{x^2}{2!} + 5.6\\frac{x^3}{3!} + \\epsilon \\quad where \\quad \\epsilon ～ N(0,0.1^2).\n$$\n 噪声$\\epsilon$服从均值为0，标准差为1的正太分布。在优化的过程中，我们通常希望避免非常大的梯度值或损失值。这就是我们将特征从$x^i$调整为$\\frac{x^i}{i!}$的原因，这样可以避免很大的$i$带来特别大的指数值。我们将训练集和测试集各生成100个样本。\n\n```python\nmax_degree = 20 #多项式的最大阶数\nn_train, n_test = 100 #训练和测试数据集将大小\ntrue_w = np.zeros(max_degree) # 分配大量的空间\ntrue_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\n\nfeatures = np.random.normal(size=(n_train + n_test, 1))\nnp.random.shuffle(features)\npoly_features = np.power(features, np.arange(max_degree).reshape(1,-1))\nfor i in range(max_degree):\n    ploy_features[:,i] /= math.gamma(i+1) #gamma(n) = (n-1)!\n# labels的维度（n_train + n_test,)\nlabels = np.dot(poly_features, true_w)\nlabels += np.random.normal(scale = 0.1, size = labels.shape)\n```\n\n同样，存储在ploy_features中的单项式由gamma函数重新缩放，其中$\\Gamma(n) = (n-1)!$。从生成的数据集中查看一下前两个样本，第一个值是与偏置相对应的常量特征。\n\n```python\n# NumPy ndarray转换为tensor\ntrue_w, features, poly_features, labels = [torch.tensor(x, dtype=\n    torch.float32) for x in [true_w, features, poly_features, labels]]\n\nfeatures[:2], poly_features[:2, :], labels[:2]\n```\n\n#### 对模型进行训练和测试\n\n```python\ndef train(train_features, test_features, train_labels, test_labels\n         num_epochs = 400):\n    loss = nn.MESLoss(reduction='none')\n    input_shape = train_features.shape[-1]\n    #不设置偏置，因为我们已经在多项式中实现了它\n    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\n    batch_size = min(10, train_labels.shape[0])\n    train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),\n                               batch_size)\n    test_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),\n                              batch_size, is_train = False)\n    trainer = torch.optim.SGD(net.parameters(), lr=0.01)\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss', y.scale='log',\n                           xlim=[1,num_epochs], ylim = [1e-3, 1e2],\n                           legend = ['train', 'test'])\n    for epoch in range(num_epochs):\n        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\n        if epoch==0 or (epoch + 1)%20 == 0:\n            animator.add(epoch + 1, (evaluate_loss(net, train_iter,loss),\n                                    evaluate_loss(net, test_iter,loss)))\n            \n    print('weight:', net[0].weight.data.numpy())\n```\n\n\n\n#### 三阶多项式函数拟合\n\n我们将首先使用三阶多项式函数，它与数据生成函数的阶数相同。 结果表明，该模型能有效降低训练损失和测试损失。学习到的模型参数也接近真实值$\\omega=[5, 1.2, -3.4, 5.6]$。\n\n```python\n#从多项式特征中选取前四个维度，即1, x, x^2/2!, x^3/3!\ntrain(poly_features[:n_train, :4], ploy_features[n_train:, :4],\n      labels[:n_train], labels[n_train:])\n```\n\n```python\nweight: [[ 4.993645   1.2287872 -3.3972282  5.559377 ]]\n```\n\n![2](/home/xxfs/study/recording/deep_learning/photos/2023-08-18 14-51-23 的屏幕截图.png)\n\n\n\n#### 线性函数拟合（欠拟合）\n\n让我们再看看线性函数拟合，减少该模型的训练损失相对困难。 在最后一个迭代周期完成后，训练损失仍然很高。 当用来拟合非线性模式（如这里的三阶多项式函数）时，线性模型容易欠拟合。\n\n```python\n# 从多项式特征中选择前2个维度，即1和x\ntrain(poly_features[:n_train, :2], poly_features[n_train:, :2],\n      labels[:n_train], labels[n_train:])\n```\n\n```python\nweight: [[2.5148914 4.2223625]]\n```\n\n![2023-08-18 14-54-34 的屏幕截图](/home/xxfs/study/recording/deep_learning/photos/2023-08-18 14-54-34 的屏幕截图.png)\n\n#### 高阶多项式拟合（过拟合）\n\n现在，让我们尝试使用一个阶数过高的多项式来训练模型。 在这种情况下，没有足够的数据用于学到高阶系数应该具有接近于零的值。 因此，这个过于复杂的模型会轻易受到训练数据中噪声的影响。 虽然训练损失可以有效地降低，但测试损失仍然很高。 结果表明，复杂模型对数据造成了过拟合。\n\n```python\n# 从多项式特征中选取所有维度\ntrain(poly_features[:n_train, :], poly_features[n_train:, :],\n      labels[:n_train], labels[n_train:], num_epochs=1500)\n```\n\n![1](/home/xxfs/study/recording/deep_learning/photos/2023-08-18 14-56-01 的屏幕截图.png)\n\n\n\n\n\n## 权重衰减\n\n### L1/L2正则化和权重衰减\n\nL2范数也被称为欧几里得范数，可以简单理解为向模长。\n\n范数定义的公式如下：\n$$\n||x||_p :=  (\\sum_{i = 1}^{n}|x_i|^p)^{\\frac{1}{p}}\n$$\n\n#### L1范数\n\n$p= 1$时称为$L1$范数(L1-norm)：\n$$\n||x||_1 := \\sum^n_{i = 1}|x_i|\n$$\n$L1$范数是一组数的绝对值累加和。\n\n#### L2范数\n\n$p = 2$时，称为$L2$范数：\n$$\n||x||_2 := (\\sum_{i =1}^n x^{(i)})^{\\frac{1}{2}}\n$$\n可以理解为空间或平面内某一点到原点的距离。\n\n\n\n#### L1/L2正则化和权重衰减\n\n通过在loss上增加了$L1$或$L2$范数项，达到参数惩罚的作用，即实现了正则化效果，从而称为$L1/L2$正则化。\n\n{% asset_img example.jpg This is an example image %}\n\n![2023-09-15 21-17-12 的屏幕截图](/source/images/2023-09-15 21-17-12 的屏幕截图.png)\n\n由于其高次项参数的使用，使得模型对训练数据过分拟合，导致对未来更一般的数据预测性大大下降，为了缓解这种过拟合的现象，我们可以采用L2正则化。 使用$L2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。具体来说就是在原有的损失函数上添加L2正则化项(l2-norm的平方)：\n\n原来的损失：\n$$\nQ(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y} - y)^2\n$$\n加上$L2$正则化项后的损失：\n$$\nJ(\\theta) = Q(x) + \\frac{1}{2n} \\lambda \\sum_{j=1}^{n} \\theta_j^2\n$$\n这里，通过正则化系数$\\lambda$可以较好地惩罚高次项的特征，从而起到降低过拟合，正则化的效果。\n\n添加$L2$正则化修正以后的模型：\n\n![2023-09-15 21-39-05 的屏幕截图](source/images/2023-09-15 21-39-05 的屏幕截图.png)\n\n### 权重衰减\n\n权重衰减weight decay，并不是一个规范的定义，而只是俗称而已，可以理解为削减/惩罚权重。在大多数情况下weight dacay 可以等价为L2正则化。L2正则化的作用就在于削减权重，降低模型过拟合，其行为即直接导致每轮迭代过程中的权重weight参数被削减/惩罚了一部分，故也称为权重衰减weight decay。从这个角度看，不论你用L1正则化还是L2正则化，亦或是其他的正则化方法，只要是削减了权重，那都可以称为weight dacay。从这个角度看，不论你用$L1$正则化还是$L2$正则化，亦或是其他的正则化方法，只要是削减了权重，那都可以称为weight dacay。\n\n设：\n\n- 参数矩阵为p（包括weight和bias）；\n- 模型训练迭代过程中计算出的loss对参数梯度为d_p；\n- 学习率lr；\n- 权重衰减参数为decay\n\n则不设dacay时，迭代时参数的更新过程可以表示为：\n$$\np = p - lr × d\\_p\n$$\n增加weight_dacay参数后更新过程可以表示为：\n$$\np = p - lr × （d\\_p + p × dacay)\n$$\n\n### 代码实现\n\n在深度学习框架的实现中，可以通过设置weight_decay参数，直接对weight矩阵中的数值进行削减（而不是像L2正则一样，通过修改loss函数）起到正则化的参数惩罚作用。二者通过不同方式，同样起到了对权重参数削减/惩罚的作用，实际上在通常的随机梯度下降算法(SGD)中，通过数学计算L2正则化完全可以等价于直接权重衰减。（少数情况除外，譬如使用Adam优化器时，可以参考：[L2正则=Weight Decay？并不是这样](https://zhuanlan.zhihu.com/p/40814046)）\n\n正因如此，深度学习框架通常实现weight dacay/L2正则化的方式很简单，直接指定weight_dacay参数即可。\n\n在pytorch/tensorflow等框架中，我们可以方便地指定weight_dacay参数，来达到正则化的效果，譬如在pytorch的sgd优化器中，直接指定weight_decay = 0.0001：\n\n```python\noptimizer = torch.optim.SGD(net.parameters(), lr=0.001, weight_decay=0.0001)\n```\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n#0.05 + 0.01X + e where e \\in N(0, 0.01^2)\n\nn_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\ntrue_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05\ntrain_data = d2l.synthetic_data(true_w, true_b, n_train)\ntrain_iter = d2l.load_array(train_data, batch_size)\ntest_data = d2l.synthetic_data(true_w, true_b, n_test)\ntest_iter = d2l.load_array(test_data, batch_size, is_train=False)\n\n#定义一个函数来随机初始化参数模型\ndef init_params():\n    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)\n    b = torch.zeros(1, requires_grad=True)\n    return [w,b]\n\ndef l2_penalty(w):\n    return torch.sum(w.pow(2)) / 2\n\n#定义训练代码的实现\ndef train(lambd):\n    w, b = init_params()\n    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n    num_epochs, lr = 100, 0.003\n    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',\n                            xlim=[5, num_epochs], legend=['train', 'test'])\n    for epoch in range(num_epochs):\n        for X, y in train_iter:\n            # 增加了L2范数惩罚项，\n            # 广播机制使l2_penalty(w)成为一个长度为batch_size的向量\n            l = loss(net(X), y) + lambd * l2_penalty(w)\n            l.sum().backward()\n            d2l.sgd([w, b], lr, batch_size)\n        if (epoch + 1) % 5 == 0:\n            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\n                                     d2l.evaluate_loss(net, test_iter, loss)))\n    print('w的L2范数是：', torch.norm(w).item())\n    \n\n#忽略正则化直接进行训练\ntrain(lambd=0)\n\n#使用权重衰减\ntrain(lambd=3)\n```\n\n\n\n### 简洁实现\n\n由于权重衰减在神经网络优化中很常用， 深度学习框架为了便于我们使用权重衰减， 将权重衰减集成到优化算法中，以便与任何损失函数结合使用。 此外，这种集成还有计算上的好处， 允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。 由于更新的权重衰减部分仅依赖于每个参数的当前值， 因此优化器必须至少接触每个参数一次。\n\n```python\ndef train_concise(wd):\n    net = nn.Sequential(nn.Linear(num_inputs, 1))\n    for param in net.parameters():\n        param.data.normal_()\n    loss = nn.MSELoss(reduction='none')\n    num_epochs, lr = 100, 0.003\n    # 偏置参数没有衰减\n    trainer = torch.optim.SGD([\n        {\"params\":net[0].weight,'weight_decay': wd},\n        {\"params\":net[0].bias}], lr=lr)\n    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',\n                            xlim=[5, num_epochs], legend=['train', 'test'])\n    for epoch in range(num_epochs):\n        for X, y in train_iter:\n            trainer.zero_grad()\n            l = loss(net(X), y)\n            l.mean().backward()\n            trainer.step()\n        if (epoch + 1) % 5 == 0:\n            animator.add(epoch + 1,\n                         (d2l.evaluate_loss(net, train_iter, loss),\n                          d2l.evaluate_loss(net, test_iter, loss)))\n    print('w的L2范数：', net[0].weight.norm().item())\n```\n\n\n\n\n\n## 暂退法 Dropout\n\n#### 重新审视过拟合\n\n当面对更多的特征而样本不足时，线性模型往往会过拟合。相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。\n\n泛化性和灵活性之间的权衡被描述为**偏差-方差权衡**。线性模型有很高的偏差：它们只能表示一小类函数。然而，这些模型的方差很低：它们在不同的随机数据样本上可以得出相似的结果。\n\n深度学习网络位于偏差-方差谱的另一端。于线性模型不同，神经网络并不局限于查看每个特征，而是学习特征之间的交互。\n\n在探究泛化之前，我们先来定义以下什么是“好”的预测模型？我们期待好的预测模型能在未知的数据上有很好的表现， 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。\n\n简单性的另一个度量角度是平滑性，即函数不应该对其输入的微小变化而敏感 例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。在2014年，斯里瓦斯塔瓦等人就如何将毕晓普的想法应用于网络的内部层提出了一个想法： 在训练过程中他们建议在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。\n\n这个想法被称为暂退法。暂退法在前向传播过程中，计算每一层内部的同时注入噪音，这已经成为训练神经网络的常用技术。这种方法之所以被称为暂退法，因为我们表面上看是在训练过程中丢弃的一些神经元。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。\n\n需要说明的是，暂退法的原始论文提到了一个关于有性繁殖的类比： 神经网络过拟合与每一层都依赖于前一层激活值相关，称这种情况为“共适应性”。 作者认为，暂退法会破坏共适应性，就像有性生殖会破坏共适应的基因一样。\n\n那么关键的挑战就是如何注入这种噪声。 一种想法是以一种*无偏向*（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。\n\n可以考虑将高斯噪声加入到线性模型的输入中。在没次训练中，他将从均值为零的分布$\\epsilon ～ N（0,\\sigma)$采样噪声添加到输入$x$，从而产生扰动点$x' = x + \\epsilon$，期望是$E[x'] = x$。\n\n在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值ℎ以*暂退概率$p$由随机变量$ℎ′$替换，如下所示：\n$$\nh' = \n\\left\\{\n\\begin{array}{**lr**}  \n0 \\quad 概率为0\n\\\\\n\\frac{h}{1-p} \\quad 其他情况\n\\end{array}  \n\\right.  \n$$\n根据此模型的设计，其期望值保持不变，即$E[x'] = x$。\n\n#### 总结\n\ndropout相当于给出一个概率$p$，比方说$p=40\\%$，那么就是说有$40\\%$的文件要被删除，只留下$60%$的神经元，那么这就是我们的表面理解。对于程序来说，就是将这40%的神经元赋值0，那么可以想一下一个神经元等于0了，那么他对下一层还能产出结果吗，0乘多少权重都是0，相当于这个被dropout选中的神经元没价值了，那他就相当于被删了。\n\n\n\n#### 实践中的暂退法\n\n带有1个隐藏层和5个隐藏单元的多层感知机。 当我们将暂退法应用到隐藏层，以$p$的概率将隐藏单元置为零时， 结果可以看作一个只包含原始神经元子集的网络。假设隐藏单元为$h1,h2,h3,h4,h5$，我们删除了$h2,h5$，因此输出的计算不依赖$h2,h5$并且它们各自的梯度在之执行反向传播也会消失。这样，输出层的计算不能过度依赖$h1,...,h5$中的任意一个元素。\n\n#### 从零开始实现\n\n要实现单层的暂退法函数，我们从均匀分布$U[0,1]$中抽取样本，样本数于这层神经网络的维度一致。然后我们保留那些对应样本大于$p$的节点，把剩下的丢弃。\n\n在下面的代码中，我们实现dropout_layer函数，该函数以dropout的概率丢弃丢弃张量输入X中的元素，如上述重新缩放剩余部分：将剩余部分除以1.0-dropout。\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n\ndef dropout_layer(X, dropout):\n    assert 0 <= dropout <= 1\n    # 在本情况中，所有元素都被丢弃\n    if dropout == 1:\n        return torch.zeros_like(X)\n    # 在本情况中，所有元素都被保留\n    if dropout == 0:\n        return X\n    mask = (torch.rand(X.shape) > dropout).float()\n    return mask * X / (1.0 - dropout)\n```\n\n#### 定义模型参数\n\n引入Fashion-MNIST数据集。我们定义具有两个隐藏层的多层感知机，每个隐藏层包含256个单元。\n\n```python\nnum_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n```\n\n#### 定义模型\n\n我们可以将暂退法应用于每个隐藏层的输出（在激活函数之后）， 并且可以为每一层分别设置暂退概率： 常见的技巧是在靠近输入层的地方设置较低的暂退概率。 下面的模型将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5， 并且暂退法只在训练期间有效。\n\n```python\ndropout1, dropout2 = 0.2, 0.5\n\nclass Net(nn.Module):\n    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,\n                 is_training = True):\n        super(Net, self).__init__()\n        self.num_inputs = num_inputs\n        self.training = is_training\n        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\n        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\n        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\n        # 只有在训练模型时才使用dropout\n        if self.training == True:\n            # 在第一个全连接层之后添加一个dropout层\n            H1 = dropout_layer(H1, dropout1)\n        H2 = self.relu(self.lin2(H1))\n        if self.training == True:\n            # 在第二个全连接层之后添加一个dropout层\n            H2 = dropout_layer(H2, dropout2)\n        out = self.lin3(H2)\n        return out\n\n\nnet = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)\n```\n\n#### 训练和测试\n\n```python\nnum_epochs, lr, batch_size = 10, 0.5, 256\nloss = nn.CrossEntropyLoss(reduction='none')\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n#### 简洁实现\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n\ndef dropout_layer(X, dropout):\n    assert 0 <= dropout <= 1\n    # 在本情况中，所有元素都被丢弃\n    if dropout == 1:\n        return torch.zeros_like(X)\n    # 在本情况中，所有元素都被保留\n    if dropout == 0:\n        return X\n    mask = (torch.rand(X.shape) > dropout).float()\n    return mask * X / (1.0 - dropout)\n\nX= torch.arange(16, dtype = torch.float32).reshape((2, 8))\nprint(X)\nprint(dropout_layer(X, 0.))\nprint(dropout_layer(X, 0.5))\nprint(dropout_layer(X, 1.))\n\nnum_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n\ndropout1, dropout2 = 0.2, 0.5\n\nnet = nn.Sequential(nn.Flatten(),\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        # 在第一个全连接层之后添加一个dropout层\n        nn.Dropout(dropout1),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n        # 在第二个全连接层之后添加一个dropout层\n        nn.Dropout(dropout2),\n        nn.Linear(256, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights);\n\nnum_epochs, lr, batch_size = 10, 0.5, 256\nloss = nn.CrossEntropyLoss()\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\n\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n\n\n## 前向传播、反向传播和计算图\n\n我们已经学习了如何用小批量随机梯度下降训练模型。 然而当实现该算法时，我们只考虑了通过*前向传播*（forward propagation）所涉及的计算。 在计算梯度时，我们只调用了深度学习框架提供的反向传播函数，而不知其所以然。\n\n梯度的自动计算（自动微分）大大简化了深度学习算法的实现。 在自动微分之前，即使是对复杂模型的微小调整也需要手工重新计算复杂的导数， 学术论文也不得不分配大量页面来推导更新规则。 本节将通过一些基本的数学和计算图， 深入探讨*反向传播*的细节。 首先，我们将重点放在带权重衰减（ $L2$ 正则化）的单隐藏层多层感知机上。\n\n#### 前向传播\n\n前向传播指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。\n\n我们将一步一步研究单隐藏层神经网络的机制，为简单起见，我们假设输入样本是$x \\in R^d$，并且我们的隐藏层不包括偏置项。这里的中间变量是：\n$$\nz = W^{(1)}x\n$$\n其中$W^{(1)} \\in R^{h*d}$是隐藏层的权重参数。将中间变量$z \\in R^h$通过激活函数$\\phi$，我们得到长度为$h$的隐藏激活向量：\n$$\nh = \\phi(z)\n$$\n隐藏变量$h$也是一个中间变量。假设输出层的参数只有权重$W^{(2)} \\in R^{q*h}$，我们可以得到输出层的变量，它是一个长度为$q$的向量：\n$$\no = W^{(2)}h\n$$\n假设损失函数为$l$，样本标签为$y$，我们可以单个计算数据样本的损失项，$L = l(o,y)$\n\n根据$L2$正则化的定义，给定超参数$\\lambda$，正则化项为\n$$\ns = \\frac{\\lambda}{2}(||W||_F^2 + ||W||_F^2)\n$$\n其中矩阵的Frobenius范数是将矩阵展平为向量后应用的$L2$范数。最后，模型在给定数据样本上的正则化损失为：\n$$\nJ = L + s\n$$\n在下面讨论中，我们将$J$称为目标函数。\n\n#### 反向传播\n\n反向传播指的是计算神经网络参数梯度的方法。简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。假设我们有函数$Y=f(X)$和$Z = g(X)$，其中输入和输出为$X,Y,Z$是任意形状的张量。利用链式法则，我们可以计算$Z$关于$X$的导数\n$$\n\\frac{\\partial Z}{\\partial L} = prod(\\frac{\\partial Z}{\\partial Y}, \\frac{\\partial{Y}}{\\partial X})\n$$\n这里我们使用prod运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。对于向量，这很简单，它只是矩阵-矩阵乘法。对于高维向量，我们使用适当的对因项。运算符prod代指了所有的这些符号。\n\n回想以下，在计算图中的单隐藏层简单网络的参数是$W^{(1)}$和$W^{(2)}$。反向传播的目的是计算度$\\partial J/\\partial W^{(1)}$和$\\partial J/ \\partial W^{(2)}$。为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。第一步是计算目标函数$J = L + s$相对于损失项L和正则项$s$的梯度。\n$$\n\\frac{\\partial J}{\\partial o}= prod(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial o}) = \\frac{\\partial L}{\\partial o} \\in R^q\n$$\n接下来，我们计算正则化项两个参数的梯度：\n\n$\\frac{\\partial s}{\\partial W^{(1)}} = \\lambda W^{(1)} and \\frac{\\partial s}{\\partial W^{(2)}} = \\lambda W^{(2)}$\n\n现在我们可以计算最接近输出层的模型的梯度$\\frac{\\partial J}{\\partial W^{(2)}} \\in R^{q*h}$。使用链式法则得出：\n$$\n\\frac{\\partial J}{\\partial W^{(2)}} = prod(\\frac{\\partial J}{\\partial o}, \\frac{\\partial o}{\\partial W^{(2)}}) + prod(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial W^{(2)}}) = \\lambda W^{(2)}\n$$\n为了获得关于$W^{(1)}$的梯度，我们需要继续沿着输出层到隐藏层反向传播。关于隐藏层输出的梯度$\\partial J/ \\partial h \\in R^h$由下式给出：\n$$\n\\frac{\\partial J}{\\partial h} = prod(\\frac{\\partial J}{\\partial o}) = W^{(2)^T} \\frac{\\partial J}{\\partial o}\n$$\n由于激活函数$\\phi$是按元素计算的，计算中间变量$z$的梯度$\\partial J/ \\partial z \\in R^n$需要使用按元素乘法运算符，我们用$\\odot$来表示：\n$$\n\\frac{\\partial J}{\\partial z} = prod(\\frac{\\partial J}{\\partial h}, \\frac{\\partial h}{\\partial z}) = \\frac{\\partial J}{\\partial h } \\odot \\phi'(z)\n$$\n最后，我们可以得到最接近输入层的的模型参数的梯度$\\partial J / \\partial W^{(1)} \\in R^{h*d}$。根据链式法则，我们得到：\n$$\n\\frac{\\partial J}{\\partial W^{(1)}} = prod(\\frac{\\partial J}{\\partial z},\\frac{\\partial z}{\\partial W^{(1)}}) + prod(\\frac{\\partial J}{\\partial s}, \\frac{s}{W^{(1)}}) = \\frac{\\partial J}{\\partial z}x^T + \\lambda W^{(1)}\n$$\n\n#### 训练神经网络\n\n在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。\n\n以上述简单网络为例：一方面，在前向传播期间计算正则项取决于模型参数$W^{(1)}$和$W^{(2)}$的当前值。它们是由优化算法根据最近迭代的反向传播给出的。另一方面，反向传播期间参数的梯度计算，取决于由前向传播给出的隐藏层变量$h$的当前值。\n\n因此，在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致*内存不足*（out of memory）错误。\n\n\n\n## 数值稳定和模型初始化\n\n### part1：为什么要用梯度更新\n\n","slug":"DeepLearning","published":1,"updated":"2023-09-17T15:24:32.033Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clmnlyz6n0003myqb9tgkg2bc","content":"<h1 id=\"DeepLearning\"><a href=\"#DeepLearning\" class=\"headerlink\" title=\"DeepLearning\"></a>DeepLearning</h1><p>关于 dl 一些笔记，或是不懂的问题的记录……:happy:</p>\n<span id=\"more\"></span>\n<h2 id=\"多层感知机\"><a href=\"#多层感知机\" class=\"headerlink\" title=\"多层感知机\"></a>多层感知机</h2><h3 id=\"1-1-线性模型可能会出错\"><a href=\"#1-1-线性模型可能会出错\" class=\"headerlink\" title=\"1.1 线性模型可能会出错\"></a>1.1 线性模型可能会出错</h3><p>例如，线性意味着单调假设：任何特征的增大都有可能导致模型输出的增大（如果相应的权重为正）；或导致模型权重的减小（如果相应的权重为负）。</p>\n<p>此外，数据的表示可能考虑到特征之间的相关交互作用。在此表示的基础上建立一个线性模型可能会是合适的， 但我们不知道如何手动计算这么一种表示。 对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。</p>\n<h3 id=\"1-2-在网络中加入隐藏层\"><a href=\"#1-2-在网络中加入隐藏层\" class=\"headerlink\" title=\"1.2 在网络中加入隐藏层\"></a>1.2 在网络中加入隐藏层</h3><p>我们可以考虑在网络中加入一个或多个隐藏层来克服线性模型的限制，能使其处理更普遍函数之间的关系。要做到这一点，最简单的方法就是将许多全连接层堆叠到一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前面的$L-1$层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机。</p>\n<h3 id=\"1-3-从线性到非线性\"><a href=\"#1-3-从线性到非线性\" class=\"headerlink\" title=\"1.3 从线性到非线性\"></a>1.3 从线性到非线性</h3><p>我们通过$X \\in R^{n×h}$来表示n个样本的小批量，其中每个样本具有d个输入特征。对于具有$h$个隐藏单元的单层隐藏多层感知机，用$H\\in R^{n<em>h}$表示隐藏层的输出，称为隐藏表示。在数学或代码中，$H$也被称为隐藏层变量（hidden-layer variable）或隐藏变量（hidden variable）。因为隐藏层和输出层都是全连接的，所以我们具有隐藏层权重$W \\in R^{d×h}$和隐藏层偏置$b^{(1)} \\in R^{1</em>h}$以及输出层权重$W^{(2)} \\in R^{h×q}$和输出层偏置$b^{(2)} \\in R^{1×q}$。形式上我们按如下方式计算单隐藏层多层感知机的输出$O \\in R^{n×q}$：</p>\n<script type=\"math/tex; mode=display\">\nH = XW^{ (1) } + b^{ (1) }，\nO = HW^{ (2) }  + b^{ (2) },\n\\tag{1}</script><p>注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。 可我们能从中得到什么好处呢？在上面定义的模型里，我们没有好处！ 原因很简单：上面的隐藏单元由输入的仿射函数给出， 而输出（softmax操作前）只是隐藏单元的仿射函数。 仿射函数的仿射函数本身就是仿射函数， 但是我们之前的线性模型已经能够表示任何仿射函数。</p>\n<p>为了发挥多层架构的潜力，我们还需要一个额外的关键因素：在仿射变换之后对每个隐藏单元应用非线性激活函数（activation function）$\\sigma$。激活函数的输出（例如，$\\sigma(.)$）被称为活性值（activation）。一般来说，有了激活函数，就不可能再将我们的多层感知机退化成现行模型：</p>\n<script type=\"math/tex; mode=display\">\nH = \\sigma(XW^{(1)} + b^{ (1) } )\nO = HW^{(2)} + b^{ (2) } \\tag{2}</script><p>由于$X$中的每一行都对应于小批量中的一个样本，处于记号习惯的考量，我们定义非线性函数$\\sigma$也以按行的方式作用于其输入，即一次计算一个样本。但是本节应用于隐藏层的激活函数通常不按行进行操作，也按元素操作。</p>\n<p>这意味着，在计算每一层的线性部分之后，我们可以计算每个活性值，而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。</p>\n<h3 id=\"1-4-通用近似定理\"><a href=\"#1-4-通用近似定理\" class=\"headerlink\" title=\"1.4 通用近似定理\"></a>1.4 通用近似定理</h3><p>多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算。例如，在一对输入上进行基本的逻辑操作，多层感知机是通用近似器。即使网络只有一个隐藏层，给足够的神经元和足够的权重，我们可以对任意函数建模，尽管实际中学习该函数是很困难的神经网络有点像C语言。 C语言和任何其他现代编程语言一样，能够表达任何可计算的程序。 但实际上，想出一个符合规范的程序才是最困难的部分。</p>\n<p>而且，虽然一个单隐层网络能学习任何函数， 但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。 我们将在后面的章节中进行更细致的讨论。</p>\n<h3 id=\"1-5-激活函数\"><a href=\"#1-5-激活函数\" class=\"headerlink\" title=\"1.5 激活函数\"></a>1.5 激活函数</h3><p>激活函数（activate function）通过计算加权和并加上偏置来确定神经元是否应该被激活，他们将输入信号转换为输出的可微运算，大多数激活函数都是非线性的。由于激活函数是深度学习的基础，下面介绍一些简单的激活函数。</p>\n<h3 id=\"1-6-ReLU函数\"><a href=\"#1-6-ReLU函数\" class=\"headerlink\" title=\"1.6 ReLU函数\"></a>1.6 ReLU函数</h3><p>最受欢迎的激活函数是线性修正单元，因为它实现简单，同时在各种预测任务中表现良好。ReLU提供了一种非常简单的线性变换。给定元素$x$，ReLU函数被定义为该元素于0的最大值：</p>\n<script type=\"math/tex; mode=display\">\nReLU(x) = max(x,0)</script><p>通俗地说， ReLU 函数通过将相应的活性值设置为0，仅保留正元素，并丢弃所有负元素。为了直观的感受一下，我们可以画出函数的曲线图。正如从图中所看到，激活函数是分段线性的。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.arange(-<span class=\"number\">8.0</span>, <span class=\"number\">8.0</span>, <span class=\"number\">0.1</span>, requires_grad = <span class=\"literal\">True</span>)</span><br><span class=\"line\">y = torch.relu(x)</span><br><span class=\"line\">d2l.plot(x.detach(), y.detach(), <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;relu(x)&#x27;</span>, figsize=(<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br><span class=\"line\"><span class=\"comment\">#返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>这样我们就会继续使用这个新的<code>tensor进行计算，后面当我们进行</code>反向传播时，到该调用detach()的<code>tensor</code>就会停止，不能再继续向前进行传播。</p>\n<p>注意：使用detach()返回的Tensor和原始的tensor共用一个内存，即一个修改另一个也会跟着改变。</p>\n<p>当使用detach()分离tensor但是没有更改这个tensor时，并不会影响backward()。</p>\n<p>当使用detach()分离tensor，然后用这个分离出来的tensor去求导数，会影响backward()，会出现错误。</p>\n<p>当使用detach()分离tensor并且更改这个tensor时，即使再对原来的out求导数，会影响backward()，会出现错误。</p>\n</blockquote>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-14 20-48-23 的屏幕截图.png\" alt=\"8\"></p>\n<p>当输入为负数时，ReLU导数为0，当输入为正数时，ReLU函数的导数为1。注意，输入值精确等于0时，ReLU函数不可导。在此时，我们默认使用左边导数，即当输入0的导数为0。我们可以忽略这种情况，因为输入可能永远都不会是0。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y.backward(torch.ones_like(x), retrain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\">d2l.plot(x.detach(), x.grad(), <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;grad of relu&#x27;</span>, figsize = (<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n<p>下面我们绘制ReLU函数的导数。</p>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-14 20-53-33 的屏幕截图.png\" alt=\"7\">下面我们绘制ReLU函数的导数。</p>\n<p>使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且<strong>ReLU减轻了困扰以往神经网络的梯度消失问题。</strong></p>\n<p>注意，ReLU函数有很多变体，包括参数化ReLU函数。改变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：</p>\n<script type=\"math/tex; mode=display\">\npReLU(x) = max(0,x) + \\alpha min(0, x)</script><h3 id=\"1-7-sigmoid函数\"><a href=\"#1-7-sigmoid函数\" class=\"headerlink\" title=\"1.7 sigmoid函数\"></a>1.7 sigmoid函数</h3><p>对一个定义域在$R$上的输入，sigmoid函数将输入变换为区间$(0,1)$上的输出。因此，sigmoid函数通常称为挤压函数：它将范围$(-inf, inf)$中的任意输入压缩到区间$(0,1)$中的某个值：</p>\n<script type=\"math/tex; mode=display\">\nsigmoid(x) = \\frac{1} {1 + exp(-x)}</script><p> 注意，当输入接近0时，sigmoid函数接近线性变换。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.sigmoid(x)</span><br><span class=\"line\">d2l.plot(x.detach(), y.detach(), <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;sigmoid(x)&#x27;</span>, figsize = (<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n<p>sigmoid的导数为以下公式$\\frac{d} {dx}sigmoid(x) = \\frac{exp(-x)}{ { (1+exp(-x)) }^2} = sigmoid(x)(1-sigmoid(x))$ </p>\n<p>sigmoid函数的导数图像如下。注意，当输入为0时，sigmoid函数的导数最大可以达到0.25；而输入在任意方向上越远离0时，导数越接近0。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 清除以前的梯度</span></span><br><span class=\"line\">x.grad.data.zero_()</span><br><span class=\"line\">y.backward(torch.ones_like(x),retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\">d2l.plot(x.detach(), x.grad, <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;grad of sigmoid&#x27;</span>, figsize=(<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-14 21-18-27 的屏幕截图.png\" alt=\"6\"></p>\n<h3 id=\"1-8-tanh-函数\"><a href=\"#1-8-tanh-函数\" class=\"headerlink\" title=\"1.8 tanh 函数\"></a>1.8 tanh 函数</h3><p>与sigmoid函数类似，tanh （双曲正切）函数也能将其输入压缩转换到区间$(-1,1)$上。tanh 函数的公式如下：</p>\n<script type=\"math/tex; mode=display\">\ntanh(x) = \\frac{1 - exp(-2x)}{1 + exp(-2x)}</script><p>下面我们绘制tanh 函数。注意，当输入在0附近时，tanh函数接近线性变换。函数的形状类似于sigmoid函数，不同的是tanh函数关于坐标系远点中心对称。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.tanh(x)</span><br><span class=\"line\">d2l.plot(x.detach(), y.detach(), <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;tanh(x)&#x27;</span>, figsize=(<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-14 21-52-57 的屏幕截图.png\" alt=\"5\"></p>\n<p>tanh 的物理导数是：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{d}{dx}tanh(x) = 1 - tanh^2(x)</script><p>tanh 函数的导数图像如下所示。 当输入接近0时，tanh函数的导数接近最大值1。 与我们在sigmoid函数图像中看到的类似， 输入在任一方向上越远离0点，导数越接近0。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 清除以前的梯度</span></span><br><span class=\"line\">x.grad.data.zero_()</span><br><span class=\"line\">y.backward(torch.ones_like(x),retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\">d2l.plot(x.detach(), x.grad, <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;grad of tanh&#x27;</span>, figsize=(<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-14 22-07-56 的屏幕截图.png\" alt=\"4\"></p>\n<h2 id=\"模型选择，过拟合和欠拟合\"><a href=\"#模型选择，过拟合和欠拟合\" class=\"headerlink\" title=\"模型选择，过拟合和欠拟合\"></a>模型选择，过拟合和欠拟合</h2><h3 id=\"误差\"><a href=\"#误差\" class=\"headerlink\" title=\"误差\"></a>误差</h3><h4 id=\"训练误差\"><a href=\"#训练误差\" class=\"headerlink\" title=\"训练误差\"></a>训练误差</h4><p>训练误差是指模型在训练集上的错分样本比率，说白了就是在训练集上训练完毕后在训练集本身上进行预测得到了错分率</p>\n<h4 id=\"泛化误差\"><a href=\"#泛化误差\" class=\"headerlink\" title=\"泛化误差\"></a>泛化误差</h4><p><em>泛化误差</em>（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p>\n<p>问题是我们永远不能准确的计算出泛化误差。这是因为无限多的数据样本是一个虚构的对象。在实际中，我们只能通过模型应用于一个独立的测试集来估计泛化误差，该测试集由随机选取的，未曾在训练集中出现的样本构成。</p>\n<p>泛化误差的意义，其实就是在模型训练后查看模型是否具有代表性。</p>\n<p>泛化误差的公式为:$E_G(\\omega) = \\sum_{x \\in X}p(x)(\\hat f (x|\\omega) - f(x))^2$，即全集X中x出现的概率乘以其相对应的训练误差。</p>\n<p>但是过分追求低训练误差会使得模型过拟合于训练集反而不使用于其他数据。</p>\n<p>因此在样本集划分时，如果得到的训练集与测试集的数据没有交集，此时测试误差基本等同于泛化误差。</p>\n<h3 id=\"系统学习理论\"><a href=\"#系统学习理论\" class=\"headerlink\" title=\"系统学习理论\"></a>系统学习理论</h3><h4 id=\"独立同分布\"><a href=\"#独立同分布\" class=\"headerlink\" title=\"独立同分布\"></a>独立同分布</h4><p>假设训练数据和测试数据都是从相同的分布中独立提取的，这通常被称为独立同分布假设，这意味这对数据进行采样的过程没有进行”记忆”。</p>\n<p>影响模型泛化的因素：</p>\n<ol>\n<li>可调整参数的数量。当可调整参数的数量（有时称为<em>自由度</em>）很大时，模型往往更容易过拟合。</li>\n<li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li>\n<li>训练样本的值。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。</li>\n</ol>\n<h4 id=\"模型选择\"><a href=\"#模型选择\" class=\"headerlink\" title=\"模型选择\"></a>模型选择</h4><p>在机器学习中，在我们确定所有超参数之前，我们不希望用到测试集。如果我们在模型选择过程中使用测试数据，有可能会过拟合测试数据的风险，那就麻烦大了。如果我们过拟合来训练数据，还可以在测试数据上的评估来判断过拟合。但是如果我们拟合了测试数据集，我们又该怎么知道呢?</p>\n<p>因此，我们决不能靠测试数据进行模型的选择。然而，我们也不能依靠训练模型来选择模型，因为我们无法估计训练数据的泛化误差。</p>\n<p>在实际应用中，情况变得更加复杂。虽然理想情况下，我们只会使用测试数据一次，以评估最好的模型或比较一些模型的效果，但现实是测试数据很少在使用一次后被丢弃。我们很少能有充足的实验来对每一轮实验才用全新的测试集。</p>\n<p>解决此问题的常见做法是将我们的数据分成三份，除了训练集和测试集外，还增加依一个验证数据集，也叫验证集（validation dataset）。但现实是验证数据和测试数据之间模糊地令人担忧。除非另有明确说明，否则在本书的实验中，我们实际上实在使用应该被正确地称为训练数据和验证数据的数据集，并没有真正的测试数据集。因此，文中每次实验报告的准确度都是验证集准确度，而不是测试集准确度。</p>\n<h4 id=\"K折交叉验证\"><a href=\"#K折交叉验证\" class=\"headerlink\" title=\"K折交叉验证\"></a>K折交叉验证</h4><p>当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个适合的验证集。这个问题的一个流行解决方案是采用K折交叉验证。这里，原始训练数据被分成K个不重叠的子集。然后执行K次模型训练和验证，每次在$K-1$个子集上进行训练，并在剩余一个子集（该轮中没有用于训练的子集）进行验证。最后，通过对K次实验的结果取平均值来估计训练和验证的误差。</p>\n<h3 id=\"欠拟合-amp-amp-过拟合\"><a href=\"#欠拟合-amp-amp-过拟合\" class=\"headerlink\" title=\"欠拟合&amp;&amp;过拟合\"></a>欠拟合&amp;&amp;过拟合</h3><h4 id=\"欠拟合\"><a href=\"#欠拟合\" class=\"headerlink\" title=\"欠拟合\"></a>欠拟合</h4><p>欠拟合是指模型不能在训练集上获得足够低的误差。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律。</p>\n<p>当我们比较训练和验证误差时，我们要注意两种常见的情况。</p>\n<h4 id=\"如何解决欠拟合\"><a href=\"#如何解决欠拟合\" class=\"headerlink\" title=\"如何解决欠拟合\"></a>如何解决欠拟合</h4><p>欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过<strong>增加网络复杂度</strong>或者在模型中<strong>增加特征</strong>，这些都是很好解决欠拟合的方法。</p>\n<h4 id=\"过拟合\"><a href=\"#过拟合\" class=\"headerlink\" title=\"过拟合\"></a>过拟合</h4><p>过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，<strong>模型在训练集上表现很好，但在测试集上却表现很差</strong>。模型对训练集”死记硬背”（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，<strong>泛化能力差</strong>。</p>\n<h4 id=\"为什么会出现过拟合现象？\"><a href=\"#为什么会出现过拟合现象？\" class=\"headerlink\" title=\"为什么会出现过拟合现象？\"></a><strong>为什么会出现过拟合现象？</strong></h4><p>造成原因主要有以下几种：<br>1、<strong>训练数据集样本单一，样本不足</strong>。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。<br>2、<strong>训练数据中噪声干扰过大</strong>。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。<br>3、<strong>模型过于复杂。</strong>模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。</p>\n<h4 id=\"如何防止过拟合？\"><a href=\"#如何防止过拟合？\" class=\"headerlink\" title=\"如何防止过拟合？\"></a>如何防止过拟合？</h4><p>通过正则化：修改学习算法，使其降低泛化误差而非训练误差。</p>\n<p>常用的正则化方法根据具体的使用策略不同可以分为：</p>\n<ol>\n<li>直接提供正则化约束的参数正则化方法，如$L1/L2$正则化；</li>\n<li>通过工程上的技巧来实现更低泛化误差的方法，如提前终止（early stopping）和（Drop）</li>\n<li>不直接提供约束的隐式正则化方法，如数据增强等等。</li>\n</ol>\n<p><strong>1. 获取和使用更多的数据（数据集增强） ——-解决过拟合的根本性方法</strong></p>\n<p>让机器学习或深度学习模型泛化能力更好的办法就是使用更多的数据进行训练。但是，在实践中，我们拥有的数据量是有限的。解决这个问题的一种方法就是<strong>创建“假数据”并添加到训练集中——数据集增强</strong>。通过增加训练集的额外副本来增加训练集的大小，进而改进模型的泛化能力。</p>\n<p>我们以图像数据集举例，能够做：旋转图像、缩放图像、随机裁剪、加入随机噪声、平移、镜像等方式来增加数据量。另外补充一句，在物体分类问题里，<strong>CNN在图像识别的过程中有强大的“不变性”规则，即待辨识的物体在图像中的形状、姿势、位置、图像整体明暗度都不会影响分类结果</strong>。我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充。</p>\n<p><strong>2. 采用适合的模型（控制模型的复杂度）</strong></p>\n<p>对于过于复杂的模型会带来过拟合问题1。对于模型的设计，目前公认的一个深度学习的规律是”deeper is better”。比如许多大牛通过实验和竞赛发现，对于CNN来说，层数越多，效果越好，但也更容易产生过拟合，并且计算所耗费的时间也越长。</p>\n<p><strong>对于模型的设计而言，我们应该选择简单、合适的模型解决复杂的问题。</strong></p>\n<p><strong>3.降低特征的数量</strong></p>\n<p>对于一些特征工程而言，可以降低特征的数量——删除冗余特征，人工选择保留哪些特征。这种方法也可以解决过拟合问题。</p>\n<p><strong>4. L1/L2正则化</strong></p>\n<p><strong>(1) L1正则化</strong></p>\n<p>在原始的损失函数后面加上一个L1正则化项</p>\n<p>首先，我们要注意这样的情况：</p>\n<ol>\n<li><p>训练误差和验证误差都很严重；</p>\n</li>\n<li><p>训练误差和验证误差之间仅有一点差距。</p>\n</li>\n</ol>\n<p>如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式。此外，由于我们的训练和验证误差之间的泛化误差很小，我们有理由相信可以用一个更复杂的模型降低训练误差。这种现象被称为欠拟合（underfitting）。</p>\n<p>另一方方面，当我们的训练误差明显小于验证误差时要小心，这表明严重的过拟合（overfitting）。 注意，<em>过拟合</em>并不总是一件坏事。 特别是在深度学习领域，众所周知， 最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。 最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。</p>\n<p><strong>过拟合或欠拟合的因素：</strong></p>\n<ol>\n<li>模型的复杂性；</li>\n<li>训练数据集的大小。</li>\n</ol>\n<p><strong>模型的复杂性</strong></p>\n<p>为了说明一些关于过拟合和模型复杂性的经典直觉，我们给出一个多项式的例子。给定由单个特征$x$和和对应实数标签$y$组成的训练数据，我们试图找到下面的$d$阶多项式来估计标签$y$。</p>\n<script type=\"math/tex; mode=display\">\n\\hat{y} = \\sum \\limits_{i=0}^d x^i \\omega_i</script><p>由于这是一个线性回归问题，我们可以用平方误差作为我们的损失函数。</p>\n<p>高阶函数比低阶函数复杂得多，高阶函数的参数较多，模型的选择范围较广。因此在固定训练数据集的情况下，高阶多项式函数相对于低阶多项式的的训练误差应该始终更低（最坏也是相等）。事实上，当数据样本包含了$x$的不同值时，函数阶数等于样本数据量的多项式函数可以完美拟合训练集。下图中我们直观描述了过拟合和欠拟合的关系。</p>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-18 10-37-40 的屏幕截图.png\" alt=\"3\"></p>\n<p><strong>数据集大小</strong></p>\n<p>另一个重要因素是数据集的大小。 训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。 此外，一般来说，更多的数据不会有什么坏处。 对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系。 给出更多的数据，我们可能会尝试拟合一个更复杂的模型。 能够拟合更复杂的模型可能是有益的。 如果没有足够的数据，简单的模型可能更有用。 对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。 从一定程度上来说，深度学习目前的生机要归功于 廉价存储、互联设备以及数字化经济带来的海量数据集。</p>\n<h3 id=\"多项式回归\"><a href=\"#多项式回归\" class=\"headerlink\" title=\"多项式回归\"></a>多项式回归</h3><p>我们现在可以通过多项式拟合来探索这些概念。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n<h4 id=\"生成数据集\"><a href=\"#生成数据集\" class=\"headerlink\" title=\"生成数据集\"></a>生成数据集</h4><p>给定$x$，我们将使用以下三阶多项式来生成训练和测试数据的标签：</p>\n<script type=\"math/tex; mode=display\">\ny = 5 + 1.2x -3.4\\frac{x^2}{2!} + 5.6\\frac{x^3}{3!} + \\epsilon \\quad where \\quad \\epsilon ～ N(0,0.1^2).</script><p> 噪声$\\epsilon$服从均值为0，标准差为1的正太分布。在优化的过程中，我们通常希望避免非常大的梯度值或损失值。这就是我们将特征从$x^i$调整为$\\frac{x^i}{i!}$的原因，这样可以避免很大的$i$带来特别大的指数值。我们将训练集和测试集各生成100个样本。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">max_degree = <span class=\"number\">20</span> <span class=\"comment\">#多项式的最大阶数</span></span><br><span class=\"line\">n_train, n_test = <span class=\"number\">100</span> <span class=\"comment\">#训练和测试数据集将大小</span></span><br><span class=\"line\">true_w = np.zeros(max_degree) <span class=\"comment\"># 分配大量的空间</span></span><br><span class=\"line\">true_w[<span class=\"number\">0</span>:<span class=\"number\">4</span>] = np.array([<span class=\"number\">5</span>, <span class=\"number\">1.2</span>, -<span class=\"number\">3.4</span>, <span class=\"number\">5.6</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">features = np.random.normal(size=(n_train + n_test, <span class=\"number\">1</span>))</span><br><span class=\"line\">np.random.shuffle(features)</span><br><span class=\"line\">poly_features = np.power(features, np.arange(max_degree).reshape(<span class=\"number\">1</span>,-<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_degree):</span><br><span class=\"line\">    ploy_features[:,i] /= math.gamma(i+<span class=\"number\">1</span>) <span class=\"comment\">#gamma(n) = (n-1)!</span></span><br><span class=\"line\"><span class=\"comment\"># labels的维度（n_train + n_test,)</span></span><br><span class=\"line\">labels = np.dot(poly_features, true_w)</span><br><span class=\"line\">labels += np.random.normal(scale = <span class=\"number\">0.1</span>, size = labels.shape)</span><br></pre></td></tr></table></figure>\n<p>同样，存储在ploy_features中的单项式由gamma函数重新缩放，其中$\\Gamma(n) = (n-1)!$。从生成的数据集中查看一下前两个样本，第一个值是与偏置相对应的常量特征。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># NumPy ndarray转换为tensor</span></span><br><span class=\"line\">true_w, features, poly_features, labels = [torch.tensor(x, dtype=</span><br><span class=\"line\">    torch.float32) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> [true_w, features, poly_features, labels]]</span><br><span class=\"line\"></span><br><span class=\"line\">features[:<span class=\"number\">2</span>], poly_features[:<span class=\"number\">2</span>, :], labels[:<span class=\"number\">2</span>]</span><br></pre></td></tr></table></figure>\n<h4 id=\"对模型进行训练和测试\"><a href=\"#对模型进行训练和测试\" class=\"headerlink\" title=\"对模型进行训练和测试\"></a>对模型进行训练和测试</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">train_features, test_features, train_labels, test_labels</span></span><br><span class=\"line\"><span class=\"params\">         num_epochs = <span class=\"number\">400</span></span>):</span><br><span class=\"line\">    loss = nn.MESLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">    input_shape = train_features.shape[-<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"comment\">#不设置偏置，因为我们已经在多项式中实现了它</span></span><br><span class=\"line\">    net = nn.Sequential(nn.Linear(input_shape, <span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>))</span><br><span class=\"line\">    batch_size = <span class=\"built_in\">min</span>(<span class=\"number\">10</span>, train_labels.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">    train_iter = d2l.load_array((train_features, train_labels.reshape(-<span class=\"number\">1</span>,<span class=\"number\">1</span>)),</span><br><span class=\"line\">                               batch_size)</span><br><span class=\"line\">    test_iter = d2l.load_array((test_features, test_labels.reshape(-<span class=\"number\">1</span>,<span class=\"number\">1</span>)),</span><br><span class=\"line\">                              batch_size, is_train = <span class=\"literal\">False</span>)</span><br><span class=\"line\">    trainer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, y.scale=<span class=\"string\">&#x27;log&#x27;</span>,</span><br><span class=\"line\">                           xlim=[<span class=\"number\">1</span>,num_epochs], ylim = [<span class=\"number\">1e-3</span>, <span class=\"number\">1e2</span>],</span><br><span class=\"line\">                           legend = [<span class=\"string\">&#x27;train&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> epoch==<span class=\"number\">0</span> <span class=\"keyword\">or</span> (epoch + <span class=\"number\">1</span>)%<span class=\"number\">20</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            animator.add(epoch + <span class=\"number\">1</span>, (evaluate_loss(net, train_iter,loss),</span><br><span class=\"line\">                                    evaluate_loss(net, test_iter,loss)))</span><br><span class=\"line\">            </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:&#x27;</span>, net[<span class=\"number\">0</span>].weight.data.numpy())</span><br></pre></td></tr></table></figure>\n<h4 id=\"三阶多项式函数拟合\"><a href=\"#三阶多项式函数拟合\" class=\"headerlink\" title=\"三阶多项式函数拟合\"></a>三阶多项式函数拟合</h4><p>我们将首先使用三阶多项式函数，它与数据生成函数的阶数相同。 结果表明，该模型能有效降低训练损失和测试损失。学习到的模型参数也接近真实值$\\omega=[5, 1.2, -3.4, 5.6]$。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#从多项式特征中选取前四个维度，即1, x, x^2/2!, x^3/3!</span></span><br><span class=\"line\">train(poly_features[:n_train, :<span class=\"number\">4</span>], ploy_features[n_train:, :<span class=\"number\">4</span>],</span><br><span class=\"line\">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight: [[ <span class=\"number\">4.993645</span>   <span class=\"number\">1.2287872</span> -<span class=\"number\">3.3972282</span>  <span class=\"number\">5.559377</span> ]]</span><br></pre></td></tr></table></figure>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-18 14-51-23 的屏幕截图.png\" alt=\"2\"></p>\n<h4 id=\"线性函数拟合（欠拟合）\"><a href=\"#线性函数拟合（欠拟合）\" class=\"headerlink\" title=\"线性函数拟合（欠拟合）\"></a>线性函数拟合（欠拟合）</h4><p>让我们再看看线性函数拟合，减少该模型的训练损失相对困难。 在最后一个迭代周期完成后，训练损失仍然很高。 当用来拟合非线性模式（如这里的三阶多项式函数）时，线性模型容易欠拟合。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 从多项式特征中选择前2个维度，即1和x</span></span><br><span class=\"line\">train(poly_features[:n_train, :<span class=\"number\">2</span>], poly_features[n_train:, :<span class=\"number\">2</span>],</span><br><span class=\"line\">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight: [[<span class=\"number\">2.5148914</span> <span class=\"number\">4.2223625</span>]]</span><br></pre></td></tr></table></figure>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-18 14-54-34 的屏幕截图.png\" alt=\"2023-08-18 14-54-34 的屏幕截图\"></p>\n<h4 id=\"高阶多项式拟合（过拟合）\"><a href=\"#高阶多项式拟合（过拟合）\" class=\"headerlink\" title=\"高阶多项式拟合（过拟合）\"></a>高阶多项式拟合（过拟合）</h4><p>现在，让我们尝试使用一个阶数过高的多项式来训练模型。 在这种情况下，没有足够的数据用于学到高阶系数应该具有接近于零的值。 因此，这个过于复杂的模型会轻易受到训练数据中噪声的影响。 虽然训练损失可以有效地降低，但测试损失仍然很高。 结果表明，复杂模型对数据造成了过拟合。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 从多项式特征中选取所有维度</span></span><br><span class=\"line\">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class=\"line\">      labels[:n_train], labels[n_train:], num_epochs=<span class=\"number\">1500</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-18 14-56-01 的屏幕截图.png\" alt=\"1\"></p>\n<h2 id=\"权重衰减\"><a href=\"#权重衰减\" class=\"headerlink\" title=\"权重衰减\"></a>权重衰减</h2><h3 id=\"L1-L2正则化和权重衰减\"><a href=\"#L1-L2正则化和权重衰减\" class=\"headerlink\" title=\"L1/L2正则化和权重衰减\"></a>L1/L2正则化和权重衰减</h3><p>L2范数也被称为欧几里得范数，可以简单理解为向模长。</p>\n<p>范数定义的公式如下：</p>\n<script type=\"math/tex; mode=display\">\n||x||_p :=  (\\sum_{i = 1}^{n}|x_i|^p)^{\\frac{1}{p}}</script><h4 id=\"L1范数\"><a href=\"#L1范数\" class=\"headerlink\" title=\"L1范数\"></a>L1范数</h4><p>$p= 1$时称为$L1$范数(L1-norm)：</p>\n<script type=\"math/tex; mode=display\">\n||x||_1 := \\sum^n_{i = 1}|x_i|</script><p>$L1$范数是一组数的绝对值累加和。</p>\n<h4 id=\"L2范数\"><a href=\"#L2范数\" class=\"headerlink\" title=\"L2范数\"></a>L2范数</h4><p>$p = 2$时，称为$L2$范数：</p>\n<script type=\"math/tex; mode=display\">\n||x||_2 := (\\sum_{i =1}^n x^{(i)})^{\\frac{1}{2}}</script><p>可以理解为空间或平面内某一点到原点的距离。</p>\n<h4 id=\"L1-L2正则化和权重衰减-1\"><a href=\"#L1-L2正则化和权重衰减-1\" class=\"headerlink\" title=\"L1/L2正则化和权重衰减\"></a>L1/L2正则化和权重衰减</h4><p>通过在loss上增加了$L1$或$L2$范数项，达到参数惩罚的作用，即实现了正则化效果，从而称为$L1/L2$正则化。</p>\n\n<p><img src=\"/source/images/2023-09-15 21-17-12 的屏幕截图.png\" alt=\"2023-09-15 21-17-12 的屏幕截图\"></p>\n<p>由于其高次项参数的使用，使得模型对训练数据过分拟合，导致对未来更一般的数据预测性大大下降，为了缓解这种过拟合的现象，我们可以采用L2正则化。 使用$L2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。具体来说就是在原有的损失函数上添加L2正则化项(l2-norm的平方)：</p>\n<p>原来的损失：</p>\n<script type=\"math/tex; mode=display\">\nQ(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y} - y)^2</script><p>加上$L2$正则化项后的损失：</p>\n<script type=\"math/tex; mode=display\">\nJ(\\theta) = Q(x) + \\frac{1}{2n} \\lambda \\sum_{j=1}^{n} \\theta_j^2</script><p>这里，通过正则化系数$\\lambda$可以较好地惩罚高次项的特征，从而起到降低过拟合，正则化的效果。</p>\n<p>添加$L2$正则化修正以后的模型：</p>\n<p><img src=\"source/images/2023-09-15 21-39-05 的屏幕截图.png\" alt=\"2023-09-15 21-39-05 的屏幕截图\"></p>\n<h3 id=\"权重衰减-1\"><a href=\"#权重衰减-1\" class=\"headerlink\" title=\"权重衰减\"></a>权重衰减</h3><p>权重衰减weight decay，并不是一个规范的定义，而只是俗称而已，可以理解为削减/惩罚权重。在大多数情况下weight dacay 可以等价为L2正则化。L2正则化的作用就在于削减权重，降低模型过拟合，其行为即直接导致每轮迭代过程中的权重weight参数被削减/惩罚了一部分，故也称为权重衰减weight decay。从这个角度看，不论你用L1正则化还是L2正则化，亦或是其他的正则化方法，只要是削减了权重，那都可以称为weight dacay。从这个角度看，不论你用$L1$正则化还是$L2$正则化，亦或是其他的正则化方法，只要是削减了权重，那都可以称为weight dacay。</p>\n<p>设：</p>\n<ul>\n<li>参数矩阵为p（包括weight和bias）；</li>\n<li>模型训练迭代过程中计算出的loss对参数梯度为d_p；</li>\n<li>学习率lr；</li>\n<li>权重衰减参数为decay</li>\n</ul>\n<p>则不设dacay时，迭代时参数的更新过程可以表示为：</p>\n<script type=\"math/tex; mode=display\">\np = p - lr × d\\_p</script><p>增加weight_dacay参数后更新过程可以表示为：</p>\n<script type=\"math/tex; mode=display\">\np = p - lr × （d\\_p + p × dacay)</script><h3 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h3><p>在深度学习框架的实现中，可以通过设置weight_decay参数，直接对weight矩阵中的数值进行削减（而不是像L2正则一样，通过修改loss函数）起到正则化的参数惩罚作用。二者通过不同方式，同样起到了对权重参数削减/惩罚的作用，实际上在通常的随机梯度下降算法(SGD)中，通过数学计算L2正则化完全可以等价于直接权重衰减。（少数情况除外，譬如使用Adam优化器时，可以参考：<a href=\"https://zhuanlan.zhihu.com/p/40814046\">L2正则=Weight Decay？并不是这样</a>）</p>\n<p>正因如此，深度学习框架通常实现weight dacay/L2正则化的方式很简单，直接指定weight_dacay参数即可。</p>\n<p>在pytorch/tensorflow等框架中，我们可以方便地指定weight_dacay参数，来达到正则化的效果，譬如在pytorch的sgd优化器中，直接指定weight_decay = 0.0001：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">optimizer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.001</span>, weight_decay=<span class=\"number\">0.0001</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#0.05 + 0.01X + e where e \\in N(0, 0.01^2)</span></span><br><span class=\"line\"></span><br><span class=\"line\">n_train, n_test, num_inputs, batch_size = <span class=\"number\">20</span>, <span class=\"number\">100</span>, <span class=\"number\">200</span>, <span class=\"number\">5</span></span><br><span class=\"line\">true_w, true_b = torch.ones((num_inputs, <span class=\"number\">1</span>)) * <span class=\"number\">0.01</span>, <span class=\"number\">0.05</span></span><br><span class=\"line\">train_data = d2l.synthetic_data(true_w, true_b, n_train)</span><br><span class=\"line\">train_iter = d2l.load_array(train_data, batch_size)</span><br><span class=\"line\">test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class=\"line\">test_iter = d2l.load_array(test_data, batch_size, is_train=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#定义一个函数来随机初始化参数模型</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_params</span>():</span><br><span class=\"line\">    w = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, size=(num_inputs, <span class=\"number\">1</span>), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    b = torch.zeros(<span class=\"number\">1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [w,b]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">l2_penalty</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">sum</span>(w.<span class=\"built_in\">pow</span>(<span class=\"number\">2</span>)) / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#定义训练代码的实现</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">lambd</span>):</span><br><span class=\"line\">    w, b = init_params()</span><br><span class=\"line\">    net, loss = <span class=\"keyword\">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss</span><br><span class=\"line\">    num_epochs, lr = <span class=\"number\">100</span>, <span class=\"number\">0.003</span></span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epochs&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>,</span><br><span class=\"line\">                            xlim=[<span class=\"number\">5</span>, num_epochs], legend=[<span class=\"string\">&#x27;train&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">            <span class=\"comment\"># 增加了L2范数惩罚项，</span></span><br><span class=\"line\">            <span class=\"comment\"># 广播机制使l2_penalty(w)成为一个长度为batch_size的向量</span></span><br><span class=\"line\">            l = loss(net(X), y) + lambd * l2_penalty(w)</span><br><span class=\"line\">            l.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">            d2l.sgd([w, b], lr, batch_size)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">5</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            animator.add(epoch + <span class=\"number\">1</span>, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class=\"line\">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;w的L2范数是：&#x27;</span>, torch.norm(w).item())</span><br><span class=\"line\">    </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#忽略正则化直接进行训练</span></span><br><span class=\"line\">train(lambd=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#使用权重衰减</span></span><br><span class=\"line\">train(lambd=<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"简洁实现\"><a href=\"#简洁实现\" class=\"headerlink\" title=\"简洁实现\"></a>简洁实现</h3><p>由于权重衰减在神经网络优化中很常用， 深度学习框架为了便于我们使用权重衰减， 将权重衰减集成到优化算法中，以便与任何损失函数结合使用。 此外，这种集成还有计算上的好处， 允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。 由于更新的权重衰减部分仅依赖于每个参数的当前值， 因此优化器必须至少接触每个参数一次。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_concise</span>(<span class=\"params\">wd</span>):</span><br><span class=\"line\">    net = nn.Sequential(nn.Linear(num_inputs, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> net.parameters():</span><br><span class=\"line\">        param.data.normal_()</span><br><span class=\"line\">    loss = nn.MSELoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">    num_epochs, lr = <span class=\"number\">100</span>, <span class=\"number\">0.003</span></span><br><span class=\"line\">    <span class=\"comment\"># 偏置参数没有衰减</span></span><br><span class=\"line\">    trainer = torch.optim.SGD([</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;params&quot;</span>:net[<span class=\"number\">0</span>].weight,<span class=\"string\">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;params&quot;</span>:net[<span class=\"number\">0</span>].bias&#125;], lr=lr)</span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epochs&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>,</span><br><span class=\"line\">                            xlim=[<span class=\"number\">5</span>, num_epochs], legend=[<span class=\"string\">&#x27;train&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">            trainer.zero_grad()</span><br><span class=\"line\">            l = loss(net(X), y)</span><br><span class=\"line\">            l.mean().backward()</span><br><span class=\"line\">            trainer.step()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">5</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            animator.add(epoch + <span class=\"number\">1</span>,</span><br><span class=\"line\">                         (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class=\"line\">                          d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;w的L2范数：&#x27;</span>, net[<span class=\"number\">0</span>].weight.norm().item())</span><br></pre></td></tr></table></figure>\n<h2 id=\"暂退法-Dropout\"><a href=\"#暂退法-Dropout\" class=\"headerlink\" title=\"暂退法 Dropout\"></a>暂退法 Dropout</h2><h4 id=\"重新审视过拟合\"><a href=\"#重新审视过拟合\" class=\"headerlink\" title=\"重新审视过拟合\"></a>重新审视过拟合</h4><p>当面对更多的特征而样本不足时，线性模型往往会过拟合。相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。</p>\n<p>泛化性和灵活性之间的权衡被描述为<strong>偏差-方差权衡</strong>。线性模型有很高的偏差：它们只能表示一小类函数。然而，这些模型的方差很低：它们在不同的随机数据样本上可以得出相似的结果。</p>\n<p>深度学习网络位于偏差-方差谱的另一端。于线性模型不同，神经网络并不局限于查看每个特征，而是学习特征之间的交互。</p>\n<p>在探究泛化之前，我们先来定义以下什么是“好”的预测模型？我们期待好的预测模型能在未知的数据上有很好的表现， 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。</p>\n<p>简单性的另一个度量角度是平滑性，即函数不应该对其输入的微小变化而敏感 例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。在2014年，斯里瓦斯塔瓦等人就如何将毕晓普的想法应用于网络的内部层提出了一个想法： 在训练过程中他们建议在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。</p>\n<p>这个想法被称为暂退法。暂退法在前向传播过程中，计算每一层内部的同时注入噪音，这已经成为训练神经网络的常用技术。这种方法之所以被称为暂退法，因为我们表面上看是在训练过程中丢弃的一些神经元。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。</p>\n<p>需要说明的是，暂退法的原始论文提到了一个关于有性繁殖的类比： 神经网络过拟合与每一层都依赖于前一层激活值相关，称这种情况为“共适应性”。 作者认为，暂退法会破坏共适应性，就像有性生殖会破坏共适应的基因一样。</p>\n<p>那么关键的挑战就是如何注入这种噪声。 一种想法是以一种<em>无偏向</em>（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。</p>\n<p>可以考虑将高斯噪声加入到线性模型的输入中。在没次训练中，他将从均值为零的分布$\\epsilon ～ N（0,\\sigma)$采样噪声添加到输入$x$，从而产生扰动点$x’ = x + \\epsilon$，期望是$E[x’] = x$。</p>\n<p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值ℎ以*暂退概率$p$由随机变量$ℎ′$替换，如下所示：</p>\n<script type=\"math/tex; mode=display\">\nh' = \n\\left\\{\n\\begin{array}{**lr**}  \n0 \\quad 概率为0\n\\\\\n\\frac{h}{1-p} \\quad 其他情况\n\\end{array}  \n\\right.</script><p>根据此模型的设计，其期望值保持不变，即$E[x’] = x$。</p>\n<h4 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h4><p>dropout相当于给出一个概率$p$，比方说$p=40\\%$，那么就是说有$40\\%$的文件要被删除，只留下$60%$的神经元，那么这就是我们的表面理解。对于程序来说，就是将这40%的神经元赋值0，那么可以想一下一个神经元等于0了，那么他对下一层还能产出结果吗，0乘多少权重都是0，相当于这个被dropout选中的神经元没价值了，那他就相当于被删了。</p>\n<h4 id=\"实践中的暂退法\"><a href=\"#实践中的暂退法\" class=\"headerlink\" title=\"实践中的暂退法\"></a>实践中的暂退法</h4><p>带有1个隐藏层和5个隐藏单元的多层感知机。 当我们将暂退法应用到隐藏层，以$p$的概率将隐藏单元置为零时， 结果可以看作一个只包含原始神经元子集的网络。假设隐藏单元为$h1,h2,h3,h4,h5$，我们删除了$h2,h5$，因此输出的计算不依赖$h2,h5$并且它们各自的梯度在之执行反向传播也会消失。这样，输出层的计算不能过度依赖$h1,…,h5$中的任意一个元素。</p>\n<h4 id=\"从零开始实现\"><a href=\"#从零开始实现\" class=\"headerlink\" title=\"从零开始实现\"></a>从零开始实现</h4><p>要实现单层的暂退法函数，我们从均匀分布$U[0,1]$中抽取样本，样本数于这层神经网络的维度一致。然后我们保留那些对应样本大于$p$的节点，把剩下的丢弃。</p>\n<p>在下面的代码中，我们实现dropout_layer函数，该函数以dropout的概率丢弃丢弃张量输入X中的元素，如上述重新缩放剩余部分：将剩余部分除以1.0-dropout。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dropout_layer</span>(<span class=\"params\">X, dropout</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">0</span> &lt;= dropout &lt;= <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># 在本情况中，所有元素都被丢弃</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout == <span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.zeros_like(X)</span><br><span class=\"line\">    <span class=\"comment\"># 在本情况中，所有元素都被保留</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br><span class=\"line\">    mask = (torch.rand(X.shape) &gt; dropout).<span class=\"built_in\">float</span>()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask * X / (<span class=\"number\">1.0</span> - dropout)</span><br></pre></td></tr></table></figure>\n<h4 id=\"定义模型参数\"><a href=\"#定义模型参数\" class=\"headerlink\" title=\"定义模型参数\"></a>定义模型参数</h4><p>引入Fashion-MNIST数据集。我们定义具有两个隐藏层的多层感知机，每个隐藏层包含256个单元。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class=\"number\">784</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span>, <span class=\"number\">256</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"定义模型\"><a href=\"#定义模型\" class=\"headerlink\" title=\"定义模型\"></a>定义模型</h4><p>我们可以将暂退法应用于每个隐藏层的输出（在激活函数之后）， 并且可以为每一层分别设置暂退概率： 常见的技巧是在靠近输入层的地方设置较低的暂退概率。 下面的模型将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5， 并且暂退法只在训练期间有效。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dropout1, dropout2 = <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Net</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span></span><br><span class=\"line\"><span class=\"params\">                 is_training = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Net, self).__init__()</span><br><span class=\"line\">        self.num_inputs = num_inputs</span><br><span class=\"line\">        self.training = is_training</span><br><span class=\"line\">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class=\"line\">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class=\"line\">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class=\"line\">        self.relu = nn.ReLU()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        H1 = self.relu(self.lin1(X.reshape((-<span class=\"number\">1</span>, self.num_inputs))))</span><br><span class=\"line\">        <span class=\"comment\"># 只有在训练模型时才使用dropout</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.training == <span class=\"literal\">True</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">            H1 = dropout_layer(H1, dropout1)</span><br><span class=\"line\">        H2 = self.relu(self.lin2(H1))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.training == <span class=\"literal\">True</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">            H2 = dropout_layer(H2, dropout2)</span><br><span class=\"line\">        out = self.lin3(H2)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br></pre></td></tr></table></figure>\n<h4 id=\"训练和测试\"><a href=\"#训练和测试\" class=\"headerlink\" title=\"训练和测试\"></a>训练和测试</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, lr, batch_size = <span class=\"number\">10</span>, <span class=\"number\">0.5</span>, <span class=\"number\">256</span></span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>\n<h4 id=\"简洁实现-1\"><a href=\"#简洁实现-1\" class=\"headerlink\" title=\"简洁实现\"></a>简洁实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dropout_layer</span>(<span class=\"params\">X, dropout</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">0</span> &lt;= dropout &lt;= <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># 在本情况中，所有元素都被丢弃</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout == <span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.zeros_like(X)</span><br><span class=\"line\">    <span class=\"comment\"># 在本情况中，所有元素都被保留</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br><span class=\"line\">    mask = (torch.rand(X.shape) &gt; dropout).<span class=\"built_in\">float</span>()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask * X / (<span class=\"number\">1.0</span> - dropout)</span><br><span class=\"line\"></span><br><span class=\"line\">X= torch.arange(<span class=\"number\">16</span>, dtype = torch.float32).reshape((<span class=\"number\">2</span>, <span class=\"number\">8</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dropout_layer(X, <span class=\"number\">0.</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dropout_layer(X, <span class=\"number\">0.5</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dropout_layer(X, <span class=\"number\">1.</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class=\"number\">784</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span>, <span class=\"number\">256</span></span><br><span class=\"line\"></span><br><span class=\"line\">dropout1, dropout2 = <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span></span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(nn.Flatten(),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">784</span>, <span class=\"number\">256</span>),</span><br><span class=\"line\">        nn.ReLU(),</span><br><span class=\"line\">        <span class=\"comment\"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">        nn.Dropout(dropout1),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">256</span>),</span><br><span class=\"line\">        nn.ReLU(),</span><br><span class=\"line\">        <span class=\"comment\"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">        nn.Dropout(dropout2),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_weights);</span><br><span class=\"line\"></span><br><span class=\"line\">num_epochs, lr, batch_size = <span class=\"number\">10</span>, <span class=\"number\">0.5</span>, <span class=\"number\">256</span></span><br><span class=\"line\">loss = nn.CrossEntropyLoss()</span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>\n<h2 id=\"前向传播、反向传播和计算图\"><a href=\"#前向传播、反向传播和计算图\" class=\"headerlink\" title=\"前向传播、反向传播和计算图\"></a>前向传播、反向传播和计算图</h2><p>我们已经学习了如何用小批量随机梯度下降训练模型。 然而当实现该算法时，我们只考虑了通过<em>前向传播</em>（forward propagation）所涉及的计算。 在计算梯度时，我们只调用了深度学习框架提供的反向传播函数，而不知其所以然。</p>\n<p>梯度的自动计算（自动微分）大大简化了深度学习算法的实现。 在自动微分之前，即使是对复杂模型的微小调整也需要手工重新计算复杂的导数， 学术论文也不得不分配大量页面来推导更新规则。 本节将通过一些基本的数学和计算图， 深入探讨<em>反向传播</em>的细节。 首先，我们将重点放在带权重衰减（ $L2$ 正则化）的单隐藏层多层感知机上。</p>\n<h4 id=\"前向传播\"><a href=\"#前向传播\" class=\"headerlink\" title=\"前向传播\"></a>前向传播</h4><p>前向传播指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。</p>\n<p>我们将一步一步研究单隐藏层神经网络的机制，为简单起见，我们假设输入样本是$x \\in R^d$，并且我们的隐藏层不包括偏置项。这里的中间变量是：</p>\n<script type=\"math/tex; mode=display\">\nz = W^{(1)}x</script><p>其中$W^{(1)} \\in R^{h*d}$是隐藏层的权重参数。将中间变量$z \\in R^h$通过激活函数$\\phi$，我们得到长度为$h$的隐藏激活向量：</p>\n<script type=\"math/tex; mode=display\">\nh = \\phi(z)</script><p>隐藏变量$h$也是一个中间变量。假设输出层的参数只有权重$W^{(2)} \\in R^{q*h}$，我们可以得到输出层的变量，它是一个长度为$q$的向量：</p>\n<script type=\"math/tex; mode=display\">\no = W^{(2)}h</script><p>假设损失函数为$l$，样本标签为$y$，我们可以单个计算数据样本的损失项，$L = l(o,y)$</p>\n<p>根据$L2$正则化的定义，给定超参数$\\lambda$，正则化项为</p>\n<script type=\"math/tex; mode=display\">\ns = \\frac{\\lambda}{2}(||W||_F^2 + ||W||_F^2)</script><p>其中矩阵的Frobenius范数是将矩阵展平为向量后应用的$L2$范数。最后，模型在给定数据样本上的正则化损失为：</p>\n<script type=\"math/tex; mode=display\">\nJ = L + s</script><p>在下面讨论中，我们将$J$称为目标函数。</p>\n<h4 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h4><p>反向传播指的是计算神经网络参数梯度的方法。简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。假设我们有函数$Y=f(X)$和$Z = g(X)$，其中输入和输出为$X,Y,Z$是任意形状的张量。利用链式法则，我们可以计算$Z$关于$X$的导数</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial Z}{\\partial L} = prod(\\frac{\\partial Z}{\\partial Y}, \\frac{\\partial{Y}}{\\partial X})</script><p>这里我们使用prod运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。对于向量，这很简单，它只是矩阵-矩阵乘法。对于高维向量，我们使用适当的对因项。运算符prod代指了所有的这些符号。</p>\n<p>回想以下，在计算图中的单隐藏层简单网络的参数是$W^{(1)}$和$W^{(2)}$。反向传播的目的是计算度$\\partial J/\\partial W^{(1)}$和$\\partial J/ \\partial W^{(2)}$。为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。第一步是计算目标函数$J = L + s$相对于损失项L和正则项$s$的梯度。</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial o}= prod(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial o}) = \\frac{\\partial L}{\\partial o} \\in R^q</script><p>接下来，我们计算正则化项两个参数的梯度：</p>\n<p>$\\frac{\\partial s}{\\partial W^{(1)}} = \\lambda W^{(1)} and \\frac{\\partial s}{\\partial W^{(2)}} = \\lambda W^{(2)}$</p>\n<p>现在我们可以计算最接近输出层的模型的梯度$\\frac{\\partial J}{\\partial W^{(2)}} \\in R^{q*h}$。使用链式法则得出：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial W^{(2)}} = prod(\\frac{\\partial J}{\\partial o}, \\frac{\\partial o}{\\partial W^{(2)}}) + prod(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial W^{(2)}}) = \\lambda W^{(2)}</script><p>为了获得关于$W^{(1)}$的梯度，我们需要继续沿着输出层到隐藏层反向传播。关于隐藏层输出的梯度$\\partial J/ \\partial h \\in R^h$由下式给出：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial h} = prod(\\frac{\\partial J}{\\partial o}) = W^{(2)^T} \\frac{\\partial J}{\\partial o}</script><p>由于激活函数$\\phi$是按元素计算的，计算中间变量$z$的梯度$\\partial J/ \\partial z \\in R^n$需要使用按元素乘法运算符，我们用$\\odot$来表示：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial z} = prod(\\frac{\\partial J}{\\partial h}, \\frac{\\partial h}{\\partial z}) = \\frac{\\partial J}{\\partial h } \\odot \\phi'(z)</script><p>最后，我们可以得到最接近输入层的的模型参数的梯度$\\partial J / \\partial W^{(1)} \\in R^{h*d}$。根据链式法则，我们得到：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial W^{(1)}} = prod(\\frac{\\partial J}{\\partial z},\\frac{\\partial z}{\\partial W^{(1)}}) + prod(\\frac{\\partial J}{\\partial s}, \\frac{s}{W^{(1)}}) = \\frac{\\partial J}{\\partial z}x^T + \\lambda W^{(1)}</script><h4 id=\"训练神经网络\"><a href=\"#训练神经网络\" class=\"headerlink\" title=\"训练神经网络\"></a>训练神经网络</h4><p>在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。</p>\n<p>以上述简单网络为例：一方面，在前向传播期间计算正则项取决于模型参数$W^{(1)}$和$W^{(2)}$的当前值。它们是由优化算法根据最近迭代的反向传播给出的。另一方面，反向传播期间参数的梯度计算，取决于由前向传播给出的隐藏层变量$h$的当前值。</p>\n<p>因此，在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致<em>内存不足</em>（out of memory）错误。</p>\n<h2 id=\"数值稳定和模型初始化\"><a href=\"#数值稳定和模型初始化\" class=\"headerlink\" title=\"数值稳定和模型初始化\"></a>数值稳定和模型初始化</h2><h3 id=\"part1：为什么要用梯度更新\"><a href=\"#part1：为什么要用梯度更新\" class=\"headerlink\" title=\"part1：为什么要用梯度更新\"></a>part1：为什么要用梯度更新</h3>","site":{"data":{}},"excerpt":"<h1 id=\"DeepLearning\"><a href=\"#DeepLearning\" class=\"headerlink\" title=\"DeepLearning\"></a>DeepLearning</h1><p>关于 dl 一些笔记，或是不懂的问题的记录……:happy:</p>","more":"<h2 id=\"多层感知机\"><a href=\"#多层感知机\" class=\"headerlink\" title=\"多层感知机\"></a>多层感知机</h2><h3 id=\"1-1-线性模型可能会出错\"><a href=\"#1-1-线性模型可能会出错\" class=\"headerlink\" title=\"1.1 线性模型可能会出错\"></a>1.1 线性模型可能会出错</h3><p>例如，线性意味着单调假设：任何特征的增大都有可能导致模型输出的增大（如果相应的权重为正）；或导致模型权重的减小（如果相应的权重为负）。</p>\n<p>此外，数据的表示可能考虑到特征之间的相关交互作用。在此表示的基础上建立一个线性模型可能会是合适的， 但我们不知道如何手动计算这么一种表示。 对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。</p>\n<h3 id=\"1-2-在网络中加入隐藏层\"><a href=\"#1-2-在网络中加入隐藏层\" class=\"headerlink\" title=\"1.2 在网络中加入隐藏层\"></a>1.2 在网络中加入隐藏层</h3><p>我们可以考虑在网络中加入一个或多个隐藏层来克服线性模型的限制，能使其处理更普遍函数之间的关系。要做到这一点，最简单的方法就是将许多全连接层堆叠到一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前面的$L-1$层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机。</p>\n<h3 id=\"1-3-从线性到非线性\"><a href=\"#1-3-从线性到非线性\" class=\"headerlink\" title=\"1.3 从线性到非线性\"></a>1.3 从线性到非线性</h3><p>我们通过$X \\in R^{n×h}$来表示n个样本的小批量，其中每个样本具有d个输入特征。对于具有$h$个隐藏单元的单层隐藏多层感知机，用$H\\in R^{n<em>h}$表示隐藏层的输出，称为隐藏表示。在数学或代码中，$H$也被称为隐藏层变量（hidden-layer variable）或隐藏变量（hidden variable）。因为隐藏层和输出层都是全连接的，所以我们具有隐藏层权重$W \\in R^{d×h}$和隐藏层偏置$b^{(1)} \\in R^{1</em>h}$以及输出层权重$W^{(2)} \\in R^{h×q}$和输出层偏置$b^{(2)} \\in R^{1×q}$。形式上我们按如下方式计算单隐藏层多层感知机的输出$O \\in R^{n×q}$：</p>\n<script type=\"math/tex; mode=display\">\nH = XW^{ (1) } + b^{ (1) }，\nO = HW^{ (2) }  + b^{ (2) },\n\\tag{1}</script><p>注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。 可我们能从中得到什么好处呢？在上面定义的模型里，我们没有好处！ 原因很简单：上面的隐藏单元由输入的仿射函数给出， 而输出（softmax操作前）只是隐藏单元的仿射函数。 仿射函数的仿射函数本身就是仿射函数， 但是我们之前的线性模型已经能够表示任何仿射函数。</p>\n<p>为了发挥多层架构的潜力，我们还需要一个额外的关键因素：在仿射变换之后对每个隐藏单元应用非线性激活函数（activation function）$\\sigma$。激活函数的输出（例如，$\\sigma(.)$）被称为活性值（activation）。一般来说，有了激活函数，就不可能再将我们的多层感知机退化成现行模型：</p>\n<script type=\"math/tex; mode=display\">\nH = \\sigma(XW^{(1)} + b^{ (1) } )\nO = HW^{(2)} + b^{ (2) } \\tag{2}</script><p>由于$X$中的每一行都对应于小批量中的一个样本，处于记号习惯的考量，我们定义非线性函数$\\sigma$也以按行的方式作用于其输入，即一次计算一个样本。但是本节应用于隐藏层的激活函数通常不按行进行操作，也按元素操作。</p>\n<p>这意味着，在计算每一层的线性部分之后，我们可以计算每个活性值，而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。</p>\n<h3 id=\"1-4-通用近似定理\"><a href=\"#1-4-通用近似定理\" class=\"headerlink\" title=\"1.4 通用近似定理\"></a>1.4 通用近似定理</h3><p>多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算。例如，在一对输入上进行基本的逻辑操作，多层感知机是通用近似器。即使网络只有一个隐藏层，给足够的神经元和足够的权重，我们可以对任意函数建模，尽管实际中学习该函数是很困难的神经网络有点像C语言。 C语言和任何其他现代编程语言一样，能够表达任何可计算的程序。 但实际上，想出一个符合规范的程序才是最困难的部分。</p>\n<p>而且，虽然一个单隐层网络能学习任何函数， 但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。 我们将在后面的章节中进行更细致的讨论。</p>\n<h3 id=\"1-5-激活函数\"><a href=\"#1-5-激活函数\" class=\"headerlink\" title=\"1.5 激活函数\"></a>1.5 激活函数</h3><p>激活函数（activate function）通过计算加权和并加上偏置来确定神经元是否应该被激活，他们将输入信号转换为输出的可微运算，大多数激活函数都是非线性的。由于激活函数是深度学习的基础，下面介绍一些简单的激活函数。</p>\n<h3 id=\"1-6-ReLU函数\"><a href=\"#1-6-ReLU函数\" class=\"headerlink\" title=\"1.6 ReLU函数\"></a>1.6 ReLU函数</h3><p>最受欢迎的激活函数是线性修正单元，因为它实现简单，同时在各种预测任务中表现良好。ReLU提供了一种非常简单的线性变换。给定元素$x$，ReLU函数被定义为该元素于0的最大值：</p>\n<script type=\"math/tex; mode=display\">\nReLU(x) = max(x,0)</script><p>通俗地说， ReLU 函数通过将相应的活性值设置为0，仅保留正元素，并丢弃所有负元素。为了直观的感受一下，我们可以画出函数的曲线图。正如从图中所看到，激活函数是分段线性的。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.arange(-<span class=\"number\">8.0</span>, <span class=\"number\">8.0</span>, <span class=\"number\">0.1</span>, requires_grad = <span class=\"literal\">True</span>)</span><br><span class=\"line\">y = torch.relu(x)</span><br><span class=\"line\">d2l.plot(x.detach(), y.detach(), <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;relu(x)&#x27;</span>, figsize=(<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br><span class=\"line\"><span class=\"comment\">#返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>这样我们就会继续使用这个新的<code>tensor进行计算，后面当我们进行</code>反向传播时，到该调用detach()的<code>tensor</code>就会停止，不能再继续向前进行传播。</p>\n<p>注意：使用detach()返回的Tensor和原始的tensor共用一个内存，即一个修改另一个也会跟着改变。</p>\n<p>当使用detach()分离tensor但是没有更改这个tensor时，并不会影响backward()。</p>\n<p>当使用detach()分离tensor，然后用这个分离出来的tensor去求导数，会影响backward()，会出现错误。</p>\n<p>当使用detach()分离tensor并且更改这个tensor时，即使再对原来的out求导数，会影响backward()，会出现错误。</p>\n</blockquote>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-14 20-48-23 的屏幕截图.png\" alt=\"8\"></p>\n<p>当输入为负数时，ReLU导数为0，当输入为正数时，ReLU函数的导数为1。注意，输入值精确等于0时，ReLU函数不可导。在此时，我们默认使用左边导数，即当输入0的导数为0。我们可以忽略这种情况，因为输入可能永远都不会是0。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y.backward(torch.ones_like(x), retrain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\">d2l.plot(x.detach(), x.grad(), <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;grad of relu&#x27;</span>, figsize = (<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n<p>下面我们绘制ReLU函数的导数。</p>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-14 20-53-33 的屏幕截图.png\" alt=\"7\">下面我们绘制ReLU函数的导数。</p>\n<p>使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且<strong>ReLU减轻了困扰以往神经网络的梯度消失问题。</strong></p>\n<p>注意，ReLU函数有很多变体，包括参数化ReLU函数。改变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：</p>\n<script type=\"math/tex; mode=display\">\npReLU(x) = max(0,x) + \\alpha min(0, x)</script><h3 id=\"1-7-sigmoid函数\"><a href=\"#1-7-sigmoid函数\" class=\"headerlink\" title=\"1.7 sigmoid函数\"></a>1.7 sigmoid函数</h3><p>对一个定义域在$R$上的输入，sigmoid函数将输入变换为区间$(0,1)$上的输出。因此，sigmoid函数通常称为挤压函数：它将范围$(-inf, inf)$中的任意输入压缩到区间$(0,1)$中的某个值：</p>\n<script type=\"math/tex; mode=display\">\nsigmoid(x) = \\frac{1} {1 + exp(-x)}</script><p> 注意，当输入接近0时，sigmoid函数接近线性变换。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.sigmoid(x)</span><br><span class=\"line\">d2l.plot(x.detach(), y.detach(), <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;sigmoid(x)&#x27;</span>, figsize = (<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n<p>sigmoid的导数为以下公式$\\frac{d} {dx}sigmoid(x) = \\frac{exp(-x)}{ { (1+exp(-x)) }^2} = sigmoid(x)(1-sigmoid(x))$ </p>\n<p>sigmoid函数的导数图像如下。注意，当输入为0时，sigmoid函数的导数最大可以达到0.25；而输入在任意方向上越远离0时，导数越接近0。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 清除以前的梯度</span></span><br><span class=\"line\">x.grad.data.zero_()</span><br><span class=\"line\">y.backward(torch.ones_like(x),retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\">d2l.plot(x.detach(), x.grad, <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;grad of sigmoid&#x27;</span>, figsize=(<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-14 21-18-27 的屏幕截图.png\" alt=\"6\"></p>\n<h3 id=\"1-8-tanh-函数\"><a href=\"#1-8-tanh-函数\" class=\"headerlink\" title=\"1.8 tanh 函数\"></a>1.8 tanh 函数</h3><p>与sigmoid函数类似，tanh （双曲正切）函数也能将其输入压缩转换到区间$(-1,1)$上。tanh 函数的公式如下：</p>\n<script type=\"math/tex; mode=display\">\ntanh(x) = \\frac{1 - exp(-2x)}{1 + exp(-2x)}</script><p>下面我们绘制tanh 函数。注意，当输入在0附近时，tanh函数接近线性变换。函数的形状类似于sigmoid函数，不同的是tanh函数关于坐标系远点中心对称。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.tanh(x)</span><br><span class=\"line\">d2l.plot(x.detach(), y.detach(), <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;tanh(x)&#x27;</span>, figsize=(<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-14 21-52-57 的屏幕截图.png\" alt=\"5\"></p>\n<p>tanh 的物理导数是：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{d}{dx}tanh(x) = 1 - tanh^2(x)</script><p>tanh 函数的导数图像如下所示。 当输入接近0时，tanh函数的导数接近最大值1。 与我们在sigmoid函数图像中看到的类似， 输入在任一方向上越远离0点，导数越接近0。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 清除以前的梯度</span></span><br><span class=\"line\">x.grad.data.zero_()</span><br><span class=\"line\">y.backward(torch.ones_like(x),retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\">d2l.plot(x.detach(), x.grad, <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;grad of tanh&#x27;</span>, figsize=(<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-14 22-07-56 的屏幕截图.png\" alt=\"4\"></p>\n<h2 id=\"模型选择，过拟合和欠拟合\"><a href=\"#模型选择，过拟合和欠拟合\" class=\"headerlink\" title=\"模型选择，过拟合和欠拟合\"></a>模型选择，过拟合和欠拟合</h2><h3 id=\"误差\"><a href=\"#误差\" class=\"headerlink\" title=\"误差\"></a>误差</h3><h4 id=\"训练误差\"><a href=\"#训练误差\" class=\"headerlink\" title=\"训练误差\"></a>训练误差</h4><p>训练误差是指模型在训练集上的错分样本比率，说白了就是在训练集上训练完毕后在训练集本身上进行预测得到了错分率</p>\n<h4 id=\"泛化误差\"><a href=\"#泛化误差\" class=\"headerlink\" title=\"泛化误差\"></a>泛化误差</h4><p><em>泛化误差</em>（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p>\n<p>问题是我们永远不能准确的计算出泛化误差。这是因为无限多的数据样本是一个虚构的对象。在实际中，我们只能通过模型应用于一个独立的测试集来估计泛化误差，该测试集由随机选取的，未曾在训练集中出现的样本构成。</p>\n<p>泛化误差的意义，其实就是在模型训练后查看模型是否具有代表性。</p>\n<p>泛化误差的公式为:$E_G(\\omega) = \\sum_{x \\in X}p(x)(\\hat f (x|\\omega) - f(x))^2$，即全集X中x出现的概率乘以其相对应的训练误差。</p>\n<p>但是过分追求低训练误差会使得模型过拟合于训练集反而不使用于其他数据。</p>\n<p>因此在样本集划分时，如果得到的训练集与测试集的数据没有交集，此时测试误差基本等同于泛化误差。</p>\n<h3 id=\"系统学习理论\"><a href=\"#系统学习理论\" class=\"headerlink\" title=\"系统学习理论\"></a>系统学习理论</h3><h4 id=\"独立同分布\"><a href=\"#独立同分布\" class=\"headerlink\" title=\"独立同分布\"></a>独立同分布</h4><p>假设训练数据和测试数据都是从相同的分布中独立提取的，这通常被称为独立同分布假设，这意味这对数据进行采样的过程没有进行”记忆”。</p>\n<p>影响模型泛化的因素：</p>\n<ol>\n<li>可调整参数的数量。当可调整参数的数量（有时称为<em>自由度</em>）很大时，模型往往更容易过拟合。</li>\n<li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li>\n<li>训练样本的值。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。</li>\n</ol>\n<h4 id=\"模型选择\"><a href=\"#模型选择\" class=\"headerlink\" title=\"模型选择\"></a>模型选择</h4><p>在机器学习中，在我们确定所有超参数之前，我们不希望用到测试集。如果我们在模型选择过程中使用测试数据，有可能会过拟合测试数据的风险，那就麻烦大了。如果我们过拟合来训练数据，还可以在测试数据上的评估来判断过拟合。但是如果我们拟合了测试数据集，我们又该怎么知道呢?</p>\n<p>因此，我们决不能靠测试数据进行模型的选择。然而，我们也不能依靠训练模型来选择模型，因为我们无法估计训练数据的泛化误差。</p>\n<p>在实际应用中，情况变得更加复杂。虽然理想情况下，我们只会使用测试数据一次，以评估最好的模型或比较一些模型的效果，但现实是测试数据很少在使用一次后被丢弃。我们很少能有充足的实验来对每一轮实验才用全新的测试集。</p>\n<p>解决此问题的常见做法是将我们的数据分成三份，除了训练集和测试集外，还增加依一个验证数据集，也叫验证集（validation dataset）。但现实是验证数据和测试数据之间模糊地令人担忧。除非另有明确说明，否则在本书的实验中，我们实际上实在使用应该被正确地称为训练数据和验证数据的数据集，并没有真正的测试数据集。因此，文中每次实验报告的准确度都是验证集准确度，而不是测试集准确度。</p>\n<h4 id=\"K折交叉验证\"><a href=\"#K折交叉验证\" class=\"headerlink\" title=\"K折交叉验证\"></a>K折交叉验证</h4><p>当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个适合的验证集。这个问题的一个流行解决方案是采用K折交叉验证。这里，原始训练数据被分成K个不重叠的子集。然后执行K次模型训练和验证，每次在$K-1$个子集上进行训练，并在剩余一个子集（该轮中没有用于训练的子集）进行验证。最后，通过对K次实验的结果取平均值来估计训练和验证的误差。</p>\n<h3 id=\"欠拟合-amp-amp-过拟合\"><a href=\"#欠拟合-amp-amp-过拟合\" class=\"headerlink\" title=\"欠拟合&amp;&amp;过拟合\"></a>欠拟合&amp;&amp;过拟合</h3><h4 id=\"欠拟合\"><a href=\"#欠拟合\" class=\"headerlink\" title=\"欠拟合\"></a>欠拟合</h4><p>欠拟合是指模型不能在训练集上获得足够低的误差。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律。</p>\n<p>当我们比较训练和验证误差时，我们要注意两种常见的情况。</p>\n<h4 id=\"如何解决欠拟合\"><a href=\"#如何解决欠拟合\" class=\"headerlink\" title=\"如何解决欠拟合\"></a>如何解决欠拟合</h4><p>欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过<strong>增加网络复杂度</strong>或者在模型中<strong>增加特征</strong>，这些都是很好解决欠拟合的方法。</p>\n<h4 id=\"过拟合\"><a href=\"#过拟合\" class=\"headerlink\" title=\"过拟合\"></a>过拟合</h4><p>过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，<strong>模型在训练集上表现很好，但在测试集上却表现很差</strong>。模型对训练集”死记硬背”（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，<strong>泛化能力差</strong>。</p>\n<h4 id=\"为什么会出现过拟合现象？\"><a href=\"#为什么会出现过拟合现象？\" class=\"headerlink\" title=\"为什么会出现过拟合现象？\"></a><strong>为什么会出现过拟合现象？</strong></h4><p>造成原因主要有以下几种：<br>1、<strong>训练数据集样本单一，样本不足</strong>。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。<br>2、<strong>训练数据中噪声干扰过大</strong>。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。<br>3、<strong>模型过于复杂。</strong>模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。</p>\n<h4 id=\"如何防止过拟合？\"><a href=\"#如何防止过拟合？\" class=\"headerlink\" title=\"如何防止过拟合？\"></a>如何防止过拟合？</h4><p>通过正则化：修改学习算法，使其降低泛化误差而非训练误差。</p>\n<p>常用的正则化方法根据具体的使用策略不同可以分为：</p>\n<ol>\n<li>直接提供正则化约束的参数正则化方法，如$L1/L2$正则化；</li>\n<li>通过工程上的技巧来实现更低泛化误差的方法，如提前终止（early stopping）和（Drop）</li>\n<li>不直接提供约束的隐式正则化方法，如数据增强等等。</li>\n</ol>\n<p><strong>1. 获取和使用更多的数据（数据集增强） ——-解决过拟合的根本性方法</strong></p>\n<p>让机器学习或深度学习模型泛化能力更好的办法就是使用更多的数据进行训练。但是，在实践中，我们拥有的数据量是有限的。解决这个问题的一种方法就是<strong>创建“假数据”并添加到训练集中——数据集增强</strong>。通过增加训练集的额外副本来增加训练集的大小，进而改进模型的泛化能力。</p>\n<p>我们以图像数据集举例，能够做：旋转图像、缩放图像、随机裁剪、加入随机噪声、平移、镜像等方式来增加数据量。另外补充一句，在物体分类问题里，<strong>CNN在图像识别的过程中有强大的“不变性”规则，即待辨识的物体在图像中的形状、姿势、位置、图像整体明暗度都不会影响分类结果</strong>。我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充。</p>\n<p><strong>2. 采用适合的模型（控制模型的复杂度）</strong></p>\n<p>对于过于复杂的模型会带来过拟合问题1。对于模型的设计，目前公认的一个深度学习的规律是”deeper is better”。比如许多大牛通过实验和竞赛发现，对于CNN来说，层数越多，效果越好，但也更容易产生过拟合，并且计算所耗费的时间也越长。</p>\n<p><strong>对于模型的设计而言，我们应该选择简单、合适的模型解决复杂的问题。</strong></p>\n<p><strong>3.降低特征的数量</strong></p>\n<p>对于一些特征工程而言，可以降低特征的数量——删除冗余特征，人工选择保留哪些特征。这种方法也可以解决过拟合问题。</p>\n<p><strong>4. L1/L2正则化</strong></p>\n<p><strong>(1) L1正则化</strong></p>\n<p>在原始的损失函数后面加上一个L1正则化项</p>\n<p>首先，我们要注意这样的情况：</p>\n<ol>\n<li><p>训练误差和验证误差都很严重；</p>\n</li>\n<li><p>训练误差和验证误差之间仅有一点差距。</p>\n</li>\n</ol>\n<p>如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式。此外，由于我们的训练和验证误差之间的泛化误差很小，我们有理由相信可以用一个更复杂的模型降低训练误差。这种现象被称为欠拟合（underfitting）。</p>\n<p>另一方方面，当我们的训练误差明显小于验证误差时要小心，这表明严重的过拟合（overfitting）。 注意，<em>过拟合</em>并不总是一件坏事。 特别是在深度学习领域，众所周知， 最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。 最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。</p>\n<p><strong>过拟合或欠拟合的因素：</strong></p>\n<ol>\n<li>模型的复杂性；</li>\n<li>训练数据集的大小。</li>\n</ol>\n<p><strong>模型的复杂性</strong></p>\n<p>为了说明一些关于过拟合和模型复杂性的经典直觉，我们给出一个多项式的例子。给定由单个特征$x$和和对应实数标签$y$组成的训练数据，我们试图找到下面的$d$阶多项式来估计标签$y$。</p>\n<script type=\"math/tex; mode=display\">\n\\hat{y} = \\sum \\limits_{i=0}^d x^i \\omega_i</script><p>由于这是一个线性回归问题，我们可以用平方误差作为我们的损失函数。</p>\n<p>高阶函数比低阶函数复杂得多，高阶函数的参数较多，模型的选择范围较广。因此在固定训练数据集的情况下，高阶多项式函数相对于低阶多项式的的训练误差应该始终更低（最坏也是相等）。事实上，当数据样本包含了$x$的不同值时，函数阶数等于样本数据量的多项式函数可以完美拟合训练集。下图中我们直观描述了过拟合和欠拟合的关系。</p>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-18 10-37-40 的屏幕截图.png\" alt=\"3\"></p>\n<p><strong>数据集大小</strong></p>\n<p>另一个重要因素是数据集的大小。 训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。 此外，一般来说，更多的数据不会有什么坏处。 对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系。 给出更多的数据，我们可能会尝试拟合一个更复杂的模型。 能够拟合更复杂的模型可能是有益的。 如果没有足够的数据，简单的模型可能更有用。 对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。 从一定程度上来说，深度学习目前的生机要归功于 廉价存储、互联设备以及数字化经济带来的海量数据集。</p>\n<h3 id=\"多项式回归\"><a href=\"#多项式回归\" class=\"headerlink\" title=\"多项式回归\"></a>多项式回归</h3><p>我们现在可以通过多项式拟合来探索这些概念。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n<h4 id=\"生成数据集\"><a href=\"#生成数据集\" class=\"headerlink\" title=\"生成数据集\"></a>生成数据集</h4><p>给定$x$，我们将使用以下三阶多项式来生成训练和测试数据的标签：</p>\n<script type=\"math/tex; mode=display\">\ny = 5 + 1.2x -3.4\\frac{x^2}{2!} + 5.6\\frac{x^3}{3!} + \\epsilon \\quad where \\quad \\epsilon ～ N(0,0.1^2).</script><p> 噪声$\\epsilon$服从均值为0，标准差为1的正太分布。在优化的过程中，我们通常希望避免非常大的梯度值或损失值。这就是我们将特征从$x^i$调整为$\\frac{x^i}{i!}$的原因，这样可以避免很大的$i$带来特别大的指数值。我们将训练集和测试集各生成100个样本。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">max_degree = <span class=\"number\">20</span> <span class=\"comment\">#多项式的最大阶数</span></span><br><span class=\"line\">n_train, n_test = <span class=\"number\">100</span> <span class=\"comment\">#训练和测试数据集将大小</span></span><br><span class=\"line\">true_w = np.zeros(max_degree) <span class=\"comment\"># 分配大量的空间</span></span><br><span class=\"line\">true_w[<span class=\"number\">0</span>:<span class=\"number\">4</span>] = np.array([<span class=\"number\">5</span>, <span class=\"number\">1.2</span>, -<span class=\"number\">3.4</span>, <span class=\"number\">5.6</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">features = np.random.normal(size=(n_train + n_test, <span class=\"number\">1</span>))</span><br><span class=\"line\">np.random.shuffle(features)</span><br><span class=\"line\">poly_features = np.power(features, np.arange(max_degree).reshape(<span class=\"number\">1</span>,-<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_degree):</span><br><span class=\"line\">    ploy_features[:,i] /= math.gamma(i+<span class=\"number\">1</span>) <span class=\"comment\">#gamma(n) = (n-1)!</span></span><br><span class=\"line\"><span class=\"comment\"># labels的维度（n_train + n_test,)</span></span><br><span class=\"line\">labels = np.dot(poly_features, true_w)</span><br><span class=\"line\">labels += np.random.normal(scale = <span class=\"number\">0.1</span>, size = labels.shape)</span><br></pre></td></tr></table></figure>\n<p>同样，存储在ploy_features中的单项式由gamma函数重新缩放，其中$\\Gamma(n) = (n-1)!$。从生成的数据集中查看一下前两个样本，第一个值是与偏置相对应的常量特征。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># NumPy ndarray转换为tensor</span></span><br><span class=\"line\">true_w, features, poly_features, labels = [torch.tensor(x, dtype=</span><br><span class=\"line\">    torch.float32) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> [true_w, features, poly_features, labels]]</span><br><span class=\"line\"></span><br><span class=\"line\">features[:<span class=\"number\">2</span>], poly_features[:<span class=\"number\">2</span>, :], labels[:<span class=\"number\">2</span>]</span><br></pre></td></tr></table></figure>\n<h4 id=\"对模型进行训练和测试\"><a href=\"#对模型进行训练和测试\" class=\"headerlink\" title=\"对模型进行训练和测试\"></a>对模型进行训练和测试</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">train_features, test_features, train_labels, test_labels</span></span><br><span class=\"line\"><span class=\"params\">         num_epochs = <span class=\"number\">400</span></span>):</span><br><span class=\"line\">    loss = nn.MESLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">    input_shape = train_features.shape[-<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"comment\">#不设置偏置，因为我们已经在多项式中实现了它</span></span><br><span class=\"line\">    net = nn.Sequential(nn.Linear(input_shape, <span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>))</span><br><span class=\"line\">    batch_size = <span class=\"built_in\">min</span>(<span class=\"number\">10</span>, train_labels.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">    train_iter = d2l.load_array((train_features, train_labels.reshape(-<span class=\"number\">1</span>,<span class=\"number\">1</span>)),</span><br><span class=\"line\">                               batch_size)</span><br><span class=\"line\">    test_iter = d2l.load_array((test_features, test_labels.reshape(-<span class=\"number\">1</span>,<span class=\"number\">1</span>)),</span><br><span class=\"line\">                              batch_size, is_train = <span class=\"literal\">False</span>)</span><br><span class=\"line\">    trainer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, y.scale=<span class=\"string\">&#x27;log&#x27;</span>,</span><br><span class=\"line\">                           xlim=[<span class=\"number\">1</span>,num_epochs], ylim = [<span class=\"number\">1e-3</span>, <span class=\"number\">1e2</span>],</span><br><span class=\"line\">                           legend = [<span class=\"string\">&#x27;train&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> epoch==<span class=\"number\">0</span> <span class=\"keyword\">or</span> (epoch + <span class=\"number\">1</span>)%<span class=\"number\">20</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            animator.add(epoch + <span class=\"number\">1</span>, (evaluate_loss(net, train_iter,loss),</span><br><span class=\"line\">                                    evaluate_loss(net, test_iter,loss)))</span><br><span class=\"line\">            </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:&#x27;</span>, net[<span class=\"number\">0</span>].weight.data.numpy())</span><br></pre></td></tr></table></figure>\n<h4 id=\"三阶多项式函数拟合\"><a href=\"#三阶多项式函数拟合\" class=\"headerlink\" title=\"三阶多项式函数拟合\"></a>三阶多项式函数拟合</h4><p>我们将首先使用三阶多项式函数，它与数据生成函数的阶数相同。 结果表明，该模型能有效降低训练损失和测试损失。学习到的模型参数也接近真实值$\\omega=[5, 1.2, -3.4, 5.6]$。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#从多项式特征中选取前四个维度，即1, x, x^2/2!, x^3/3!</span></span><br><span class=\"line\">train(poly_features[:n_train, :<span class=\"number\">4</span>], ploy_features[n_train:, :<span class=\"number\">4</span>],</span><br><span class=\"line\">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight: [[ <span class=\"number\">4.993645</span>   <span class=\"number\">1.2287872</span> -<span class=\"number\">3.3972282</span>  <span class=\"number\">5.559377</span> ]]</span><br></pre></td></tr></table></figure>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-18 14-51-23 的屏幕截图.png\" alt=\"2\"></p>\n<h4 id=\"线性函数拟合（欠拟合）\"><a href=\"#线性函数拟合（欠拟合）\" class=\"headerlink\" title=\"线性函数拟合（欠拟合）\"></a>线性函数拟合（欠拟合）</h4><p>让我们再看看线性函数拟合，减少该模型的训练损失相对困难。 在最后一个迭代周期完成后，训练损失仍然很高。 当用来拟合非线性模式（如这里的三阶多项式函数）时，线性模型容易欠拟合。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 从多项式特征中选择前2个维度，即1和x</span></span><br><span class=\"line\">train(poly_features[:n_train, :<span class=\"number\">2</span>], poly_features[n_train:, :<span class=\"number\">2</span>],</span><br><span class=\"line\">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight: [[<span class=\"number\">2.5148914</span> <span class=\"number\">4.2223625</span>]]</span><br></pre></td></tr></table></figure>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-18 14-54-34 的屏幕截图.png\" alt=\"2023-08-18 14-54-34 的屏幕截图\"></p>\n<h4 id=\"高阶多项式拟合（过拟合）\"><a href=\"#高阶多项式拟合（过拟合）\" class=\"headerlink\" title=\"高阶多项式拟合（过拟合）\"></a>高阶多项式拟合（过拟合）</h4><p>现在，让我们尝试使用一个阶数过高的多项式来训练模型。 在这种情况下，没有足够的数据用于学到高阶系数应该具有接近于零的值。 因此，这个过于复杂的模型会轻易受到训练数据中噪声的影响。 虽然训练损失可以有效地降低，但测试损失仍然很高。 结果表明，复杂模型对数据造成了过拟合。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 从多项式特征中选取所有维度</span></span><br><span class=\"line\">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class=\"line\">      labels[:n_train], labels[n_train:], num_epochs=<span class=\"number\">1500</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/home/xxfs/study/recording/deep_learning/photos/2023-08-18 14-56-01 的屏幕截图.png\" alt=\"1\"></p>\n<h2 id=\"权重衰减\"><a href=\"#权重衰减\" class=\"headerlink\" title=\"权重衰减\"></a>权重衰减</h2><h3 id=\"L1-L2正则化和权重衰减\"><a href=\"#L1-L2正则化和权重衰减\" class=\"headerlink\" title=\"L1/L2正则化和权重衰减\"></a>L1/L2正则化和权重衰减</h3><p>L2范数也被称为欧几里得范数，可以简单理解为向模长。</p>\n<p>范数定义的公式如下：</p>\n<script type=\"math/tex; mode=display\">\n||x||_p :=  (\\sum_{i = 1}^{n}|x_i|^p)^{\\frac{1}{p}}</script><h4 id=\"L1范数\"><a href=\"#L1范数\" class=\"headerlink\" title=\"L1范数\"></a>L1范数</h4><p>$p= 1$时称为$L1$范数(L1-norm)：</p>\n<script type=\"math/tex; mode=display\">\n||x||_1 := \\sum^n_{i = 1}|x_i|</script><p>$L1$范数是一组数的绝对值累加和。</p>\n<h4 id=\"L2范数\"><a href=\"#L2范数\" class=\"headerlink\" title=\"L2范数\"></a>L2范数</h4><p>$p = 2$时，称为$L2$范数：</p>\n<script type=\"math/tex; mode=display\">\n||x||_2 := (\\sum_{i =1}^n x^{(i)})^{\\frac{1}{2}}</script><p>可以理解为空间或平面内某一点到原点的距离。</p>\n<h4 id=\"L1-L2正则化和权重衰减-1\"><a href=\"#L1-L2正则化和权重衰减-1\" class=\"headerlink\" title=\"L1/L2正则化和权重衰减\"></a>L1/L2正则化和权重衰减</h4><p>通过在loss上增加了$L1$或$L2$范数项，达到参数惩罚的作用，即实现了正则化效果，从而称为$L1/L2$正则化。</p>\n\n<p><img src=\"/source/images/2023-09-15 21-17-12 的屏幕截图.png\" alt=\"2023-09-15 21-17-12 的屏幕截图\"></p>\n<p>由于其高次项参数的使用，使得模型对训练数据过分拟合，导致对未来更一般的数据预测性大大下降，为了缓解这种过拟合的现象，我们可以采用L2正则化。 使用$L2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。具体来说就是在原有的损失函数上添加L2正则化项(l2-norm的平方)：</p>\n<p>原来的损失：</p>\n<script type=\"math/tex; mode=display\">\nQ(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y} - y)^2</script><p>加上$L2$正则化项后的损失：</p>\n<script type=\"math/tex; mode=display\">\nJ(\\theta) = Q(x) + \\frac{1}{2n} \\lambda \\sum_{j=1}^{n} \\theta_j^2</script><p>这里，通过正则化系数$\\lambda$可以较好地惩罚高次项的特征，从而起到降低过拟合，正则化的效果。</p>\n<p>添加$L2$正则化修正以后的模型：</p>\n<p><img src=\"source/images/2023-09-15 21-39-05 的屏幕截图.png\" alt=\"2023-09-15 21-39-05 的屏幕截图\"></p>\n<h3 id=\"权重衰减-1\"><a href=\"#权重衰减-1\" class=\"headerlink\" title=\"权重衰减\"></a>权重衰减</h3><p>权重衰减weight decay，并不是一个规范的定义，而只是俗称而已，可以理解为削减/惩罚权重。在大多数情况下weight dacay 可以等价为L2正则化。L2正则化的作用就在于削减权重，降低模型过拟合，其行为即直接导致每轮迭代过程中的权重weight参数被削减/惩罚了一部分，故也称为权重衰减weight decay。从这个角度看，不论你用L1正则化还是L2正则化，亦或是其他的正则化方法，只要是削减了权重，那都可以称为weight dacay。从这个角度看，不论你用$L1$正则化还是$L2$正则化，亦或是其他的正则化方法，只要是削减了权重，那都可以称为weight dacay。</p>\n<p>设：</p>\n<ul>\n<li>参数矩阵为p（包括weight和bias）；</li>\n<li>模型训练迭代过程中计算出的loss对参数梯度为d_p；</li>\n<li>学习率lr；</li>\n<li>权重衰减参数为decay</li>\n</ul>\n<p>则不设dacay时，迭代时参数的更新过程可以表示为：</p>\n<script type=\"math/tex; mode=display\">\np = p - lr × d\\_p</script><p>增加weight_dacay参数后更新过程可以表示为：</p>\n<script type=\"math/tex; mode=display\">\np = p - lr × （d\\_p + p × dacay)</script><h3 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h3><p>在深度学习框架的实现中，可以通过设置weight_decay参数，直接对weight矩阵中的数值进行削减（而不是像L2正则一样，通过修改loss函数）起到正则化的参数惩罚作用。二者通过不同方式，同样起到了对权重参数削减/惩罚的作用，实际上在通常的随机梯度下降算法(SGD)中，通过数学计算L2正则化完全可以等价于直接权重衰减。（少数情况除外，譬如使用Adam优化器时，可以参考：<a href=\"https://zhuanlan.zhihu.com/p/40814046\">L2正则=Weight Decay？并不是这样</a>）</p>\n<p>正因如此，深度学习框架通常实现weight dacay/L2正则化的方式很简单，直接指定weight_dacay参数即可。</p>\n<p>在pytorch/tensorflow等框架中，我们可以方便地指定weight_dacay参数，来达到正则化的效果，譬如在pytorch的sgd优化器中，直接指定weight_decay = 0.0001：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">optimizer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.001</span>, weight_decay=<span class=\"number\">0.0001</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#0.05 + 0.01X + e where e \\in N(0, 0.01^2)</span></span><br><span class=\"line\"></span><br><span class=\"line\">n_train, n_test, num_inputs, batch_size = <span class=\"number\">20</span>, <span class=\"number\">100</span>, <span class=\"number\">200</span>, <span class=\"number\">5</span></span><br><span class=\"line\">true_w, true_b = torch.ones((num_inputs, <span class=\"number\">1</span>)) * <span class=\"number\">0.01</span>, <span class=\"number\">0.05</span></span><br><span class=\"line\">train_data = d2l.synthetic_data(true_w, true_b, n_train)</span><br><span class=\"line\">train_iter = d2l.load_array(train_data, batch_size)</span><br><span class=\"line\">test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class=\"line\">test_iter = d2l.load_array(test_data, batch_size, is_train=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#定义一个函数来随机初始化参数模型</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_params</span>():</span><br><span class=\"line\">    w = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, size=(num_inputs, <span class=\"number\">1</span>), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    b = torch.zeros(<span class=\"number\">1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [w,b]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">l2_penalty</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">sum</span>(w.<span class=\"built_in\">pow</span>(<span class=\"number\">2</span>)) / <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#定义训练代码的实现</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">lambd</span>):</span><br><span class=\"line\">    w, b = init_params()</span><br><span class=\"line\">    net, loss = <span class=\"keyword\">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss</span><br><span class=\"line\">    num_epochs, lr = <span class=\"number\">100</span>, <span class=\"number\">0.003</span></span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epochs&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>,</span><br><span class=\"line\">                            xlim=[<span class=\"number\">5</span>, num_epochs], legend=[<span class=\"string\">&#x27;train&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">            <span class=\"comment\"># 增加了L2范数惩罚项，</span></span><br><span class=\"line\">            <span class=\"comment\"># 广播机制使l2_penalty(w)成为一个长度为batch_size的向量</span></span><br><span class=\"line\">            l = loss(net(X), y) + lambd * l2_penalty(w)</span><br><span class=\"line\">            l.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">            d2l.sgd([w, b], lr, batch_size)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">5</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            animator.add(epoch + <span class=\"number\">1</span>, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class=\"line\">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;w的L2范数是：&#x27;</span>, torch.norm(w).item())</span><br><span class=\"line\">    </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#忽略正则化直接进行训练</span></span><br><span class=\"line\">train(lambd=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#使用权重衰减</span></span><br><span class=\"line\">train(lambd=<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"简洁实现\"><a href=\"#简洁实现\" class=\"headerlink\" title=\"简洁实现\"></a>简洁实现</h3><p>由于权重衰减在神经网络优化中很常用， 深度学习框架为了便于我们使用权重衰减， 将权重衰减集成到优化算法中，以便与任何损失函数结合使用。 此外，这种集成还有计算上的好处， 允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。 由于更新的权重衰减部分仅依赖于每个参数的当前值， 因此优化器必须至少接触每个参数一次。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_concise</span>(<span class=\"params\">wd</span>):</span><br><span class=\"line\">    net = nn.Sequential(nn.Linear(num_inputs, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> net.parameters():</span><br><span class=\"line\">        param.data.normal_()</span><br><span class=\"line\">    loss = nn.MSELoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">    num_epochs, lr = <span class=\"number\">100</span>, <span class=\"number\">0.003</span></span><br><span class=\"line\">    <span class=\"comment\"># 偏置参数没有衰减</span></span><br><span class=\"line\">    trainer = torch.optim.SGD([</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;params&quot;</span>:net[<span class=\"number\">0</span>].weight,<span class=\"string\">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;params&quot;</span>:net[<span class=\"number\">0</span>].bias&#125;], lr=lr)</span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epochs&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, yscale=<span class=\"string\">&#x27;log&#x27;</span>,</span><br><span class=\"line\">                            xlim=[<span class=\"number\">5</span>, num_epochs], legend=[<span class=\"string\">&#x27;train&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">            trainer.zero_grad()</span><br><span class=\"line\">            l = loss(net(X), y)</span><br><span class=\"line\">            l.mean().backward()</span><br><span class=\"line\">            trainer.step()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">5</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            animator.add(epoch + <span class=\"number\">1</span>,</span><br><span class=\"line\">                         (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class=\"line\">                          d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;w的L2范数：&#x27;</span>, net[<span class=\"number\">0</span>].weight.norm().item())</span><br></pre></td></tr></table></figure>\n<h2 id=\"暂退法-Dropout\"><a href=\"#暂退法-Dropout\" class=\"headerlink\" title=\"暂退法 Dropout\"></a>暂退法 Dropout</h2><h4 id=\"重新审视过拟合\"><a href=\"#重新审视过拟合\" class=\"headerlink\" title=\"重新审视过拟合\"></a>重新审视过拟合</h4><p>当面对更多的特征而样本不足时，线性模型往往会过拟合。相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。</p>\n<p>泛化性和灵活性之间的权衡被描述为<strong>偏差-方差权衡</strong>。线性模型有很高的偏差：它们只能表示一小类函数。然而，这些模型的方差很低：它们在不同的随机数据样本上可以得出相似的结果。</p>\n<p>深度学习网络位于偏差-方差谱的另一端。于线性模型不同，神经网络并不局限于查看每个特征，而是学习特征之间的交互。</p>\n<p>在探究泛化之前，我们先来定义以下什么是“好”的预测模型？我们期待好的预测模型能在未知的数据上有很好的表现， 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。</p>\n<p>简单性的另一个度量角度是平滑性，即函数不应该对其输入的微小变化而敏感 例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。在2014年，斯里瓦斯塔瓦等人就如何将毕晓普的想法应用于网络的内部层提出了一个想法： 在训练过程中他们建议在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。</p>\n<p>这个想法被称为暂退法。暂退法在前向传播过程中，计算每一层内部的同时注入噪音，这已经成为训练神经网络的常用技术。这种方法之所以被称为暂退法，因为我们表面上看是在训练过程中丢弃的一些神经元。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。</p>\n<p>需要说明的是，暂退法的原始论文提到了一个关于有性繁殖的类比： 神经网络过拟合与每一层都依赖于前一层激活值相关，称这种情况为“共适应性”。 作者认为，暂退法会破坏共适应性，就像有性生殖会破坏共适应的基因一样。</p>\n<p>那么关键的挑战就是如何注入这种噪声。 一种想法是以一种<em>无偏向</em>（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。</p>\n<p>可以考虑将高斯噪声加入到线性模型的输入中。在没次训练中，他将从均值为零的分布$\\epsilon ～ N（0,\\sigma)$采样噪声添加到输入$x$，从而产生扰动点$x’ = x + \\epsilon$，期望是$E[x’] = x$。</p>\n<p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值ℎ以*暂退概率$p$由随机变量$ℎ′$替换，如下所示：</p>\n<script type=\"math/tex; mode=display\">\nh' = \n\\left\\{\n\\begin{array}{**lr**}  \n0 \\quad 概率为0\n\\\\\n\\frac{h}{1-p} \\quad 其他情况\n\\end{array}  \n\\right.</script><p>根据此模型的设计，其期望值保持不变，即$E[x’] = x$。</p>\n<h4 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h4><p>dropout相当于给出一个概率$p$，比方说$p=40\\%$，那么就是说有$40\\%$的文件要被删除，只留下$60%$的神经元，那么这就是我们的表面理解。对于程序来说，就是将这40%的神经元赋值0，那么可以想一下一个神经元等于0了，那么他对下一层还能产出结果吗，0乘多少权重都是0，相当于这个被dropout选中的神经元没价值了，那他就相当于被删了。</p>\n<h4 id=\"实践中的暂退法\"><a href=\"#实践中的暂退法\" class=\"headerlink\" title=\"实践中的暂退法\"></a>实践中的暂退法</h4><p>带有1个隐藏层和5个隐藏单元的多层感知机。 当我们将暂退法应用到隐藏层，以$p$的概率将隐藏单元置为零时， 结果可以看作一个只包含原始神经元子集的网络。假设隐藏单元为$h1,h2,h3,h4,h5$，我们删除了$h2,h5$，因此输出的计算不依赖$h2,h5$并且它们各自的梯度在之执行反向传播也会消失。这样，输出层的计算不能过度依赖$h1,…,h5$中的任意一个元素。</p>\n<h4 id=\"从零开始实现\"><a href=\"#从零开始实现\" class=\"headerlink\" title=\"从零开始实现\"></a>从零开始实现</h4><p>要实现单层的暂退法函数，我们从均匀分布$U[0,1]$中抽取样本，样本数于这层神经网络的维度一致。然后我们保留那些对应样本大于$p$的节点，把剩下的丢弃。</p>\n<p>在下面的代码中，我们实现dropout_layer函数，该函数以dropout的概率丢弃丢弃张量输入X中的元素，如上述重新缩放剩余部分：将剩余部分除以1.0-dropout。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dropout_layer</span>(<span class=\"params\">X, dropout</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">0</span> &lt;= dropout &lt;= <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># 在本情况中，所有元素都被丢弃</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout == <span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.zeros_like(X)</span><br><span class=\"line\">    <span class=\"comment\"># 在本情况中，所有元素都被保留</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br><span class=\"line\">    mask = (torch.rand(X.shape) &gt; dropout).<span class=\"built_in\">float</span>()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask * X / (<span class=\"number\">1.0</span> - dropout)</span><br></pre></td></tr></table></figure>\n<h4 id=\"定义模型参数\"><a href=\"#定义模型参数\" class=\"headerlink\" title=\"定义模型参数\"></a>定义模型参数</h4><p>引入Fashion-MNIST数据集。我们定义具有两个隐藏层的多层感知机，每个隐藏层包含256个单元。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class=\"number\">784</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span>, <span class=\"number\">256</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"定义模型\"><a href=\"#定义模型\" class=\"headerlink\" title=\"定义模型\"></a>定义模型</h4><p>我们可以将暂退法应用于每个隐藏层的输出（在激活函数之后）， 并且可以为每一层分别设置暂退概率： 常见的技巧是在靠近输入层的地方设置较低的暂退概率。 下面的模型将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5， 并且暂退法只在训练期间有效。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dropout1, dropout2 = <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Net</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span></span><br><span class=\"line\"><span class=\"params\">                 is_training = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Net, self).__init__()</span><br><span class=\"line\">        self.num_inputs = num_inputs</span><br><span class=\"line\">        self.training = is_training</span><br><span class=\"line\">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class=\"line\">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class=\"line\">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class=\"line\">        self.relu = nn.ReLU()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        H1 = self.relu(self.lin1(X.reshape((-<span class=\"number\">1</span>, self.num_inputs))))</span><br><span class=\"line\">        <span class=\"comment\"># 只有在训练模型时才使用dropout</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.training == <span class=\"literal\">True</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">            H1 = dropout_layer(H1, dropout1)</span><br><span class=\"line\">        H2 = self.relu(self.lin2(H1))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.training == <span class=\"literal\">True</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">            H2 = dropout_layer(H2, dropout2)</span><br><span class=\"line\">        out = self.lin3(H2)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br></pre></td></tr></table></figure>\n<h4 id=\"训练和测试\"><a href=\"#训练和测试\" class=\"headerlink\" title=\"训练和测试\"></a>训练和测试</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, lr, batch_size = <span class=\"number\">10</span>, <span class=\"number\">0.5</span>, <span class=\"number\">256</span></span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>\n<h4 id=\"简洁实现-1\"><a href=\"#简洁实现-1\" class=\"headerlink\" title=\"简洁实现\"></a>简洁实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dropout_layer</span>(<span class=\"params\">X, dropout</span>):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">0</span> &lt;= dropout &lt;= <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># 在本情况中，所有元素都被丢弃</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout == <span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.zeros_like(X)</span><br><span class=\"line\">    <span class=\"comment\"># 在本情况中，所有元素都被保留</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br><span class=\"line\">    mask = (torch.rand(X.shape) &gt; dropout).<span class=\"built_in\">float</span>()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask * X / (<span class=\"number\">1.0</span> - dropout)</span><br><span class=\"line\"></span><br><span class=\"line\">X= torch.arange(<span class=\"number\">16</span>, dtype = torch.float32).reshape((<span class=\"number\">2</span>, <span class=\"number\">8</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dropout_layer(X, <span class=\"number\">0.</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dropout_layer(X, <span class=\"number\">0.5</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dropout_layer(X, <span class=\"number\">1.</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class=\"number\">784</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span>, <span class=\"number\">256</span></span><br><span class=\"line\"></span><br><span class=\"line\">dropout1, dropout2 = <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span></span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(nn.Flatten(),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">784</span>, <span class=\"number\">256</span>),</span><br><span class=\"line\">        nn.ReLU(),</span><br><span class=\"line\">        <span class=\"comment\"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">        nn.Dropout(dropout1),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">256</span>),</span><br><span class=\"line\">        nn.ReLU(),</span><br><span class=\"line\">        <span class=\"comment\"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">        nn.Dropout(dropout2),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_weights);</span><br><span class=\"line\"></span><br><span class=\"line\">num_epochs, lr, batch_size = <span class=\"number\">10</span>, <span class=\"number\">0.5</span>, <span class=\"number\">256</span></span><br><span class=\"line\">loss = nn.CrossEntropyLoss()</span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>\n<h2 id=\"前向传播、反向传播和计算图\"><a href=\"#前向传播、反向传播和计算图\" class=\"headerlink\" title=\"前向传播、反向传播和计算图\"></a>前向传播、反向传播和计算图</h2><p>我们已经学习了如何用小批量随机梯度下降训练模型。 然而当实现该算法时，我们只考虑了通过<em>前向传播</em>（forward propagation）所涉及的计算。 在计算梯度时，我们只调用了深度学习框架提供的反向传播函数，而不知其所以然。</p>\n<p>梯度的自动计算（自动微分）大大简化了深度学习算法的实现。 在自动微分之前，即使是对复杂模型的微小调整也需要手工重新计算复杂的导数， 学术论文也不得不分配大量页面来推导更新规则。 本节将通过一些基本的数学和计算图， 深入探讨<em>反向传播</em>的细节。 首先，我们将重点放在带权重衰减（ $L2$ 正则化）的单隐藏层多层感知机上。</p>\n<h4 id=\"前向传播\"><a href=\"#前向传播\" class=\"headerlink\" title=\"前向传播\"></a>前向传播</h4><p>前向传播指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。</p>\n<p>我们将一步一步研究单隐藏层神经网络的机制，为简单起见，我们假设输入样本是$x \\in R^d$，并且我们的隐藏层不包括偏置项。这里的中间变量是：</p>\n<script type=\"math/tex; mode=display\">\nz = W^{(1)}x</script><p>其中$W^{(1)} \\in R^{h*d}$是隐藏层的权重参数。将中间变量$z \\in R^h$通过激活函数$\\phi$，我们得到长度为$h$的隐藏激活向量：</p>\n<script type=\"math/tex; mode=display\">\nh = \\phi(z)</script><p>隐藏变量$h$也是一个中间变量。假设输出层的参数只有权重$W^{(2)} \\in R^{q*h}$，我们可以得到输出层的变量，它是一个长度为$q$的向量：</p>\n<script type=\"math/tex; mode=display\">\no = W^{(2)}h</script><p>假设损失函数为$l$，样本标签为$y$，我们可以单个计算数据样本的损失项，$L = l(o,y)$</p>\n<p>根据$L2$正则化的定义，给定超参数$\\lambda$，正则化项为</p>\n<script type=\"math/tex; mode=display\">\ns = \\frac{\\lambda}{2}(||W||_F^2 + ||W||_F^2)</script><p>其中矩阵的Frobenius范数是将矩阵展平为向量后应用的$L2$范数。最后，模型在给定数据样本上的正则化损失为：</p>\n<script type=\"math/tex; mode=display\">\nJ = L + s</script><p>在下面讨论中，我们将$J$称为目标函数。</p>\n<h4 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h4><p>反向传播指的是计算神经网络参数梯度的方法。简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。假设我们有函数$Y=f(X)$和$Z = g(X)$，其中输入和输出为$X,Y,Z$是任意形状的张量。利用链式法则，我们可以计算$Z$关于$X$的导数</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial Z}{\\partial L} = prod(\\frac{\\partial Z}{\\partial Y}, \\frac{\\partial{Y}}{\\partial X})</script><p>这里我们使用prod运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。对于向量，这很简单，它只是矩阵-矩阵乘法。对于高维向量，我们使用适当的对因项。运算符prod代指了所有的这些符号。</p>\n<p>回想以下，在计算图中的单隐藏层简单网络的参数是$W^{(1)}$和$W^{(2)}$。反向传播的目的是计算度$\\partial J/\\partial W^{(1)}$和$\\partial J/ \\partial W^{(2)}$。为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。第一步是计算目标函数$J = L + s$相对于损失项L和正则项$s$的梯度。</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial o}= prod(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial o}) = \\frac{\\partial L}{\\partial o} \\in R^q</script><p>接下来，我们计算正则化项两个参数的梯度：</p>\n<p>$\\frac{\\partial s}{\\partial W^{(1)}} = \\lambda W^{(1)} and \\frac{\\partial s}{\\partial W^{(2)}} = \\lambda W^{(2)}$</p>\n<p>现在我们可以计算最接近输出层的模型的梯度$\\frac{\\partial J}{\\partial W^{(2)}} \\in R^{q*h}$。使用链式法则得出：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial W^{(2)}} = prod(\\frac{\\partial J}{\\partial o}, \\frac{\\partial o}{\\partial W^{(2)}}) + prod(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial W^{(2)}}) = \\lambda W^{(2)}</script><p>为了获得关于$W^{(1)}$的梯度，我们需要继续沿着输出层到隐藏层反向传播。关于隐藏层输出的梯度$\\partial J/ \\partial h \\in R^h$由下式给出：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial h} = prod(\\frac{\\partial J}{\\partial o}) = W^{(2)^T} \\frac{\\partial J}{\\partial o}</script><p>由于激活函数$\\phi$是按元素计算的，计算中间变量$z$的梯度$\\partial J/ \\partial z \\in R^n$需要使用按元素乘法运算符，我们用$\\odot$来表示：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial z} = prod(\\frac{\\partial J}{\\partial h}, \\frac{\\partial h}{\\partial z}) = \\frac{\\partial J}{\\partial h } \\odot \\phi'(z)</script><p>最后，我们可以得到最接近输入层的的模型参数的梯度$\\partial J / \\partial W^{(1)} \\in R^{h*d}$。根据链式法则，我们得到：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J}{\\partial W^{(1)}} = prod(\\frac{\\partial J}{\\partial z},\\frac{\\partial z}{\\partial W^{(1)}}) + prod(\\frac{\\partial J}{\\partial s}, \\frac{s}{W^{(1)}}) = \\frac{\\partial J}{\\partial z}x^T + \\lambda W^{(1)}</script><h4 id=\"训练神经网络\"><a href=\"#训练神经网络\" class=\"headerlink\" title=\"训练神经网络\"></a>训练神经网络</h4><p>在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。</p>\n<p>以上述简单网络为例：一方面，在前向传播期间计算正则项取决于模型参数$W^{(1)}$和$W^{(2)}$的当前值。它们是由优化算法根据最近迭代的反向传播给出的。另一方面，反向传播期间参数的梯度计算，取决于由前向传播给出的隐藏层变量$h$的当前值。</p>\n<p>因此，在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致<em>内存不足</em>（out of memory）错误。</p>\n<h2 id=\"数值稳定和模型初始化\"><a href=\"#数值稳定和模型初始化\" class=\"headerlink\" title=\"数值稳定和模型初始化\"></a>数值稳定和模型初始化</h2><h3 id=\"part1：为什么要用梯度更新\"><a href=\"#part1：为什么要用梯度更新\" class=\"headerlink\" title=\"part1：为什么要用梯度更新\"></a>part1：为什么要用梯度更新</h3>"},{"title":"一些记录或是吐嘈","date":"2023-08-07T08:02:29.000Z","_content":"\n\n\n\n\n# 用于记录2023暑假剩余时间的学习日程\n\n<!--more-->\n\n## 8/7\n\n1. softmax理论\n\n\n\n## 8/8\n\n1. softmax实现\n\n## 8/9\n\n1. softmax实现(我死了，一个模型看了四个小时。。。。。。。无语)\n2. miniconda配置\n\n### 8/16\n期末考试即将来临，快复习啊啊啊啊啊，感觉要学的好多啊。\n\n### 8/18\n1. 模型选择，过拟合，欠拟合\n2. \n\n### 8/21\n1. LSTM在实践了（sad）\n睡得晚感觉脑子都变旧了………为什么人可以唠嗑四五个小时，真的很神奇啊，所以说，人类还是没有那么讨厌嘛……（尼古拉斯的疯言疯语，哈哈哈）\n","source":"_posts/SummerRecord2023.md","raw":"---\ntitle: 一些记录或是吐嘈\ndate: 2023-08-07 16:02:29\ntags:\ncategories:\n- Recording\n---\n\n\n\n\n\n# 用于记录2023暑假剩余时间的学习日程\n\n<!--more-->\n\n## 8/7\n\n1. softmax理论\n\n\n\n## 8/8\n\n1. softmax实现\n\n## 8/9\n\n1. softmax实现(我死了，一个模型看了四个小时。。。。。。。无语)\n2. miniconda配置\n\n### 8/16\n期末考试即将来临，快复习啊啊啊啊啊，感觉要学的好多啊。\n\n### 8/18\n1. 模型选择，过拟合，欠拟合\n2. \n\n### 8/21\n1. LSTM在实践了（sad）\n睡得晚感觉脑子都变旧了………为什么人可以唠嗑四五个小时，真的很神奇啊，所以说，人类还是没有那么讨厌嘛……（尼古拉斯的疯言疯语，哈哈哈）\n","slug":"SummerRecord2023","published":1,"updated":"2023-08-21T11:07:15.088Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clmnlyz6p0006myqb2t36fa66","content":"<h1 id=\"用于记录2023暑假剩余时间的学习日程\"><a href=\"#用于记录2023暑假剩余时间的学习日程\" class=\"headerlink\" title=\"用于记录2023暑假剩余时间的学习日程\"></a>用于记录2023暑假剩余时间的学习日程</h1><span id=\"more\"></span>\n<h2 id=\"8-7\"><a href=\"#8-7\" class=\"headerlink\" title=\"8/7\"></a>8/7</h2><ol>\n<li>softmax理论</li>\n</ol>\n<h2 id=\"8-8\"><a href=\"#8-8\" class=\"headerlink\" title=\"8/8\"></a>8/8</h2><ol>\n<li>softmax实现</li>\n</ol>\n<h2 id=\"8-9\"><a href=\"#8-9\" class=\"headerlink\" title=\"8/9\"></a>8/9</h2><ol>\n<li>softmax实现(我死了，一个模型看了四个小时。。。。。。。无语)</li>\n<li>miniconda配置</li>\n</ol>\n<h3 id=\"8-16\"><a href=\"#8-16\" class=\"headerlink\" title=\"8/16\"></a>8/16</h3><p>期末考试即将来临，快复习啊啊啊啊啊，感觉要学的好多啊。</p>\n<h3 id=\"8-18\"><a href=\"#8-18\" class=\"headerlink\" title=\"8/18\"></a>8/18</h3><ol>\n<li>模型选择，过拟合，欠拟合</li>\n<li></li>\n</ol>\n<h3 id=\"8-21\"><a href=\"#8-21\" class=\"headerlink\" title=\"8/21\"></a>8/21</h3><ol>\n<li>LSTM在实践了（sad）<br>睡得晚感觉脑子都变旧了………为什么人可以唠嗑四五个小时，真的很神奇啊，所以说，人类还是没有那么讨厌嘛……（尼古拉斯的疯言疯语，哈哈哈）</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h1 id=\"用于记录2023暑假剩余时间的学习日程\"><a href=\"#用于记录2023暑假剩余时间的学习日程\" class=\"headerlink\" title=\"用于记录2023暑假剩余时间的学习日程\"></a>用于记录2023暑假剩余时间的学习日程</h1>","more":"<h2 id=\"8-7\"><a href=\"#8-7\" class=\"headerlink\" title=\"8/7\"></a>8/7</h2><ol>\n<li>softmax理论</li>\n</ol>\n<h2 id=\"8-8\"><a href=\"#8-8\" class=\"headerlink\" title=\"8/8\"></a>8/8</h2><ol>\n<li>softmax实现</li>\n</ol>\n<h2 id=\"8-9\"><a href=\"#8-9\" class=\"headerlink\" title=\"8/9\"></a>8/9</h2><ol>\n<li>softmax实现(我死了，一个模型看了四个小时。。。。。。。无语)</li>\n<li>miniconda配置</li>\n</ol>\n<h3 id=\"8-16\"><a href=\"#8-16\" class=\"headerlink\" title=\"8/16\"></a>8/16</h3><p>期末考试即将来临，快复习啊啊啊啊啊，感觉要学的好多啊。</p>\n<h3 id=\"8-18\"><a href=\"#8-18\" class=\"headerlink\" title=\"8/18\"></a>8/18</h3><ol>\n<li>模型选择，过拟合，欠拟合</li>\n<li></li>\n</ol>\n<h3 id=\"8-21\"><a href=\"#8-21\" class=\"headerlink\" title=\"8/21\"></a>8/21</h3><ol>\n<li>LSTM在实践了（sad）<br>睡得晚感觉脑子都变旧了………为什么人可以唠嗑四五个小时，真的很神奇啊，所以说，人类还是没有那么讨厌嘛……（尼古拉斯的疯言疯语，哈哈哈）</li>\n</ol>"},{"title":"python","date":"2023-08-06T06:12:21.000Z","_content":"# Python\n\nPython的学习记录。\n\n<!--more-->\n\n\n\n## Python基础\n\n### 字符编码\n\n计算机只能处理数字，如果要处理文本，就必须把文本转化成数字才能处理。最早的计算机在设计时采用8个比特(bit)作为一个字节(byte)，所以，一个字节能表示的最大的整数就是255（二进制11111111=十进制255）。如果要表示更大的整数就要采用更多的字节。\n\n为了不与ASCII编码冲突，中国制定了`GB2312`编码，用来把中文编码进去。日本把日文编到`shift_JIS`中等等。各国有各国的标准，就会发生冲突，结果是，在很多语言混合的文本中，显示出来会有乱码。\n\n因此，Unicode字符集应运而生。Unicode把所有语言都统一到一套编码里，这样就不会出现乱码问题了。\n\nASCII编码是一个字节，而Unicode编码是两个字节。\n\n字母`A`用ASCII编码是十进制的`65`，二进制的`01000001`；\n\n字符`0`用ASCII编码是十进制的`48`，二进制的`00110000`，注意字符`'0'`和整数`0`是不同的；\n\n汉字`中`已经超出了ASCII编码的范围，用Unicode编码是十进制的`20013`，二进制的`01001110 00101101`。\n\n对于单个字符的编码，Python提供了`ord()`函数获取字符的整数表示，`chr()`函数把编码转化为对应字符。\n\n如果知道字符的整数编码，还可以用十六进制这么写str：\n\n```python\n>>>'\\u4e2d\\u6587'\n'中文'\n```\n\n这两种写法完全等价。\n\n由于Python的字符串是`str`，所以在内存中以Unicode表示，一个字符对应若干个字节。如果要在网络上传输，或者要保存到磁盘上，就需要把`str`变为以字节为单位的`bytes`。\n\nPython对`bytes`类型的数据用带`b`前缀的单引号或双引号表示：\n\n```python\nx = b'ABC'\n```\n\n纯英文的`str`可以用`ASCII`编码为`bytes`，内容是一样的，含有中文的`str`可以用`UTF-8`编码为`bytes`。含有中文的`str`无法用`ASCII`编码，Python会报错。\n\n在`bytes`中，无法显示为ASCII字符的字节，用`\\x##`显示。\n\n反过来，如果我们从网络或磁盘上读取了字节流，那么读到的数据就是`bytes`要把`bytes`变为`str`，就需要用`decode()`方法：\n\n```python\n>>>b'ABC'.decode('ascii')\n'ABC'\n>>>b'\\xe4\\xb8\\xad\\xe6\\x96\\x87',decode('utf-8')\n'中文'\n```\n\n如果`bytes`中包含无法解码的字节，`decode()`方法会报错。\n\n如果`bytes`中只有一小部分无效的字节，可以传入`errors='ignore'`忽略错误的字节：\n\n由于Python源码是一个文本文件，所以，当你的源代码中包含了中文的时候，在保存源代码时，就需要务必指定保存UTF-8编码。当Python解释器读取源码时，为了让它按`UTF-8`编码读取，我们通常在文件开头写上这两行：\n\n```python\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n```\n\n第一行注释是为了告诉Linux/OS X 系统，这是一个Python可执行程序，Windows系统会忽略这个注释；\n\n第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，你在源代码中写入的中文输入可能会出现乱码。\n\n在Python中，采用的格式化字符串的方式是与C语言一致的，用`%`实现。\n\n| 占位符 | 替换内容     |\n| :----- | :----------- |\n| %d     | 整数         |\n| %f     | 浮点数       |\n| %s     | 字符串       |\n| %x     | 十六进制整数 |\n\n其中，格式化整数和浮点数还可以制定是否补0和整数与小数的为数：\n\n```python\nprint('#2d-%02d' % (3,1))\nprint('%.2f' % 3.1415926)\n```\n\n如果你不太确定应该用什么，`%s`永远起作用，它会把任何数据类型转换为字符串：\n\n有些时候，字符串里面的`%`是一个普通字符怎么办？这个时候就需要转义，用`%%`来表示一个`%`：\n\n#### format()\n\n另一种格式化字符串的方法是使用字符串的`format()`方法，它会用传入的参数依次替换占为符`{0}`、`{1}`、……，不过这种方式写起来比%要麻烦的多：\n\n```python\n>>> 'Hello, {0}, 成绩提升了 {1:.1f}%'.format('小明', 17.125)\n'Hello, 小明, 成绩提升了 17.1%'\n```\n\n\n\n#### `f-string`\n\n最后一种格式化字符串的方法是使用以`f`开头的字符串，称之为`f-string`，它和普通字符串不同之处在于，字符串如果包含`{xxx}`，就会以对应变量替换：\n\n```python\n>>> r = 2.5\n>>> s = 3.4 * r ** 2\n>>> print(f'The area of a child with radius {r} is {s:.2f}')\n```\n\n在上述代码中，`{r}`被变量`r`替换，`{s:.2f}`被变量`{s}`的值替换，并且`:`后面的`.2f`指定了格式化参数，因此，`{s:.2f}`的替换结果是`19.62`。\n\n\n\n## 函数\n\n### 定义函数\n\n在Python中，定义一个函数要使用`def`语句，依次写出函数名，括号，括号中的参数和冒号`:`，然后，在缩进块中编写函数体，函数的返回值用`return`语句返回。\n\n我们自定义一个求绝对值的`my_abs`函数为例：\n\n```python\ndef my_abs(x):\n    if x >= 0:\n        return x\n    else:\n        return -x\n```\n\n请注意，函数体内部的语句在执行时，一旦执行到`return`时，函数就执行完毕，并将结果返回。因此，函数内部通过条件判断和循环可以实现非常复杂的逻辑。\n\n如果没有`return`语句，函数执行完毕后也会返回结果，只是结果为`None`。`return None`可以简写成`return`。\n\n在Python交互环境中定义函数时，注意Python会出现`...`的提示。函数定义结束后需要按两次回车重新回到`>>>`提示符下。\n\n如果你已经把`my_abs()`的函数定义保存为`abstest.py`文件了，那么，可以在该文件的当前目录下启动Python解释器，用`from abstest import my_abs`来导入`my_abs()`函数，注意`abstest`是文件名（不含`.py`扩展名）：\n\n```python\n>>> from abstest import my_abs\n>>> my_abs(-9)\n9\n>>>_\n```\n\n#### 空函数\n\n如果想定义一个什么事也不做的空函数，可以用`pass`语句：\n\n```python\ndef nop():\n    pass\n```\n\n一个空函数，缺少了`pass`，代码运行就会有语法错误。\n\n#### 参数检查\n\n调用函数时，如果参数个数不对，Python解释器会自动检查出来，并抛出`TypeError`。\n\n但是如果参数类型不对，Python解释器就无法帮我们检查。试试`my_abs`和内置函数``abs`的差别：\n\n```python\n>>> my_abs('A')\nTraceback (most recent call last):\n    File \"<stdin>\",line 1, in <module>\n    File \"<stdin>\",line 2, in my_abs\nTypeError:unorderable types: str() >= int()\n>>>abs('A')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: bad operand type for abs(): 'str'\n```\n\n当传入了不恰当的参数时，内置函数`abs`会检查出错误，而我们定义的`my_abs`没有参数检查，会导致`if`语句出错，出错信息和`abs`不一样。所以这个函数定义不够完善。\n\n让我们来修改一下`my_abs`的定义，对参数类型作检查，只允许整数和浮点数类型的参数。数据类型检查可以用内置函数`isinstance()`实现：\n\n```python\ndef my_abs(x):\n    if not isinstance(x,(int,float)):\n        raise TypeError('bad operand type')\n    if x >= 0:\n        return x\n    else:\n        return -x\n```\n\n\n\n\n\n## 函数式编程\n\n### 高阶函数\n\n#### map/reduce\n\n什么是高阶函数？我们以是实际代码为例子，一步步升入概念。\n\n**变量可以指向函数**\n\n以python内置的求绝对值函数`abs()`为例，调用该函数用以下代码：\n\n```python\n>>>abs(-10)\n10\n```\n\n但是如果只写`abs`呢？\n\n```python\n>>>abs\n<built-in function abs>\n```\n\n可见`abs(function)`是函数调用，而abs是函数本身。\n\n要获得函数调用的结果我们可以把函数赋值给变量：\n\n```python\n>>>x = abs(-10)\n>>>x\n10\n```\n\n结论：函数本身因为可以赋值给变量，即：变量可以指向函数。\n\n如果一个变量指向了一个函数，那么可否通过改变量来调用这个函数？用代码验证以下：\n\n```python\n>>>f = abs\n>>>f(-10)\n10\n```\n\n说明变量`f`现在已经指向了`abs`函数本身。直接调用`abs()`函数和调用`f`完全相同。\n\n**函数名也是变量**\n\n那么函数名是什么呢？函数名其实就是指向函数的变量！对于`abs`这个函数完全可以把`abs`看成变量，它指向一个可以计算绝对值的函数！如果把`abs`指向其他对象会有什么情况发生？\n\n```python\n>>> abs = 10\n>>> abs(-10)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'int' object is not callable\n```\n\n把`abs`指向`10`后，就无法通过`abs(-10)`调用该函数了！因为`abs`这个变量已经不指向求绝对值函数而是指向一个整数`10`！\n\n当然实际代码绝对不能这么写，这里是为了说明函数名也是变量。要恢复`abs`函数，请重启Python交互环境。\n\n注：由于`abs`函数实际上是定义在`import builtins`模块中的，所以要让修改`abs`变量的指向在其它模块也生效，要用`import builtins; builtins.abs = 10`。\n\n\n\n### 偏函数\n\npython的`functools`提供了偏函数功能（Partial function）。\n\n假设要转换大量的二进制字符串，每次都传入`int(x,base = 2)`非常麻烦，于是，我们想到可以定义一个函数`int2()`，默认把`base = 2`传进去：\n\n```python\ndef int2(x,base = 2):\n\treturn int(x,base)\n\n#这样我们进行二进制转换就可以了\nint2(1000000)\n\n#Partial function\nint2 = functools.partial(int,base = 2)\nint2('123456')\n64\n```\n\nPartial function作用就是，帮助我们把一个函数的某些参数固定住（也就是设置默认值），返回一个新的函数。\n\n创建偏函数时可以接受函数对象、*args、**kw这3个参数。\n\n```python\nint2 = functools.partial(int,base = 2)\n```\n\n实际上固定了`int()`函数的关键字`base`，也就是：\n\n```python\nint2('10010')\n```\n\n相当于：\n\n```python\nkw = {'base' : 2}\nint('10010' , **kw)\n```\n\n当传入：\n\n```python\nmax2 = functools.partial(max,10)\n```\n\n实际上会把10作为*args的一部分自动加到左边：\n\n```python\nmax2(5, 6, 7)\n```\n\n相当于：\n\n```python\nargs = (10, 5, 6, 7)\nmax(*args)\n```\n\n结果为10。\n\n------------------\n\n\n\n\n\n## 模块\n\n为了编写可维护代码，我们把很多函数分组，分别放到不同的文件里，这样，每个文件包含的代码就相对较少。在python中一个.py文件就可以被称为一个模块（Module）。\n\n使用模块有什么好处？\n\n最大的好处是提高了代码的可维护性。其次，编写代码不必从0开始。\n\n其次，使用模块还可以避免函数名和变量名冲突。相同名字的函数名和变量名可以存在不同模块中。\n\n如果模块名冲突怎么办？Python又引入了按目录来组织模块的方法，称为包（Package）。\n\n每个包目录下都有一个`__init__.py`文件，这个文件是必须存在的，否则，python就会把这个文件当成普通目录而不是包。`__init__.py`可以是空文件，也可以有Python代码，因为`__init__.py`本身就是一个模块，而它的模块名就是`mycompany`。\n\n\n\n### 使用模块\n\n```python\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n' a test module '\n\n__author__ = 'Michael Liao'\n\nimport sys\n\ndef test():\n    args = sys.argv\n    if len(args)==1:\n        print('Hello, world!')\n    elif len(args)==2:\n        print('Hello, %s!' % args[1])\n    else:\n        print('Too many arguments!')\n\nif __name__=='__main__':\n    test()\n'''\n当我们在命令行运行hello模块文件时，Python解释器把一个特殊变量__name__置为__main__，而如果在其他地方导入该hello模块时，if判断将失败，因此，这种if测试可以让一个模块通过命令行运行时执行一些额外的代码，最常见的就是运行测试。\n'''\n\n```\n\n代码的第一，二行是标准注释，第一行注释可以让这个`hellow.py`文件直接在Unix/Linux/Mac上运行，第二行注释表示了.py文件本身使用了UTF-8编码；\n\n第四行是一个字符串，表示模块的文档注释，任何模块的第一行字符串都被视为模块文档注释；\n\n第六行`__author__`变量把作者写进去。\n\n后面开始就是真正的代码部分。\n\n使用`sys`模块的第一步就是导入该模块：\n\n```python\nimport sys\n```\n\n导入了sys模块后，我们就有了变量sys指向该模块，利用`sys`这个变量，就可以访问`sys`模块所有的功能。\n\nsys有一个argv变量，用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是`.py`文件的名称，例如：\n\n运行``python3 hello.py`获得的`sys.argv`就是`['hellow.py']`\n\n运行`python3 hello.py Michael`获得的`sys.argv`就是`['hello.py', 'Michael']`。\n\n\n\n如果启动Python交互环境，再导入`hello`模块：\n\n```python\n$ python3\n>>>import hello\n>>>\n```\n\n导入时，没有打印`hello world!`因为没有执行`test()`函数。\n\n调用`hello.test()`时，才能打印出`hello world!`:\n\n```python\n>>>hello.test()\nHello,world!\n```\n\n#### 作用域\n\n在一个模块中，我们可能会定义很多函数和变量，但有的函数我们希望给别人使用，有的仅仅在模块内部使用。在Python中我们是通过`_`前缀来实现的。\n\n正常函数和变量名是公开的（public），可以被直接引用。\n\n类似`__xx__`是特殊变量，可以被直接引用，但有特殊用途，比如上面的`__author__`，`__name__`就是特殊变量，`hello`模块定义的文档注释也可以用特殊变量`__doc__`访问，我们自己的变量一般不要用这种变量名。\n\n类似`_xxx`和`__xxx`这样的函数或变量就是非公开的（private），不应该被直接引用，比如`_abc`，`__abc`等；\n\n之所以我们说，private函数和变量“不应该”被直接引用，而不是“不能”被直接引用，是因为Python并没有一种方法可以完全限制访问private函数或变量，但是，从编程习惯上不应该引用private函数或变量。\n\nprivate函数或变量不应该被别人引用，那它们有什么用呢？请看例子：\n\n```python\ndef _private_1(name):\n    return 'Hello, %s' % name\n\ndef _private_2(name):\n    return 'Hi, %s' % name\n\ndef greeting(name):\n    if len(name) > 3:\n        return _private_1(name)\n    else:\n        return _private_2(name)\n```\n\n我们在模块里公开`greeting()`函数，而把内部逻辑用private函数隐藏起来了，这样，调用`greeting()`函数不用关心内部的private函数细节，这也是一种非常有用的代码封装和抽象的方法，即：\n\n外部不需要引用的函数全部定义成private，只有外部需要引用的函数才定义为public。\n\n\n\n\n\n## 面向对象编程（OOP）\n\nOOP把对象作为基本单元，一个对象包含了数据和操作数据的函数。\n\nOOP把计算机程序视为一系列命令的集合，而每个对象都可以接受其他对象发过来的消息，并处理这些消息，计算机程序的执行就是一系列消息在对象之间传递。\n\n在Python中，所有的数据类型都可以视为对象，当然也可以自定义对象。自定义的对象数据类型就是面向对象中的类（Class）的概念。\n\n\n\n### 类和实例\n\nOOP最重要概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，实例是根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但是各自的数据可能不同。\n\n仍以Student类为例，在Python中，定义类是通过`class`关键字：\n\n```python\nclass Student(object):\n    pass\n```\n\n`calss`后面紧接着是类名，即`Student`类创建出`Student`实例，创建实例是通过类名+()实现的：\n\n```python\n>>> bart = Student()\n>>> bart\n<__main__.Student object at 0x10a67a590>\n>>> Student\n<class '__main__.Student'>\n```\n\n可以看到，变量`bart`指向的就是一个`Student`的实例，后面的`0x10a67a590`是内存地址，每个object的地址都不一样，而`Student`本身则是一个类。\n\n可以自由地给一个实例变量绑定属性，比如，给实例`bart`绑定一个`name`属性：\n\n```python\n>>> bart.name = 'Bart Simpson'\n>>> bart.name\n'Bart Simpson'\n```\n\n由于类可以起到模板作用，因此可以在创建实例的时候，把一些我们任务必须绑定的属性强制填进去。通过定义一个特殊的`__init__`方法，创建实例的时候就把`name`，`score`等属性绑定上去：\n\n```python\nclass Student(object):\n    \n    def __init__(self,name,score):\n        self.name = name\n        self.score = score\n```\n\n注意，`__init__`方法的第一个参数永远是`self`，表示创建的实例本身，因此，在`__init__`方法内部，就可以把各种属性绑定到`self`,因为`self`就指向创建的实例本身。\n\n有了`__init__`方法，在创建实例的时候，就不能传入空的参数了，必须传入与`__init__`方法相匹配的参数，但是self不需要传，Python解释器会自己把实例变量传进去。\n\n```python\n>>>bart = Student('Bart Simpson',59)\n>>>bart.name\n'Bart Simpson'\n>>>bart.score\n59\n```\n\n和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量`self`，并且，调用时不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所有，你仍然可以用默认参数、可变参数、关键字参数、命名关键字参数。\n\n#### 数据封装\n\nOOP的一个重要特点就是数据封装。在上面的`Student`类中，每个实例就拥有各自的`name`和`score`这些数据。我们可以通过函数来访问这些数据，比如打印一个学生的成绩。但是`Student`实例本身就拥有这些数据，要访问这些数据，就没有必要从外面的函数去访问，可以直接在`Student`类的内部定义访问数据的函数，这样，就把“数据”给封装起来了。这些封装数据的函数是和`Student`类本身是关联起来的，我们称之为类的方法：\n\n```python\nclass Student(object):\n    \n    def __init__(self,name,score):\n        self.name = name\n        self.score = score\n    \n    def print_score(self):\n        print('%s : %s' % (self.name, self.score))\n```\n\n要定义一个方法，除了第一个参数是`self`以外，其他和普通函数一样。要调用另一个方法，只需要在实例变量上直接调用，除了`self`不用传递，其他参数正常传入：\n\n```python\n>>>bart.print_score()\nBart Simpson: 59\n```\n\n\n\n### 访问限制\n\n在class内部，可以有属性和方法，而外部代码可以通过直接调用实例变量的方法来操作数据，这样，就隐藏了内部的复杂逻辑。\n\n但是从`Student`类的定义来看，外部代码还是可以自由的修改一个实例的`name`、`scorre`属性。\n\n如果要让内部属性不被外部访问，可以把属性名称前加两个下划线`__`，在python中，实例的变量名如果以`__`开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，所以，我们把`Student`类改一改：\n\n```python\nclass Student(object):\n    def __init__(self,name,score):\n        self.__name = name\n        self.__score = score\n        \n    def print_score(self):\n        prnint('%s %s' % (self.__name, self.__score))\n```\n\n改完后，对外部代码来说没什么变动，但以及无法从外部访问`实例变量.__name`和`实例变量.__score`了\n\n但是如果外部代码要获取score怎么办？可以给`Student`类增加`set_score`方法：\n\n```python\nclass Student(object):\n    ...\n    \n    def set_score(self,score):\n        self.__score = score\n```\n\n你也许会问，原先那种直接通过`bart.score = 99`也可以修改啊，为什么要定义一个方法大费周折？因为在方法中，可以对参数做检查，避免传入无效的参数：\n\n```python\nclass Student(object):\n    ...\n\n    def set_score(self, score):\n        if 0 <= score <= 100:\n            self.__score = score\n        else:\n            raise ValueError('bad score')\n```\n\n\n\n而不能直接访问`__name`是因为Python解释器对外把`__name`变量改成了`_Student__name`，所以，仍然可以通过`_Student__name`来访问`__name`变量：\n\n```python\n>>> bart._Student__name\n'Bart Simpson'\n```\n\n但是强烈建议你不要这么干，因为不同版本的Python解释器可能会把`__name`改成不同的变量名。\n\n\n\n```python\n>>> bart = Student('Bart Simpson', 59)\n>>> bart.get_name()\n'Bart Simpson'\n>>> bart.__name = 'New Name' # 设置__name变量！\n>>> bart.__name\n'New Name'\n```\n\n表面上看，外部代码“成功”地设置了`__name`变量，但实际上这个`__name`变量和class内部的`__name`变量*不是*一个变量！内部的`__name`变量已经被Python解释器自动改成了`_Student__name`，而外部代码给`bart`新增了一个`__name`变量。不信试试：\n\n```python\n>>> bart.get_name() # get_name()内部返回self.__name\n'Bart Simpson'\n```\n\n\n\n\n\n### 继承和多态\n\n在OOP程序设计中，当我们定义了一个class的时候，可以从某个现有的class继承，新的class称为子类（Subclass），而被继承的class称为基类，父类或超类（Base class，super class）。\n\n比如我们编写了一个名为`Animal`的class，有一个`run`方法可以直接打印：\n\n```python\nclass Animal(object):\n    def run(self):\n        print('Animal is running……')\n```\n\n当我们要编写dog和cat类时，就可以直接从`Animal`类继承：\n\n```python\nclass Dog(Animal):\n    pass\n\nclass Cat(Animal):\n    pass\n```\n\n对于`Dog`类来说，`Animal`就是它的父类，对于`Animal`类来说，`Dog`就是它的子类。`Cat`和`Dog`类似。\n\n继承有什么好处?最大的好处是，子类获得了父类的全部功能。由于`Animal`实现了`run()`方法，因此，`Dog`和`Cat`作为它的子类，什么事没干就自动拥有了`run()`方法：\n\n```python\ndog = Dog()\ndog.run\n\n#cat与dog的处理方式一致\ncat = Cat()\ncat.run\n```\n\n运行结果如下\n\n```python\nAnimal is running...\nAnimal is running...\n```\n\n\n\n当然，也可以对子类增加一些方法，比如Dog类：\n\n```python\nclass Dog(Animal):\n\n    def run(self):\n        print('Dog is running...')\n\n    def eat(self):\n        print('Eating meat...')\n```\n\n\n\n继承的第二个好处需要我们对代码做一点改进。你看到了，无论是`Dog`还是`Cat`，它们`run()`的时候，显示的都是`Animal is running...`，符合逻辑的做法是分别显示`Dog is running...`和`Cat is running...`，因此，对`Dog`和`Cat`类改进如下：\n\n```python\nclass Dog(Animal):\n\n    def run(self):\n        print('Dog is running...')\n\nclass Cat(Animal):\n\n    def run(self):\n        print('Cat is running...')\n```\n\n\n\n再次运行结果如下：\n\n```python\nDog is running...\nCat is running...\n```\n\n当子类和父类都存在相同的`run()`方法时，我们说，子类的`run`覆盖了父类的`run`，在代码运行的时候，总是会调用子类的`run`。这样我们就获得了继承的另一个好处：多态。\n\n要理解什么是多态，我们首先要对数据类型再作一点说明。当我们定义一个class的时候，我们实际上就定义了一种数据类型。\n\n判断一个变量是否是某个类型可以用`isinstance()`判断：\n\n```python\n>>> isinstance(a, list)\nTrue\n>>> isinstance(b, Animal)\nTrue\n>>> isinstance(c, Dog)\nTrue\n```\n\n看来`a`、`b`、`c`确实对应着`list`、`Animal`、`Dog`这3种类型。\n\n但是等等，试试：\n\n```Python\n>>> isinstance(c, Animal)\nTrue\n```\n\n看来`c`不仅仅是`Dog`，`c`还是`Animal`！\n\n在继承关系中，如果一个数据类型是某个数据类型的子类，那它的数据类型也可以被看成是父类。但是反过来就不行：\n\n```python\n>>>b = Animal()\n>>>isinstance(b,Dog)\nFalse\n```\n\n\n\n理解多态的好处，我们还需要编写一个函数，接受一个`Anmial`类型的变量：\n\n```python\ndef run_twice(animal):\n    animal.run()\n    animal.run()\n```\n\n当我们传入`Animal`的实例时，`run_twice()`就打印出：\n\n```python\n>>> run_twice(Animal())\nAnimal is running...\nAnimal is running...\n```\n\n传入`Dog`的实例时，`run_twice()`打印：\n\n```python\n>>>run_twice(Dog())\nDog is runninng……\nDog is runninng……\n```\n\n现在我们再定义一个`Tortoise`类型，也从`Animal`派生：\n\n```python\nclass Tortoise(Animal):\n    def run(self)\n    \tprint('Tortoise is running slowwly…')\n```\n\n当我们调用`run_twice()`时，传入`Tortoise`的实例：\n\n```python\n>>> run_twice(Tortoise())\nTortoise is running slowly...\nTortoise is running slowly...\n```\n\n你会发现，新增一个`Animal`的子类，不必对`run_twice`做任何修改，实际上，任何依赖`Animal`作为参数的函数或者方法都可以不加修改的正常运行，原因就在于多态。\n\n多态的好处是，当我们传入`Dog`，`Cat`，`Tortoise`……时，我们只需要接受`Anmial`类型就可以了，因为`Dog`，`Cat`，`Tortoise`……都是`Animal`类型，然后，按照`Animal`类型进行操作即可。由于`Anmial`类型有`run()`方法，因此，传入的任意类型，只要是`Aniaml`类或者是子类，就会自动调用实际类型的`run()`方法，这就是多态的意思。\n\n对于一个变量，我们只需要知道它是`Animal`类型，无需确切地知道它的子类型，就可以放心地调用`run()`方法，而具体调用的`run()`方法是作用在`Animal`、`Dog`、`Cat`还是`Tortoise`对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：调用方只管调用，不管细节，而当我们新增一种`Animal`的子类时，只要确保`run()`方法编写正确，不用管原来的代码是如何调用的。这就是著名的“开闭”原则：\n\n对扩展开放：允许新增`Animal`子类；\n\n对修改封闭：不需要修改依赖`Animal`类型的`run_twice()`等函数。\n\n\n\n### 获取对象信息\n\n当我们拿到一个对象引用时，如何知道这个对象是什么类型，有那些方法呢？\n\n#### 使用`type()`\n\n我们判断对象类型，使用`type`函数，基本类型都可以用`type`判断：\n\n```python\n>>> type(123)\n<class 'int'>\n```\n\n如果一个对象指向函数或者类，也可以用`type()`判断。但是`type()`函数返回的是什么类型呢？它返回对应的Class类型。\n\n\n\n如果要判断一个对象是否是函数怎么办？可以使用`types`模块中定义的常量：\n\n```python\n>>> import types\n>>> def fn():\n...     pass\n...\n>>> type(fn)==types.FunctionType\nTrue\n>>> type(abs)==types.BuiltinFunctionType\nTrue\n>>> type(lambda x: x)==types.LambdaType\nTrue\n>>> type((x for x in range(10)))==types.GeneratorType\nTrue\n```\n\n#### 使用dir()\n\n如果要获得一个对象的所有属性和方法，可以使用`dir()`函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法：\n\n```python\n>>> dir('ABC')\n['__add__', '__class__',..., '__subclasshook__', 'capitalize', 'casefold',..., 'zfill']\n```\n\n紧接着，可以测试对象的属性：\n\n```python\n>>>hasattr(obj,'x') #有x属性吗？\n>>>setattr(obj,'y') #设置x属性\n>>> getattr(obj, 'x') # 获取属性'x'\n```\n\n可以传入一个default参数，如果属性不存在，就返回默认值：\n\n```python\n>>> getattr(obj, 'z', 404) # 获取属性'z'，如果不存在，返回默认值404\n404\n```\n\n也可以获得对象的方法：\n\n```python\n>>> hasattr(obj, 'power') # 有属性'power'吗？\nTrue\n>>> getattr(obj, 'power') # 获取属性'power'\n<bound method MyObject.power of <__main__.MyObject object at 0x10077a6a0>>\n```\n\n\n\n## 面向对象高级编程\n\n### 使用\\_\\_slots\\_\\_\n\n正常情况下，当我们定义了一个class，创建了一个clsaa的实例后，我们可以给该实例绑定任何实例和方法，这就是动态语言的灵活性。先定义class：\n\n```python\nclass Student(object):\n    pass\n```\n\n然后，尝试给实例绑定一个属性：\n\n```python\n>>>s = Student\n>>>s.name = 'Michael' #动态给实例绑定一个对象\n>>>print('s.name')\nMicheal\n```\n\n还可以尝试给实例绑定一个方法：\n\n```python\n>>>def set_age(self,age): #定义一个函数作为实例方法\n    \tself.age=age\n    \n>>>from type import MethodType\n>>>s.set_age = MethodeType(set_age, s) #给实例绑定一个方法\n>>>s.set_age(25) #调用实例方法\n>>>s.age #测试结果\n25\n```\n\n但是==给一个实例绑定的方法对另一个实例是不起作用的。==\n\n\n\n\n\n## 进程和线程\n\n### 多进程\n\nUnix/Linux操作系统提供了一个`fork`调用，普通的函数调用，调用一次返回一次，但是`fork`调用一次，返回两次，因为操作系统自动把当前进程（父进程）复制了一份（称为子进程），然后分别在父进程和子进程内返回。\n\n子进程永远返回`0`，而父进程返回子进程的ID，子进程只需要调用`getppid()`就可以拿到父进程的ID。\n\nPython的`os`模块封装了常见的系统调用，其中就包括`fork`，可以在Python程序中轻松创建子进程：\n\n```python\nimport os\n\nprint('Process (%s) start...' % os.getpid())\n# Only works on Unix/Linux/Mac:\npid = os.fork()\nif pid == 0:\n    print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))\nelse:\n    print('I (%s) just created a child process (%s).' % (os.getpid(), pid))\n```\n\n如果要启动大量子进程，可以用进程池的方法批量创建子进程：\n\n```python\nfrom multiprocessing import Pool\nimport os, timme, random\n\ndef long_time_task(name):\n    print('Run task %s (%s)...' % (name, os.getpid()))\n    start = time.time()\n    time.sleep(random.random() * 3)\n    end = time.time()\n    print('Task %s runs %0.2f seconds.' % (name, (end - start)))\n    \nif __name__=='__main__':\n    print('Parent process %s.' % os.getpid())\n    p = Pool(4)\n    for i in range(5):\n        p.apply_async(long_time_task, args=(i,))\n    print('Waiting for all subprocesses done...')\n    p.close()\n    p.join()\n    print('All subprocesses done.')\n```\n\n代码解读：\n\n对`Pool`对象调用`join()`方法会等待所有子进程执行完毕，调用`join()`之前必须先调用`close()`，调用`close()`之后就不能继续添加新的`Process`了。\n\n\n\n例子：\n\n```python\npool = Pool(8) #可以同时跑8个进程\n\tpool.map(get_all, [i for i in range(10)])\n    pool.close\n    pool.join()\n```\n\n这里的`pool.close()`是说关闭pool，使其不在接受新的（主进程）任务。\n\n这里的`pool.join()`是说：主进程阻塞后，让子进程继续运行完成，子进程运行完成后，再把主进程全部关掉。\n\n\n\n#### 子进程\n\n很多时候，子进程并不是自生，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。\n\n==subprocess==模块可以让我们非常方便的启动一个子进程，然后控制其输入和输出。\n\n下面的例子演示了如何在Python代码中运行命令`nslookup www.python.org`，这和命令行的效果是一样的：\n\n```python\nimport subprocess\n\nprint('$ nslookup www.python.org')\nr = subprocess.call(['nslookup','www.python.org'])\nprint('Exit code:', r)\n```\n\n```python\n$ nslookup www.python.org\nServer:\t\t192.168.19.4\nAddress:\t192.168.19.4#53\n\nNon-authoritative answer:\nwww.python.org\tcanonical name = python.map.fastly.net.\nName:\tpython.map.fastly.net\nAddress: 199.27.79.223\n\nExit code: 0\n```\n\n\n\n#### subprocess模块详解\n\n`subprocess`模块是**Python 2.4**中新增的一个模块，它允许你生成新的进程，连接到它们的**input/output/error**管道，并获取它们的返回（状态）码。这个模块的目的在于替换几个旧的模块和方法，如：\n\n```python\nos.system\nos.spawn*\n```\n\n\n\n**1.subprocess模块中的常用函数**\n\n| 函数                            | 描述                                                         |\n| ------------------------------- | ------------------------------------------------------------ |\n| subprocess.run()                | py3.5中新增的函数。执行制定的命令，等待命令执行完毕后返回一个包含执行结果的CompletedProcess类的实例。 |\n| subprocess.call()               | 执行指定命令，返回命令执行状态，其功能类似于`os.system(cmd)` |\n| subprocess.check_call()         | python2.5中新增的函数，执行制定的命令，如果执行成功则返回状态码，否则抛出异常。其功能等价于subprocess.run(..., check = True) |\n| subprocess.check_output()       | python 2.7中新增的函数。执行制定的命令，如果执行状态码为0则返回命令执行结果。都则抛出异常。 |\n| subprocess.getoutput(cmd)       | 接受字符串格式的命令，执行命令并返回执行结果，其功能类似于os.popen(cmd).reead()和commands.getoutput(cmd) |\n| subprocess.getstatusoutput(cmd) | 执行cmd命令，返回一个元组（命令执行状态，命令执行结果输出），其功能类似于commands,getstatusoutput() |\n\n\n\n> 说明：\n>\n> 在Python 3.5之后的版本中，官方文档中提倡通过subprocess.run()函数替代其他函数来使用subproccess模块的功能；\n> 在Python 3.5之前的版本中，我们可以通过subprocess.call()，subprocess.getoutput()等上面列出的其他函数来使用subprocess模块的功能；\n> subprocess.run()、subprocess.call()、subprocess.check_call()和subprocess.check_output()都是通过对subprocess.Popen的封装来实现的高级函数，因此如果我们需要更复杂功能时，可以通过subprocess.Popen来完成。\n> subprocess.getoutput()和subprocess.getstatusoutput()函数是来自Python 2.x的commands模块的两个遗留函数。它们隐式的调用系统shell，并且不保证其他函数所具有的安全性和异常处理的一致性。另外，它们从Python 3.3.4开始才支持Windows平台。\n\n\n\n**上面各函数的定义以及参数说明**\n\n函数参数列表\n\n```python\nsubprocess.run(args, *, stdin=None, input=None, stdout=None, stderr=None, shell=False, timeout=None, check=False, universal_newlines=False)\n\nsubprocess.call(args, *, stdin=None, stdout=None, stderr=None, shell=False, timeout=None)\n\nsubprocess.check_call(args, *, stdin=None, stdout=None, stderr=None, shell=False, timeout=None)\n\nsubprocess.check_output(args, *, stdin=None, stderr=None, shell=False, universal_newlines=False, timeout=None)\n\nsubprocess.getstatusoutput(cmd)\n\nsubprocess.getoutput(cmd)\n```\n\n参数说明：\n\n- args：要执行的shell命令，默认应该是一个字符串序列，如['df', '-Th']，也可以是一个字符串，但要把shell的参数的值设置为True\n\n- shell：如果sehll为True，那么指定的命令将通过shell执行。\n\n- check：如果check参数的值是True，且执行命令的进程以非0状态码退出，则会抛出一个CalledProcessError的异常，且该异常会包含参数、退出状态码、以及stdout和stderr\n\n- `stdout, stderr：input`： 该参数是传递给Popen.communicate()，通常该参数的值必须是一个字节序列，如果universal_newlines=True，则其值应该是一个字符串。\n\n  1. run()函数默认不会捕获命令执行结果的正常输出和错误输出，如果我们向获取这些内容需要传递subprocess.PIPE，然后可以通过返回的CompletedProcess类实例的stdout和stderr属性或捕获相应的内容；\n\n  2. call()和check_call()函数返回的是命令执行的状态码，而不是CompletedProcess类实例，所以对于它们而言，stdout和stderr不适合赋值为subprocess.PIPE；\n     3\n  3. check_output()函数默认就会返回命令执行结果，所以不用设置stdout的值，如果我们希望在结果中捕获错误信息，可以执行stderr=subprocess.STDOUT。\n\n- `universal_newlines`： 该参数影响的是输入与输出的数据格式，比如它的值默认为False，此时stdout和stderr的输出是字节序列；当该参数的值设置为True时，stdout和stderr的输出是字符串。\n\n\n\n**3.subprocess.CompletedProcess类介绍**\n\n需要说明的是，`subprocess.run()`函数是Python3.5中新增一个高级函数，其返回值是一个`subprocess.CompletedProcess`类的实例，因此，subprocess,completedProcess类也是Python 3.5中才存在的。它表示的是一个以结束进程的状态信息。\n\n\n\n#### subprocess.Popen介绍\n\n该类用于在一个新的程序中执行一个子程序。前面我们提到过，上面介绍的这些函数艘是基于`subprocess.Popen`类实现的，通过使用这些被封装的高级函数可以很方便的完成一些常见需求。由于`subprocess`模块底层的进程创建和管理是有Popen类来处理的，因此，当我们无法通过上面哪些高级函数来实现一些不太常见的功能时就可以通过subprocess.Popen类提供灵活的api来完成。\n\n1.subprocess.Popen构造函数\n\n```python\nclass subprocess.Popen(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, \n    preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=False,\n    startup_info=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=())\n```\n\n- args： 要执行的shell命令，可以是字符串，也可以是命令各个参数组成的序列。当该参数的值是一个字符串时，该命令的解释过程是与平台相关的，因此通常建议将args参数作为一个序列传递。\n- bufsize： 指定缓存策略，0表示不缓冲，1表示行缓冲，其他大于1的数字表示缓冲区大小，负数 表示使用系统默认缓冲策略。\n- stdin, stdout, stderr： 分别表示程序标准输入、输出、错误句柄。\n- preexec_fn： 用于指定一个将在子进程运行之前被调用的可执行对象，只在Unix平台下有效。\n- close_fds： 如果该参数的值为True，则除了0,1和2之外的所有文件描述符都将会在子进程执行之前被关闭。\n- shell： 该参数用于标识是否使用shell作为要执行的程序，如果shell值为True，则建议将args参数作为一个字符串传递而不要作为一个序列传递。\n- cwd： 如果该参数值不是None，则该函数将会在执行这个子进程之前改变当前工作目录。\n- env： 用于指定子进程的环境变量，如果env=None，那么子进程的环境变量将从父进程中继承。如果env!=None，它的值必须是一个映射对象。\n- universal_newlines： 如果该参数值为True，则该文件对象的stdin，stdout和stderr将会作为文本流被打开，否则他们将会被作为二进制流被打开。\n- startupinfo和creationflags： 这两个参数只在Windows下有效，它们将被传递给底层的CreateProcess()函数，用于设置子进程的一些属性，如主窗口的外观，进程优先级等。\n\n\n\n2. subprocess.Popen类的实例可调用的方法\n\n| 方法                                        | 描述                                                         |\n| ------------------------------------------- | ------------------------------------------------------------ |\n| Popen.poll()                                | 用于检查子进程（命令）是否已经执行结束，没结束返回None，结束后返回状态码。 |\n| Popen.wait(timeout=None)                    | 等待子进程结束，并返回状态码；如果在timeout指定的秒数之后进程还没有结束，将会抛出一个TimeoutExpired异常。 |\n| Popen.communicate(imput=None, timeout=None) | 该方法可用于来与程序进行交互，比如发送数据到stdin，从stdout和stderr读取数据，直到达到文件末尾。 |\n| Popen.send_signal(signal)                   | 发送指定的信号给这个子进程                                   |\n| Popen.terminate()                           | 停止这个子进程                                               |\n| Popen.kill                                  | 杀死该子进程                                                 |\n\n\n\n\n\n#### 进程间通信\n\n`Process`之间肯定是需要通信的，操作系统提供了很多的机制来实现进程间的通信。Python的`nultiprocessing`模块包装了底层的机制，提供了`Queue`，`Pipes`等多种方式来交换数据。\n\n我们以`Queue`为例，在父进程中创建两个子进程，一个往`Queue`里写入数据，一个从`Queue`里读取数据：\n\n```python\nfrom multiprocessing import Process, Queue\nimport os, time, rendom\n\n#写数据进程执行的代码\ndef write(q):\n    print('Pricess to write: %s' % os.getpid())\n    for value in ['A', 'B', 'C']:\n        print('Put %s to queue...' % value)\n        q.put(value)\n        time.sleep(random.random())\n\n#读数据进程执行的代码\ndef read(q):\n    print('Process to readL %s' % os.getpid)\n\twrite True:\n        value = q.get(True)\n        print('Get %s from queue.' % value)\n\nif __name__ = '__main__':\n    #父进程创建的Queue，并传给各个子进程\n    q = Queue()\n    pw = Process(targe=write, args=(q,))\n    pr = Process(target=read, args=(q,))\n    #启动子进程pr,读取：\n    pr.start()\n    #等待pw结束\n    pw.join()\n    #pr进程里是四循环，无法等待其结束，只能强行终止：\n    pr.terminate()\n```\n\n---------\n\n\n\n\n\n### 多线程\n\n进程是由若干个线程组成的，一个进程至少有一个线程。\n\n由于线程的操作系统直接支持的执行单元，因此，高级语言通常都内置多线程的支持，Python也不例外，并且，Python的线程是真正的Posix Thread，而不是模拟出来的线程。\n\nPython的标准库提供了两个模块：`_thread`和`threading`，`_thread`是低级模块，`threading`是高级模块，对`_thread`进行了封装。绝大多数情况下，我们只需要使用`_threading`这个高级模块。\n\n启动一个线程就是把一个函数传入并创建`Thread`实例，然后调用`start()`开始执行：\n\n```python\nimport time, threading\n\n#新线程执行的代码：\ndef loop:\n    print('thread %s is runing...' % threading.current_thread().name)\n    n = 0\n    while n < 5:\n        n = n+ 1\n        print('thread %s >>> %s' % (threading.current_thread ().name))\n        time.sleep(1)\n    print('thread %s ended.' % threading.current_thread().name)\n    \nprint('thread %s is running...' % threading.current_thread().name)\nt = threading.Thread(target=loop, name = 'LoopThread')\nt.start()\nt.join()\nprint('thread %s ended.' % threading.current_thread().name)\n```\n\n由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的`threading`模块有个`current_thread()`函数，它永远返回当前线程的实例。主线程实例的名字叫`MainThread`，子线程的名字在创建时指定，我们用`LoopThread`命名子线程。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为`Thread-1`，`Thread-2`……\n\n\n\n#### Lock\n\n多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，所有的变量都由所有的线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时修改一个变量，把内容给该乱了。\n\n所以我们希望创建一把锁，当某个程序开始执行一个线程时，我们说，该线程获得的锁，因此其他线程不能同时执行该线程，只能等待，，直到锁被释放后，获得该锁以后才能改。由于锁只有一个，无论多少个线程，同一时刻最多只有一个线程持有该锁，所以不会造成修改冲突。创建一个锁就是通过`threading.Lock()`来实现：\n\n```python\nbalance = 0\nlock = threading.Lock()\n\ndef run_thread(n):\n    for i in range(10000)\n    #先要获取锁：\n    lock.acquire()\n    try:\n        change_it(n)\n    finally:\n        #改完了一定要释放锁\n        lock.release()\n```\n\n当多个线程同时执行`lock.acquire()`时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。\n\n获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用`try...finally`来确保锁一定会被释放。\n\n锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。\n\n\n\n#### 多核CPU\n\n如果你不幸拥有一个多核CPU，你肯定在想，多核应该可以同时执行多个线程。\n\n如果写一个死循环的话，会出现什么情况呢？\n\n打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。\n\n我们可以监控到一个死循环线程会100%占用一个CPU。\n\n如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。\n\n要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。\n\n试试用Python写个死循环：\n\n```python\nimport threading, multiprocessing\n\ndef loop():\n    x = 0\n    while True:\n        x = x ^ 1\n\nfor i in range(multiprocessing.cpu_count()):\n    t = threading.Thread(target=loop)\n    t.start()\n```\n\n启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。\n\n但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？\n\n因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。\n\nGIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。\n\n所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。\n\n不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。\n","source":"_posts/python.md","raw":"---\ntitle: python\ndate: 2023-08-06 14:12:21\ncategories: #分类\n- python\ntags:\n- python\n\n---\n# Python\n\nPython的学习记录。\n\n<!--more-->\n\n\n\n## Python基础\n\n### 字符编码\n\n计算机只能处理数字，如果要处理文本，就必须把文本转化成数字才能处理。最早的计算机在设计时采用8个比特(bit)作为一个字节(byte)，所以，一个字节能表示的最大的整数就是255（二进制11111111=十进制255）。如果要表示更大的整数就要采用更多的字节。\n\n为了不与ASCII编码冲突，中国制定了`GB2312`编码，用来把中文编码进去。日本把日文编到`shift_JIS`中等等。各国有各国的标准，就会发生冲突，结果是，在很多语言混合的文本中，显示出来会有乱码。\n\n因此，Unicode字符集应运而生。Unicode把所有语言都统一到一套编码里，这样就不会出现乱码问题了。\n\nASCII编码是一个字节，而Unicode编码是两个字节。\n\n字母`A`用ASCII编码是十进制的`65`，二进制的`01000001`；\n\n字符`0`用ASCII编码是十进制的`48`，二进制的`00110000`，注意字符`'0'`和整数`0`是不同的；\n\n汉字`中`已经超出了ASCII编码的范围，用Unicode编码是十进制的`20013`，二进制的`01001110 00101101`。\n\n对于单个字符的编码，Python提供了`ord()`函数获取字符的整数表示，`chr()`函数把编码转化为对应字符。\n\n如果知道字符的整数编码，还可以用十六进制这么写str：\n\n```python\n>>>'\\u4e2d\\u6587'\n'中文'\n```\n\n这两种写法完全等价。\n\n由于Python的字符串是`str`，所以在内存中以Unicode表示，一个字符对应若干个字节。如果要在网络上传输，或者要保存到磁盘上，就需要把`str`变为以字节为单位的`bytes`。\n\nPython对`bytes`类型的数据用带`b`前缀的单引号或双引号表示：\n\n```python\nx = b'ABC'\n```\n\n纯英文的`str`可以用`ASCII`编码为`bytes`，内容是一样的，含有中文的`str`可以用`UTF-8`编码为`bytes`。含有中文的`str`无法用`ASCII`编码，Python会报错。\n\n在`bytes`中，无法显示为ASCII字符的字节，用`\\x##`显示。\n\n反过来，如果我们从网络或磁盘上读取了字节流，那么读到的数据就是`bytes`要把`bytes`变为`str`，就需要用`decode()`方法：\n\n```python\n>>>b'ABC'.decode('ascii')\n'ABC'\n>>>b'\\xe4\\xb8\\xad\\xe6\\x96\\x87',decode('utf-8')\n'中文'\n```\n\n如果`bytes`中包含无法解码的字节，`decode()`方法会报错。\n\n如果`bytes`中只有一小部分无效的字节，可以传入`errors='ignore'`忽略错误的字节：\n\n由于Python源码是一个文本文件，所以，当你的源代码中包含了中文的时候，在保存源代码时，就需要务必指定保存UTF-8编码。当Python解释器读取源码时，为了让它按`UTF-8`编码读取，我们通常在文件开头写上这两行：\n\n```python\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n```\n\n第一行注释是为了告诉Linux/OS X 系统，这是一个Python可执行程序，Windows系统会忽略这个注释；\n\n第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，你在源代码中写入的中文输入可能会出现乱码。\n\n在Python中，采用的格式化字符串的方式是与C语言一致的，用`%`实现。\n\n| 占位符 | 替换内容     |\n| :----- | :----------- |\n| %d     | 整数         |\n| %f     | 浮点数       |\n| %s     | 字符串       |\n| %x     | 十六进制整数 |\n\n其中，格式化整数和浮点数还可以制定是否补0和整数与小数的为数：\n\n```python\nprint('#2d-%02d' % (3,1))\nprint('%.2f' % 3.1415926)\n```\n\n如果你不太确定应该用什么，`%s`永远起作用，它会把任何数据类型转换为字符串：\n\n有些时候，字符串里面的`%`是一个普通字符怎么办？这个时候就需要转义，用`%%`来表示一个`%`：\n\n#### format()\n\n另一种格式化字符串的方法是使用字符串的`format()`方法，它会用传入的参数依次替换占为符`{0}`、`{1}`、……，不过这种方式写起来比%要麻烦的多：\n\n```python\n>>> 'Hello, {0}, 成绩提升了 {1:.1f}%'.format('小明', 17.125)\n'Hello, 小明, 成绩提升了 17.1%'\n```\n\n\n\n#### `f-string`\n\n最后一种格式化字符串的方法是使用以`f`开头的字符串，称之为`f-string`，它和普通字符串不同之处在于，字符串如果包含`{xxx}`，就会以对应变量替换：\n\n```python\n>>> r = 2.5\n>>> s = 3.4 * r ** 2\n>>> print(f'The area of a child with radius {r} is {s:.2f}')\n```\n\n在上述代码中，`{r}`被变量`r`替换，`{s:.2f}`被变量`{s}`的值替换，并且`:`后面的`.2f`指定了格式化参数，因此，`{s:.2f}`的替换结果是`19.62`。\n\n\n\n## 函数\n\n### 定义函数\n\n在Python中，定义一个函数要使用`def`语句，依次写出函数名，括号，括号中的参数和冒号`:`，然后，在缩进块中编写函数体，函数的返回值用`return`语句返回。\n\n我们自定义一个求绝对值的`my_abs`函数为例：\n\n```python\ndef my_abs(x):\n    if x >= 0:\n        return x\n    else:\n        return -x\n```\n\n请注意，函数体内部的语句在执行时，一旦执行到`return`时，函数就执行完毕，并将结果返回。因此，函数内部通过条件判断和循环可以实现非常复杂的逻辑。\n\n如果没有`return`语句，函数执行完毕后也会返回结果，只是结果为`None`。`return None`可以简写成`return`。\n\n在Python交互环境中定义函数时，注意Python会出现`...`的提示。函数定义结束后需要按两次回车重新回到`>>>`提示符下。\n\n如果你已经把`my_abs()`的函数定义保存为`abstest.py`文件了，那么，可以在该文件的当前目录下启动Python解释器，用`from abstest import my_abs`来导入`my_abs()`函数，注意`abstest`是文件名（不含`.py`扩展名）：\n\n```python\n>>> from abstest import my_abs\n>>> my_abs(-9)\n9\n>>>_\n```\n\n#### 空函数\n\n如果想定义一个什么事也不做的空函数，可以用`pass`语句：\n\n```python\ndef nop():\n    pass\n```\n\n一个空函数，缺少了`pass`，代码运行就会有语法错误。\n\n#### 参数检查\n\n调用函数时，如果参数个数不对，Python解释器会自动检查出来，并抛出`TypeError`。\n\n但是如果参数类型不对，Python解释器就无法帮我们检查。试试`my_abs`和内置函数``abs`的差别：\n\n```python\n>>> my_abs('A')\nTraceback (most recent call last):\n    File \"<stdin>\",line 1, in <module>\n    File \"<stdin>\",line 2, in my_abs\nTypeError:unorderable types: str() >= int()\n>>>abs('A')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: bad operand type for abs(): 'str'\n```\n\n当传入了不恰当的参数时，内置函数`abs`会检查出错误，而我们定义的`my_abs`没有参数检查，会导致`if`语句出错，出错信息和`abs`不一样。所以这个函数定义不够完善。\n\n让我们来修改一下`my_abs`的定义，对参数类型作检查，只允许整数和浮点数类型的参数。数据类型检查可以用内置函数`isinstance()`实现：\n\n```python\ndef my_abs(x):\n    if not isinstance(x,(int,float)):\n        raise TypeError('bad operand type')\n    if x >= 0:\n        return x\n    else:\n        return -x\n```\n\n\n\n\n\n## 函数式编程\n\n### 高阶函数\n\n#### map/reduce\n\n什么是高阶函数？我们以是实际代码为例子，一步步升入概念。\n\n**变量可以指向函数**\n\n以python内置的求绝对值函数`abs()`为例，调用该函数用以下代码：\n\n```python\n>>>abs(-10)\n10\n```\n\n但是如果只写`abs`呢？\n\n```python\n>>>abs\n<built-in function abs>\n```\n\n可见`abs(function)`是函数调用，而abs是函数本身。\n\n要获得函数调用的结果我们可以把函数赋值给变量：\n\n```python\n>>>x = abs(-10)\n>>>x\n10\n```\n\n结论：函数本身因为可以赋值给变量，即：变量可以指向函数。\n\n如果一个变量指向了一个函数，那么可否通过改变量来调用这个函数？用代码验证以下：\n\n```python\n>>>f = abs\n>>>f(-10)\n10\n```\n\n说明变量`f`现在已经指向了`abs`函数本身。直接调用`abs()`函数和调用`f`完全相同。\n\n**函数名也是变量**\n\n那么函数名是什么呢？函数名其实就是指向函数的变量！对于`abs`这个函数完全可以把`abs`看成变量，它指向一个可以计算绝对值的函数！如果把`abs`指向其他对象会有什么情况发生？\n\n```python\n>>> abs = 10\n>>> abs(-10)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'int' object is not callable\n```\n\n把`abs`指向`10`后，就无法通过`abs(-10)`调用该函数了！因为`abs`这个变量已经不指向求绝对值函数而是指向一个整数`10`！\n\n当然实际代码绝对不能这么写，这里是为了说明函数名也是变量。要恢复`abs`函数，请重启Python交互环境。\n\n注：由于`abs`函数实际上是定义在`import builtins`模块中的，所以要让修改`abs`变量的指向在其它模块也生效，要用`import builtins; builtins.abs = 10`。\n\n\n\n### 偏函数\n\npython的`functools`提供了偏函数功能（Partial function）。\n\n假设要转换大量的二进制字符串，每次都传入`int(x,base = 2)`非常麻烦，于是，我们想到可以定义一个函数`int2()`，默认把`base = 2`传进去：\n\n```python\ndef int2(x,base = 2):\n\treturn int(x,base)\n\n#这样我们进行二进制转换就可以了\nint2(1000000)\n\n#Partial function\nint2 = functools.partial(int,base = 2)\nint2('123456')\n64\n```\n\nPartial function作用就是，帮助我们把一个函数的某些参数固定住（也就是设置默认值），返回一个新的函数。\n\n创建偏函数时可以接受函数对象、*args、**kw这3个参数。\n\n```python\nint2 = functools.partial(int,base = 2)\n```\n\n实际上固定了`int()`函数的关键字`base`，也就是：\n\n```python\nint2('10010')\n```\n\n相当于：\n\n```python\nkw = {'base' : 2}\nint('10010' , **kw)\n```\n\n当传入：\n\n```python\nmax2 = functools.partial(max,10)\n```\n\n实际上会把10作为*args的一部分自动加到左边：\n\n```python\nmax2(5, 6, 7)\n```\n\n相当于：\n\n```python\nargs = (10, 5, 6, 7)\nmax(*args)\n```\n\n结果为10。\n\n------------------\n\n\n\n\n\n## 模块\n\n为了编写可维护代码，我们把很多函数分组，分别放到不同的文件里，这样，每个文件包含的代码就相对较少。在python中一个.py文件就可以被称为一个模块（Module）。\n\n使用模块有什么好处？\n\n最大的好处是提高了代码的可维护性。其次，编写代码不必从0开始。\n\n其次，使用模块还可以避免函数名和变量名冲突。相同名字的函数名和变量名可以存在不同模块中。\n\n如果模块名冲突怎么办？Python又引入了按目录来组织模块的方法，称为包（Package）。\n\n每个包目录下都有一个`__init__.py`文件，这个文件是必须存在的，否则，python就会把这个文件当成普通目录而不是包。`__init__.py`可以是空文件，也可以有Python代码，因为`__init__.py`本身就是一个模块，而它的模块名就是`mycompany`。\n\n\n\n### 使用模块\n\n```python\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n' a test module '\n\n__author__ = 'Michael Liao'\n\nimport sys\n\ndef test():\n    args = sys.argv\n    if len(args)==1:\n        print('Hello, world!')\n    elif len(args)==2:\n        print('Hello, %s!' % args[1])\n    else:\n        print('Too many arguments!')\n\nif __name__=='__main__':\n    test()\n'''\n当我们在命令行运行hello模块文件时，Python解释器把一个特殊变量__name__置为__main__，而如果在其他地方导入该hello模块时，if判断将失败，因此，这种if测试可以让一个模块通过命令行运行时执行一些额外的代码，最常见的就是运行测试。\n'''\n\n```\n\n代码的第一，二行是标准注释，第一行注释可以让这个`hellow.py`文件直接在Unix/Linux/Mac上运行，第二行注释表示了.py文件本身使用了UTF-8编码；\n\n第四行是一个字符串，表示模块的文档注释，任何模块的第一行字符串都被视为模块文档注释；\n\n第六行`__author__`变量把作者写进去。\n\n后面开始就是真正的代码部分。\n\n使用`sys`模块的第一步就是导入该模块：\n\n```python\nimport sys\n```\n\n导入了sys模块后，我们就有了变量sys指向该模块，利用`sys`这个变量，就可以访问`sys`模块所有的功能。\n\nsys有一个argv变量，用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是`.py`文件的名称，例如：\n\n运行``python3 hello.py`获得的`sys.argv`就是`['hellow.py']`\n\n运行`python3 hello.py Michael`获得的`sys.argv`就是`['hello.py', 'Michael']`。\n\n\n\n如果启动Python交互环境，再导入`hello`模块：\n\n```python\n$ python3\n>>>import hello\n>>>\n```\n\n导入时，没有打印`hello world!`因为没有执行`test()`函数。\n\n调用`hello.test()`时，才能打印出`hello world!`:\n\n```python\n>>>hello.test()\nHello,world!\n```\n\n#### 作用域\n\n在一个模块中，我们可能会定义很多函数和变量，但有的函数我们希望给别人使用，有的仅仅在模块内部使用。在Python中我们是通过`_`前缀来实现的。\n\n正常函数和变量名是公开的（public），可以被直接引用。\n\n类似`__xx__`是特殊变量，可以被直接引用，但有特殊用途，比如上面的`__author__`，`__name__`就是特殊变量，`hello`模块定义的文档注释也可以用特殊变量`__doc__`访问，我们自己的变量一般不要用这种变量名。\n\n类似`_xxx`和`__xxx`这样的函数或变量就是非公开的（private），不应该被直接引用，比如`_abc`，`__abc`等；\n\n之所以我们说，private函数和变量“不应该”被直接引用，而不是“不能”被直接引用，是因为Python并没有一种方法可以完全限制访问private函数或变量，但是，从编程习惯上不应该引用private函数或变量。\n\nprivate函数或变量不应该被别人引用，那它们有什么用呢？请看例子：\n\n```python\ndef _private_1(name):\n    return 'Hello, %s' % name\n\ndef _private_2(name):\n    return 'Hi, %s' % name\n\ndef greeting(name):\n    if len(name) > 3:\n        return _private_1(name)\n    else:\n        return _private_2(name)\n```\n\n我们在模块里公开`greeting()`函数，而把内部逻辑用private函数隐藏起来了，这样，调用`greeting()`函数不用关心内部的private函数细节，这也是一种非常有用的代码封装和抽象的方法，即：\n\n外部不需要引用的函数全部定义成private，只有外部需要引用的函数才定义为public。\n\n\n\n\n\n## 面向对象编程（OOP）\n\nOOP把对象作为基本单元，一个对象包含了数据和操作数据的函数。\n\nOOP把计算机程序视为一系列命令的集合，而每个对象都可以接受其他对象发过来的消息，并处理这些消息，计算机程序的执行就是一系列消息在对象之间传递。\n\n在Python中，所有的数据类型都可以视为对象，当然也可以自定义对象。自定义的对象数据类型就是面向对象中的类（Class）的概念。\n\n\n\n### 类和实例\n\nOOP最重要概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，实例是根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但是各自的数据可能不同。\n\n仍以Student类为例，在Python中，定义类是通过`class`关键字：\n\n```python\nclass Student(object):\n    pass\n```\n\n`calss`后面紧接着是类名，即`Student`类创建出`Student`实例，创建实例是通过类名+()实现的：\n\n```python\n>>> bart = Student()\n>>> bart\n<__main__.Student object at 0x10a67a590>\n>>> Student\n<class '__main__.Student'>\n```\n\n可以看到，变量`bart`指向的就是一个`Student`的实例，后面的`0x10a67a590`是内存地址，每个object的地址都不一样，而`Student`本身则是一个类。\n\n可以自由地给一个实例变量绑定属性，比如，给实例`bart`绑定一个`name`属性：\n\n```python\n>>> bart.name = 'Bart Simpson'\n>>> bart.name\n'Bart Simpson'\n```\n\n由于类可以起到模板作用，因此可以在创建实例的时候，把一些我们任务必须绑定的属性强制填进去。通过定义一个特殊的`__init__`方法，创建实例的时候就把`name`，`score`等属性绑定上去：\n\n```python\nclass Student(object):\n    \n    def __init__(self,name,score):\n        self.name = name\n        self.score = score\n```\n\n注意，`__init__`方法的第一个参数永远是`self`，表示创建的实例本身，因此，在`__init__`方法内部，就可以把各种属性绑定到`self`,因为`self`就指向创建的实例本身。\n\n有了`__init__`方法，在创建实例的时候，就不能传入空的参数了，必须传入与`__init__`方法相匹配的参数，但是self不需要传，Python解释器会自己把实例变量传进去。\n\n```python\n>>>bart = Student('Bart Simpson',59)\n>>>bart.name\n'Bart Simpson'\n>>>bart.score\n59\n```\n\n和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量`self`，并且，调用时不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所有，你仍然可以用默认参数、可变参数、关键字参数、命名关键字参数。\n\n#### 数据封装\n\nOOP的一个重要特点就是数据封装。在上面的`Student`类中，每个实例就拥有各自的`name`和`score`这些数据。我们可以通过函数来访问这些数据，比如打印一个学生的成绩。但是`Student`实例本身就拥有这些数据，要访问这些数据，就没有必要从外面的函数去访问，可以直接在`Student`类的内部定义访问数据的函数，这样，就把“数据”给封装起来了。这些封装数据的函数是和`Student`类本身是关联起来的，我们称之为类的方法：\n\n```python\nclass Student(object):\n    \n    def __init__(self,name,score):\n        self.name = name\n        self.score = score\n    \n    def print_score(self):\n        print('%s : %s' % (self.name, self.score))\n```\n\n要定义一个方法，除了第一个参数是`self`以外，其他和普通函数一样。要调用另一个方法，只需要在实例变量上直接调用，除了`self`不用传递，其他参数正常传入：\n\n```python\n>>>bart.print_score()\nBart Simpson: 59\n```\n\n\n\n### 访问限制\n\n在class内部，可以有属性和方法，而外部代码可以通过直接调用实例变量的方法来操作数据，这样，就隐藏了内部的复杂逻辑。\n\n但是从`Student`类的定义来看，外部代码还是可以自由的修改一个实例的`name`、`scorre`属性。\n\n如果要让内部属性不被外部访问，可以把属性名称前加两个下划线`__`，在python中，实例的变量名如果以`__`开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，所以，我们把`Student`类改一改：\n\n```python\nclass Student(object):\n    def __init__(self,name,score):\n        self.__name = name\n        self.__score = score\n        \n    def print_score(self):\n        prnint('%s %s' % (self.__name, self.__score))\n```\n\n改完后，对外部代码来说没什么变动，但以及无法从外部访问`实例变量.__name`和`实例变量.__score`了\n\n但是如果外部代码要获取score怎么办？可以给`Student`类增加`set_score`方法：\n\n```python\nclass Student(object):\n    ...\n    \n    def set_score(self,score):\n        self.__score = score\n```\n\n你也许会问，原先那种直接通过`bart.score = 99`也可以修改啊，为什么要定义一个方法大费周折？因为在方法中，可以对参数做检查，避免传入无效的参数：\n\n```python\nclass Student(object):\n    ...\n\n    def set_score(self, score):\n        if 0 <= score <= 100:\n            self.__score = score\n        else:\n            raise ValueError('bad score')\n```\n\n\n\n而不能直接访问`__name`是因为Python解释器对外把`__name`变量改成了`_Student__name`，所以，仍然可以通过`_Student__name`来访问`__name`变量：\n\n```python\n>>> bart._Student__name\n'Bart Simpson'\n```\n\n但是强烈建议你不要这么干，因为不同版本的Python解释器可能会把`__name`改成不同的变量名。\n\n\n\n```python\n>>> bart = Student('Bart Simpson', 59)\n>>> bart.get_name()\n'Bart Simpson'\n>>> bart.__name = 'New Name' # 设置__name变量！\n>>> bart.__name\n'New Name'\n```\n\n表面上看，外部代码“成功”地设置了`__name`变量，但实际上这个`__name`变量和class内部的`__name`变量*不是*一个变量！内部的`__name`变量已经被Python解释器自动改成了`_Student__name`，而外部代码给`bart`新增了一个`__name`变量。不信试试：\n\n```python\n>>> bart.get_name() # get_name()内部返回self.__name\n'Bart Simpson'\n```\n\n\n\n\n\n### 继承和多态\n\n在OOP程序设计中，当我们定义了一个class的时候，可以从某个现有的class继承，新的class称为子类（Subclass），而被继承的class称为基类，父类或超类（Base class，super class）。\n\n比如我们编写了一个名为`Animal`的class，有一个`run`方法可以直接打印：\n\n```python\nclass Animal(object):\n    def run(self):\n        print('Animal is running……')\n```\n\n当我们要编写dog和cat类时，就可以直接从`Animal`类继承：\n\n```python\nclass Dog(Animal):\n    pass\n\nclass Cat(Animal):\n    pass\n```\n\n对于`Dog`类来说，`Animal`就是它的父类，对于`Animal`类来说，`Dog`就是它的子类。`Cat`和`Dog`类似。\n\n继承有什么好处?最大的好处是，子类获得了父类的全部功能。由于`Animal`实现了`run()`方法，因此，`Dog`和`Cat`作为它的子类，什么事没干就自动拥有了`run()`方法：\n\n```python\ndog = Dog()\ndog.run\n\n#cat与dog的处理方式一致\ncat = Cat()\ncat.run\n```\n\n运行结果如下\n\n```python\nAnimal is running...\nAnimal is running...\n```\n\n\n\n当然，也可以对子类增加一些方法，比如Dog类：\n\n```python\nclass Dog(Animal):\n\n    def run(self):\n        print('Dog is running...')\n\n    def eat(self):\n        print('Eating meat...')\n```\n\n\n\n继承的第二个好处需要我们对代码做一点改进。你看到了，无论是`Dog`还是`Cat`，它们`run()`的时候，显示的都是`Animal is running...`，符合逻辑的做法是分别显示`Dog is running...`和`Cat is running...`，因此，对`Dog`和`Cat`类改进如下：\n\n```python\nclass Dog(Animal):\n\n    def run(self):\n        print('Dog is running...')\n\nclass Cat(Animal):\n\n    def run(self):\n        print('Cat is running...')\n```\n\n\n\n再次运行结果如下：\n\n```python\nDog is running...\nCat is running...\n```\n\n当子类和父类都存在相同的`run()`方法时，我们说，子类的`run`覆盖了父类的`run`，在代码运行的时候，总是会调用子类的`run`。这样我们就获得了继承的另一个好处：多态。\n\n要理解什么是多态，我们首先要对数据类型再作一点说明。当我们定义一个class的时候，我们实际上就定义了一种数据类型。\n\n判断一个变量是否是某个类型可以用`isinstance()`判断：\n\n```python\n>>> isinstance(a, list)\nTrue\n>>> isinstance(b, Animal)\nTrue\n>>> isinstance(c, Dog)\nTrue\n```\n\n看来`a`、`b`、`c`确实对应着`list`、`Animal`、`Dog`这3种类型。\n\n但是等等，试试：\n\n```Python\n>>> isinstance(c, Animal)\nTrue\n```\n\n看来`c`不仅仅是`Dog`，`c`还是`Animal`！\n\n在继承关系中，如果一个数据类型是某个数据类型的子类，那它的数据类型也可以被看成是父类。但是反过来就不行：\n\n```python\n>>>b = Animal()\n>>>isinstance(b,Dog)\nFalse\n```\n\n\n\n理解多态的好处，我们还需要编写一个函数，接受一个`Anmial`类型的变量：\n\n```python\ndef run_twice(animal):\n    animal.run()\n    animal.run()\n```\n\n当我们传入`Animal`的实例时，`run_twice()`就打印出：\n\n```python\n>>> run_twice(Animal())\nAnimal is running...\nAnimal is running...\n```\n\n传入`Dog`的实例时，`run_twice()`打印：\n\n```python\n>>>run_twice(Dog())\nDog is runninng……\nDog is runninng……\n```\n\n现在我们再定义一个`Tortoise`类型，也从`Animal`派生：\n\n```python\nclass Tortoise(Animal):\n    def run(self)\n    \tprint('Tortoise is running slowwly…')\n```\n\n当我们调用`run_twice()`时，传入`Tortoise`的实例：\n\n```python\n>>> run_twice(Tortoise())\nTortoise is running slowly...\nTortoise is running slowly...\n```\n\n你会发现，新增一个`Animal`的子类，不必对`run_twice`做任何修改，实际上，任何依赖`Animal`作为参数的函数或者方法都可以不加修改的正常运行，原因就在于多态。\n\n多态的好处是，当我们传入`Dog`，`Cat`，`Tortoise`……时，我们只需要接受`Anmial`类型就可以了，因为`Dog`，`Cat`，`Tortoise`……都是`Animal`类型，然后，按照`Animal`类型进行操作即可。由于`Anmial`类型有`run()`方法，因此，传入的任意类型，只要是`Aniaml`类或者是子类，就会自动调用实际类型的`run()`方法，这就是多态的意思。\n\n对于一个变量，我们只需要知道它是`Animal`类型，无需确切地知道它的子类型，就可以放心地调用`run()`方法，而具体调用的`run()`方法是作用在`Animal`、`Dog`、`Cat`还是`Tortoise`对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：调用方只管调用，不管细节，而当我们新增一种`Animal`的子类时，只要确保`run()`方法编写正确，不用管原来的代码是如何调用的。这就是著名的“开闭”原则：\n\n对扩展开放：允许新增`Animal`子类；\n\n对修改封闭：不需要修改依赖`Animal`类型的`run_twice()`等函数。\n\n\n\n### 获取对象信息\n\n当我们拿到一个对象引用时，如何知道这个对象是什么类型，有那些方法呢？\n\n#### 使用`type()`\n\n我们判断对象类型，使用`type`函数，基本类型都可以用`type`判断：\n\n```python\n>>> type(123)\n<class 'int'>\n```\n\n如果一个对象指向函数或者类，也可以用`type()`判断。但是`type()`函数返回的是什么类型呢？它返回对应的Class类型。\n\n\n\n如果要判断一个对象是否是函数怎么办？可以使用`types`模块中定义的常量：\n\n```python\n>>> import types\n>>> def fn():\n...     pass\n...\n>>> type(fn)==types.FunctionType\nTrue\n>>> type(abs)==types.BuiltinFunctionType\nTrue\n>>> type(lambda x: x)==types.LambdaType\nTrue\n>>> type((x for x in range(10)))==types.GeneratorType\nTrue\n```\n\n#### 使用dir()\n\n如果要获得一个对象的所有属性和方法，可以使用`dir()`函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法：\n\n```python\n>>> dir('ABC')\n['__add__', '__class__',..., '__subclasshook__', 'capitalize', 'casefold',..., 'zfill']\n```\n\n紧接着，可以测试对象的属性：\n\n```python\n>>>hasattr(obj,'x') #有x属性吗？\n>>>setattr(obj,'y') #设置x属性\n>>> getattr(obj, 'x') # 获取属性'x'\n```\n\n可以传入一个default参数，如果属性不存在，就返回默认值：\n\n```python\n>>> getattr(obj, 'z', 404) # 获取属性'z'，如果不存在，返回默认值404\n404\n```\n\n也可以获得对象的方法：\n\n```python\n>>> hasattr(obj, 'power') # 有属性'power'吗？\nTrue\n>>> getattr(obj, 'power') # 获取属性'power'\n<bound method MyObject.power of <__main__.MyObject object at 0x10077a6a0>>\n```\n\n\n\n## 面向对象高级编程\n\n### 使用\\_\\_slots\\_\\_\n\n正常情况下，当我们定义了一个class，创建了一个clsaa的实例后，我们可以给该实例绑定任何实例和方法，这就是动态语言的灵活性。先定义class：\n\n```python\nclass Student(object):\n    pass\n```\n\n然后，尝试给实例绑定一个属性：\n\n```python\n>>>s = Student\n>>>s.name = 'Michael' #动态给实例绑定一个对象\n>>>print('s.name')\nMicheal\n```\n\n还可以尝试给实例绑定一个方法：\n\n```python\n>>>def set_age(self,age): #定义一个函数作为实例方法\n    \tself.age=age\n    \n>>>from type import MethodType\n>>>s.set_age = MethodeType(set_age, s) #给实例绑定一个方法\n>>>s.set_age(25) #调用实例方法\n>>>s.age #测试结果\n25\n```\n\n但是==给一个实例绑定的方法对另一个实例是不起作用的。==\n\n\n\n\n\n## 进程和线程\n\n### 多进程\n\nUnix/Linux操作系统提供了一个`fork`调用，普通的函数调用，调用一次返回一次，但是`fork`调用一次，返回两次，因为操作系统自动把当前进程（父进程）复制了一份（称为子进程），然后分别在父进程和子进程内返回。\n\n子进程永远返回`0`，而父进程返回子进程的ID，子进程只需要调用`getppid()`就可以拿到父进程的ID。\n\nPython的`os`模块封装了常见的系统调用，其中就包括`fork`，可以在Python程序中轻松创建子进程：\n\n```python\nimport os\n\nprint('Process (%s) start...' % os.getpid())\n# Only works on Unix/Linux/Mac:\npid = os.fork()\nif pid == 0:\n    print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))\nelse:\n    print('I (%s) just created a child process (%s).' % (os.getpid(), pid))\n```\n\n如果要启动大量子进程，可以用进程池的方法批量创建子进程：\n\n```python\nfrom multiprocessing import Pool\nimport os, timme, random\n\ndef long_time_task(name):\n    print('Run task %s (%s)...' % (name, os.getpid()))\n    start = time.time()\n    time.sleep(random.random() * 3)\n    end = time.time()\n    print('Task %s runs %0.2f seconds.' % (name, (end - start)))\n    \nif __name__=='__main__':\n    print('Parent process %s.' % os.getpid())\n    p = Pool(4)\n    for i in range(5):\n        p.apply_async(long_time_task, args=(i,))\n    print('Waiting for all subprocesses done...')\n    p.close()\n    p.join()\n    print('All subprocesses done.')\n```\n\n代码解读：\n\n对`Pool`对象调用`join()`方法会等待所有子进程执行完毕，调用`join()`之前必须先调用`close()`，调用`close()`之后就不能继续添加新的`Process`了。\n\n\n\n例子：\n\n```python\npool = Pool(8) #可以同时跑8个进程\n\tpool.map(get_all, [i for i in range(10)])\n    pool.close\n    pool.join()\n```\n\n这里的`pool.close()`是说关闭pool，使其不在接受新的（主进程）任务。\n\n这里的`pool.join()`是说：主进程阻塞后，让子进程继续运行完成，子进程运行完成后，再把主进程全部关掉。\n\n\n\n#### 子进程\n\n很多时候，子进程并不是自生，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。\n\n==subprocess==模块可以让我们非常方便的启动一个子进程，然后控制其输入和输出。\n\n下面的例子演示了如何在Python代码中运行命令`nslookup www.python.org`，这和命令行的效果是一样的：\n\n```python\nimport subprocess\n\nprint('$ nslookup www.python.org')\nr = subprocess.call(['nslookup','www.python.org'])\nprint('Exit code:', r)\n```\n\n```python\n$ nslookup www.python.org\nServer:\t\t192.168.19.4\nAddress:\t192.168.19.4#53\n\nNon-authoritative answer:\nwww.python.org\tcanonical name = python.map.fastly.net.\nName:\tpython.map.fastly.net\nAddress: 199.27.79.223\n\nExit code: 0\n```\n\n\n\n#### subprocess模块详解\n\n`subprocess`模块是**Python 2.4**中新增的一个模块，它允许你生成新的进程，连接到它们的**input/output/error**管道，并获取它们的返回（状态）码。这个模块的目的在于替换几个旧的模块和方法，如：\n\n```python\nos.system\nos.spawn*\n```\n\n\n\n**1.subprocess模块中的常用函数**\n\n| 函数                            | 描述                                                         |\n| ------------------------------- | ------------------------------------------------------------ |\n| subprocess.run()                | py3.5中新增的函数。执行制定的命令，等待命令执行完毕后返回一个包含执行结果的CompletedProcess类的实例。 |\n| subprocess.call()               | 执行指定命令，返回命令执行状态，其功能类似于`os.system(cmd)` |\n| subprocess.check_call()         | python2.5中新增的函数，执行制定的命令，如果执行成功则返回状态码，否则抛出异常。其功能等价于subprocess.run(..., check = True) |\n| subprocess.check_output()       | python 2.7中新增的函数。执行制定的命令，如果执行状态码为0则返回命令执行结果。都则抛出异常。 |\n| subprocess.getoutput(cmd)       | 接受字符串格式的命令，执行命令并返回执行结果，其功能类似于os.popen(cmd).reead()和commands.getoutput(cmd) |\n| subprocess.getstatusoutput(cmd) | 执行cmd命令，返回一个元组（命令执行状态，命令执行结果输出），其功能类似于commands,getstatusoutput() |\n\n\n\n> 说明：\n>\n> 在Python 3.5之后的版本中，官方文档中提倡通过subprocess.run()函数替代其他函数来使用subproccess模块的功能；\n> 在Python 3.5之前的版本中，我们可以通过subprocess.call()，subprocess.getoutput()等上面列出的其他函数来使用subprocess模块的功能；\n> subprocess.run()、subprocess.call()、subprocess.check_call()和subprocess.check_output()都是通过对subprocess.Popen的封装来实现的高级函数，因此如果我们需要更复杂功能时，可以通过subprocess.Popen来完成。\n> subprocess.getoutput()和subprocess.getstatusoutput()函数是来自Python 2.x的commands模块的两个遗留函数。它们隐式的调用系统shell，并且不保证其他函数所具有的安全性和异常处理的一致性。另外，它们从Python 3.3.4开始才支持Windows平台。\n\n\n\n**上面各函数的定义以及参数说明**\n\n函数参数列表\n\n```python\nsubprocess.run(args, *, stdin=None, input=None, stdout=None, stderr=None, shell=False, timeout=None, check=False, universal_newlines=False)\n\nsubprocess.call(args, *, stdin=None, stdout=None, stderr=None, shell=False, timeout=None)\n\nsubprocess.check_call(args, *, stdin=None, stdout=None, stderr=None, shell=False, timeout=None)\n\nsubprocess.check_output(args, *, stdin=None, stderr=None, shell=False, universal_newlines=False, timeout=None)\n\nsubprocess.getstatusoutput(cmd)\n\nsubprocess.getoutput(cmd)\n```\n\n参数说明：\n\n- args：要执行的shell命令，默认应该是一个字符串序列，如['df', '-Th']，也可以是一个字符串，但要把shell的参数的值设置为True\n\n- shell：如果sehll为True，那么指定的命令将通过shell执行。\n\n- check：如果check参数的值是True，且执行命令的进程以非0状态码退出，则会抛出一个CalledProcessError的异常，且该异常会包含参数、退出状态码、以及stdout和stderr\n\n- `stdout, stderr：input`： 该参数是传递给Popen.communicate()，通常该参数的值必须是一个字节序列，如果universal_newlines=True，则其值应该是一个字符串。\n\n  1. run()函数默认不会捕获命令执行结果的正常输出和错误输出，如果我们向获取这些内容需要传递subprocess.PIPE，然后可以通过返回的CompletedProcess类实例的stdout和stderr属性或捕获相应的内容；\n\n  2. call()和check_call()函数返回的是命令执行的状态码，而不是CompletedProcess类实例，所以对于它们而言，stdout和stderr不适合赋值为subprocess.PIPE；\n     3\n  3. check_output()函数默认就会返回命令执行结果，所以不用设置stdout的值，如果我们希望在结果中捕获错误信息，可以执行stderr=subprocess.STDOUT。\n\n- `universal_newlines`： 该参数影响的是输入与输出的数据格式，比如它的值默认为False，此时stdout和stderr的输出是字节序列；当该参数的值设置为True时，stdout和stderr的输出是字符串。\n\n\n\n**3.subprocess.CompletedProcess类介绍**\n\n需要说明的是，`subprocess.run()`函数是Python3.5中新增一个高级函数，其返回值是一个`subprocess.CompletedProcess`类的实例，因此，subprocess,completedProcess类也是Python 3.5中才存在的。它表示的是一个以结束进程的状态信息。\n\n\n\n#### subprocess.Popen介绍\n\n该类用于在一个新的程序中执行一个子程序。前面我们提到过，上面介绍的这些函数艘是基于`subprocess.Popen`类实现的，通过使用这些被封装的高级函数可以很方便的完成一些常见需求。由于`subprocess`模块底层的进程创建和管理是有Popen类来处理的，因此，当我们无法通过上面哪些高级函数来实现一些不太常见的功能时就可以通过subprocess.Popen类提供灵活的api来完成。\n\n1.subprocess.Popen构造函数\n\n```python\nclass subprocess.Popen(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, \n    preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=False,\n    startup_info=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=())\n```\n\n- args： 要执行的shell命令，可以是字符串，也可以是命令各个参数组成的序列。当该参数的值是一个字符串时，该命令的解释过程是与平台相关的，因此通常建议将args参数作为一个序列传递。\n- bufsize： 指定缓存策略，0表示不缓冲，1表示行缓冲，其他大于1的数字表示缓冲区大小，负数 表示使用系统默认缓冲策略。\n- stdin, stdout, stderr： 分别表示程序标准输入、输出、错误句柄。\n- preexec_fn： 用于指定一个将在子进程运行之前被调用的可执行对象，只在Unix平台下有效。\n- close_fds： 如果该参数的值为True，则除了0,1和2之外的所有文件描述符都将会在子进程执行之前被关闭。\n- shell： 该参数用于标识是否使用shell作为要执行的程序，如果shell值为True，则建议将args参数作为一个字符串传递而不要作为一个序列传递。\n- cwd： 如果该参数值不是None，则该函数将会在执行这个子进程之前改变当前工作目录。\n- env： 用于指定子进程的环境变量，如果env=None，那么子进程的环境变量将从父进程中继承。如果env!=None，它的值必须是一个映射对象。\n- universal_newlines： 如果该参数值为True，则该文件对象的stdin，stdout和stderr将会作为文本流被打开，否则他们将会被作为二进制流被打开。\n- startupinfo和creationflags： 这两个参数只在Windows下有效，它们将被传递给底层的CreateProcess()函数，用于设置子进程的一些属性，如主窗口的外观，进程优先级等。\n\n\n\n2. subprocess.Popen类的实例可调用的方法\n\n| 方法                                        | 描述                                                         |\n| ------------------------------------------- | ------------------------------------------------------------ |\n| Popen.poll()                                | 用于检查子进程（命令）是否已经执行结束，没结束返回None，结束后返回状态码。 |\n| Popen.wait(timeout=None)                    | 等待子进程结束，并返回状态码；如果在timeout指定的秒数之后进程还没有结束，将会抛出一个TimeoutExpired异常。 |\n| Popen.communicate(imput=None, timeout=None) | 该方法可用于来与程序进行交互，比如发送数据到stdin，从stdout和stderr读取数据，直到达到文件末尾。 |\n| Popen.send_signal(signal)                   | 发送指定的信号给这个子进程                                   |\n| Popen.terminate()                           | 停止这个子进程                                               |\n| Popen.kill                                  | 杀死该子进程                                                 |\n\n\n\n\n\n#### 进程间通信\n\n`Process`之间肯定是需要通信的，操作系统提供了很多的机制来实现进程间的通信。Python的`nultiprocessing`模块包装了底层的机制，提供了`Queue`，`Pipes`等多种方式来交换数据。\n\n我们以`Queue`为例，在父进程中创建两个子进程，一个往`Queue`里写入数据，一个从`Queue`里读取数据：\n\n```python\nfrom multiprocessing import Process, Queue\nimport os, time, rendom\n\n#写数据进程执行的代码\ndef write(q):\n    print('Pricess to write: %s' % os.getpid())\n    for value in ['A', 'B', 'C']:\n        print('Put %s to queue...' % value)\n        q.put(value)\n        time.sleep(random.random())\n\n#读数据进程执行的代码\ndef read(q):\n    print('Process to readL %s' % os.getpid)\n\twrite True:\n        value = q.get(True)\n        print('Get %s from queue.' % value)\n\nif __name__ = '__main__':\n    #父进程创建的Queue，并传给各个子进程\n    q = Queue()\n    pw = Process(targe=write, args=(q,))\n    pr = Process(target=read, args=(q,))\n    #启动子进程pr,读取：\n    pr.start()\n    #等待pw结束\n    pw.join()\n    #pr进程里是四循环，无法等待其结束，只能强行终止：\n    pr.terminate()\n```\n\n---------\n\n\n\n\n\n### 多线程\n\n进程是由若干个线程组成的，一个进程至少有一个线程。\n\n由于线程的操作系统直接支持的执行单元，因此，高级语言通常都内置多线程的支持，Python也不例外，并且，Python的线程是真正的Posix Thread，而不是模拟出来的线程。\n\nPython的标准库提供了两个模块：`_thread`和`threading`，`_thread`是低级模块，`threading`是高级模块，对`_thread`进行了封装。绝大多数情况下，我们只需要使用`_threading`这个高级模块。\n\n启动一个线程就是把一个函数传入并创建`Thread`实例，然后调用`start()`开始执行：\n\n```python\nimport time, threading\n\n#新线程执行的代码：\ndef loop:\n    print('thread %s is runing...' % threading.current_thread().name)\n    n = 0\n    while n < 5:\n        n = n+ 1\n        print('thread %s >>> %s' % (threading.current_thread ().name))\n        time.sleep(1)\n    print('thread %s ended.' % threading.current_thread().name)\n    \nprint('thread %s is running...' % threading.current_thread().name)\nt = threading.Thread(target=loop, name = 'LoopThread')\nt.start()\nt.join()\nprint('thread %s ended.' % threading.current_thread().name)\n```\n\n由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的`threading`模块有个`current_thread()`函数，它永远返回当前线程的实例。主线程实例的名字叫`MainThread`，子线程的名字在创建时指定，我们用`LoopThread`命名子线程。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为`Thread-1`，`Thread-2`……\n\n\n\n#### Lock\n\n多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，所有的变量都由所有的线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时修改一个变量，把内容给该乱了。\n\n所以我们希望创建一把锁，当某个程序开始执行一个线程时，我们说，该线程获得的锁，因此其他线程不能同时执行该线程，只能等待，，直到锁被释放后，获得该锁以后才能改。由于锁只有一个，无论多少个线程，同一时刻最多只有一个线程持有该锁，所以不会造成修改冲突。创建一个锁就是通过`threading.Lock()`来实现：\n\n```python\nbalance = 0\nlock = threading.Lock()\n\ndef run_thread(n):\n    for i in range(10000)\n    #先要获取锁：\n    lock.acquire()\n    try:\n        change_it(n)\n    finally:\n        #改完了一定要释放锁\n        lock.release()\n```\n\n当多个线程同时执行`lock.acquire()`时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。\n\n获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用`try...finally`来确保锁一定会被释放。\n\n锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。\n\n\n\n#### 多核CPU\n\n如果你不幸拥有一个多核CPU，你肯定在想，多核应该可以同时执行多个线程。\n\n如果写一个死循环的话，会出现什么情况呢？\n\n打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。\n\n我们可以监控到一个死循环线程会100%占用一个CPU。\n\n如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。\n\n要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。\n\n试试用Python写个死循环：\n\n```python\nimport threading, multiprocessing\n\ndef loop():\n    x = 0\n    while True:\n        x = x ^ 1\n\nfor i in range(multiprocessing.cpu_count()):\n    t = threading.Thread(target=loop)\n    t.start()\n```\n\n启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。\n\n但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？\n\n因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。\n\nGIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。\n\n所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。\n\n不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。\n","slug":"python","published":1,"updated":"2023-08-07T08:06:42.391Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clmnlyz6q0008myqbds0b87kl","content":"<h1 id=\"Python\"><a href=\"#Python\" class=\"headerlink\" title=\"Python\"></a>Python</h1><p>Python的学习记录。</p>\n<span id=\"more\"></span>\n<h2 id=\"Python基础\"><a href=\"#Python基础\" class=\"headerlink\" title=\"Python基础\"></a>Python基础</h2><h3 id=\"字符编码\"><a href=\"#字符编码\" class=\"headerlink\" title=\"字符编码\"></a>字符编码</h3><p>计算机只能处理数字，如果要处理文本，就必须把文本转化成数字才能处理。最早的计算机在设计时采用8个比特(bit)作为一个字节(byte)，所以，一个字节能表示的最大的整数就是255（二进制11111111=十进制255）。如果要表示更大的整数就要采用更多的字节。</p>\n<p>为了不与ASCII编码冲突，中国制定了<code>GB2312</code>编码，用来把中文编码进去。日本把日文编到<code>shift_JIS</code>中等等。各国有各国的标准，就会发生冲突，结果是，在很多语言混合的文本中，显示出来会有乱码。</p>\n<p>因此，Unicode字符集应运而生。Unicode把所有语言都统一到一套编码里，这样就不会出现乱码问题了。</p>\n<p>ASCII编码是一个字节，而Unicode编码是两个字节。</p>\n<p>字母<code>A</code>用ASCII编码是十进制的<code>65</code>，二进制的<code>01000001</code>；</p>\n<p>字符<code>0</code>用ASCII编码是十进制的<code>48</code>，二进制的<code>00110000</code>，注意字符<code>&#39;0&#39;</code>和整数<code>0</code>是不同的；</p>\n<p>汉字<code>中</code>已经超出了ASCII编码的范围，用Unicode编码是十进制的<code>20013</code>，二进制的<code>01001110 00101101</code>。</p>\n<p>对于单个字符的编码，Python提供了<code>ord()</code>函数获取字符的整数表示，<code>chr()</code>函数把编码转化为对应字符。</p>\n<p>如果知道字符的整数编码，还可以用十六进制这么写str：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"string\">&#x27;\\u4e2d\\u6587&#x27;</span></span><br><span class=\"line\"><span class=\"string\">&#x27;中文&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>这两种写法完全等价。</p>\n<p>由于Python的字符串是<code>str</code>，所以在内存中以Unicode表示，一个字符对应若干个字节。如果要在网络上传输，或者要保存到磁盘上，就需要把<code>str</code>变为以字节为单位的<code>bytes</code>。</p>\n<p>Python对<code>bytes</code>类型的数据用带<code>b</code>前缀的单引号或双引号表示：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = <span class=\"string\">b&#x27;ABC&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>纯英文的<code>str</code>可以用<code>ASCII</code>编码为<code>bytes</code>，内容是一样的，含有中文的<code>str</code>可以用<code>UTF-8</code>编码为<code>bytes</code>。含有中文的<code>str</code>无法用<code>ASCII</code>编码，Python会报错。</p>\n<p>在<code>bytes</code>中，无法显示为ASCII字符的字节，用<code>\\x##</code>显示。</p>\n<p>反过来，如果我们从网络或磁盘上读取了字节流，那么读到的数据就是<code>bytes</code>要把<code>bytes</code>变为<code>str</code>，就需要用<code>decode()</code>方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"string\">b&#x27;ABC&#x27;</span>.decode(<span class=\"string\">&#x27;ascii&#x27;</span>)</span><br><span class=\"line\"><span class=\"string\">&#x27;ABC&#x27;</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"string\">b&#x27;\\xe4\\xb8\\xad\\xe6\\x96\\x87&#x27;</span>,decode(<span class=\"string\">&#x27;utf-8&#x27;</span>)</span><br><span class=\"line\"><span class=\"string\">&#x27;中文&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>如果<code>bytes</code>中包含无法解码的字节，<code>decode()</code>方法会报错。</p>\n<p>如果<code>bytes</code>中只有一小部分无效的字节，可以传入<code>errors=&#39;ignore&#39;</code>忽略错误的字节：</p>\n<p>由于Python源码是一个文本文件，所以，当你的源代码中包含了中文的时候，在保存源代码时，就需要务必指定保存UTF-8编码。当Python解释器读取源码时，为了让它按<code>UTF-8</code>编码读取，我们通常在文件开头写上这两行：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#!/usr/bin/env python3</span></span><br><span class=\"line\"><span class=\"comment\"># -*- coding: utf-8 -*-</span></span><br></pre></td></tr></table></figure>\n<p>第一行注释是为了告诉Linux/OS X 系统，这是一个Python可执行程序，Windows系统会忽略这个注释；</p>\n<p>第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，你在源代码中写入的中文输入可能会出现乱码。</p>\n<p>在Python中，采用的格式化字符串的方式是与C语言一致的，用<code>%</code>实现。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">占位符</th>\n<th style=\"text-align:left\">替换内容</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">%d</td>\n<td style=\"text-align:left\">整数</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">%f</td>\n<td style=\"text-align:left\">浮点数</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">%s</td>\n<td style=\"text-align:left\">字符串</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">%x</td>\n<td style=\"text-align:left\">十六进制整数</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>其中，格式化整数和浮点数还可以制定是否补0和整数与小数的为数：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;#2d-%02d&#x27;</span> % (<span class=\"number\">3</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;%.2f&#x27;</span> % <span class=\"number\">3.1415926</span>)</span><br></pre></td></tr></table></figure>\n<p>如果你不太确定应该用什么，<code>%s</code>永远起作用，它会把任何数据类型转换为字符串：</p>\n<p>有些时候，字符串里面的<code>%</code>是一个普通字符怎么办？这个时候就需要转义，用<code>%%</code>来表示一个<code>%</code>：</p>\n<h4 id=\"format\"><a href=\"#format\" class=\"headerlink\" title=\"format()\"></a>format()</h4><p>另一种格式化字符串的方法是使用字符串的<code>format()</code>方法，它会用传入的参数依次替换占为符<code>&#123;0&#125;</code>、<code>&#123;1&#125;</code>、……，不过这种方式写起来比%要麻烦的多：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"string\">&#x27;Hello, &#123;0&#125;, 成绩提升了 &#123;1:.1f&#125;%&#x27;</span>.<span class=\"built_in\">format</span>(<span class=\"string\">&#x27;小明&#x27;</span>, <span class=\"number\">17.125</span>)</span><br><span class=\"line\"><span class=\"string\">&#x27;Hello, 小明, 成绩提升了 17.1%&#x27;</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"f-string\"><a href=\"#f-string\" class=\"headerlink\" title=\"f-string\"></a><code>f-string</code></h4><p>最后一种格式化字符串的方法是使用以<code>f</code>开头的字符串，称之为<code>f-string</code>，它和普通字符串不同之处在于，字符串如果包含<code>&#123;xxx&#125;</code>，就会以对应变量替换：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>r = <span class=\"number\">2.5</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>s = <span class=\"number\">3.4</span> * r ** <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;The area of a child with radius <span class=\"subst\">&#123;r&#125;</span> is <span class=\"subst\">&#123;s:<span class=\"number\">.2</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>在上述代码中，<code>&#123;r&#125;</code>被变量<code>r</code>替换，<code>&#123;s:.2f&#125;</code>被变量<code>&#123;s&#125;</code>的值替换，并且<code>:</code>后面的<code>.2f</code>指定了格式化参数，因此，<code>&#123;s:.2f&#125;</code>的替换结果是<code>19.62</code>。</p>\n<h2 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h2><h3 id=\"定义函数\"><a href=\"#定义函数\" class=\"headerlink\" title=\"定义函数\"></a>定义函数</h3><p>在Python中，定义一个函数要使用<code>def</code>语句，依次写出函数名，括号，括号中的参数和冒号<code>:</code>，然后，在缩进块中编写函数体，函数的返回值用<code>return</code>语句返回。</p>\n<p>我们自定义一个求绝对值的<code>my_abs</code>函数为例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">my_abs</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> x &gt;= <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -x</span><br></pre></td></tr></table></figure>\n<p>请注意，函数体内部的语句在执行时，一旦执行到<code>return</code>时，函数就执行完毕，并将结果返回。因此，函数内部通过条件判断和循环可以实现非常复杂的逻辑。</p>\n<p>如果没有<code>return</code>语句，函数执行完毕后也会返回结果，只是结果为<code>None</code>。<code>return None</code>可以简写成<code>return</code>。</p>\n<p>在Python交互环境中定义函数时，注意Python会出现<code>...</code>的提示。函数定义结束后需要按两次回车重新回到<code>&gt;&gt;&gt;</code>提示符下。</p>\n<p>如果你已经把<code>my_abs()</code>的函数定义保存为<code>abstest.py</code>文件了，那么，可以在该文件的当前目录下启动Python解释器，用<code>from abstest import my_abs</code>来导入<code>my_abs()</code>函数，注意<code>abstest</code>是文件名（不含<code>.py</code>扩展名）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">from</span> abstest <span class=\"keyword\">import</span> my_abs</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>my_abs(-<span class=\"number\">9</span>)</span><br><span class=\"line\"><span class=\"number\">9</span></span><br><span class=\"line\">&gt;&gt;&gt;_</span><br></pre></td></tr></table></figure>\n<h4 id=\"空函数\"><a href=\"#空函数\" class=\"headerlink\" title=\"空函数\"></a>空函数</h4><p>如果想定义一个什么事也不做的空函数，可以用<code>pass</code>语句：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">nop</span>():</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n<p>一个空函数，缺少了<code>pass</code>，代码运行就会有语法错误。</p>\n<h4 id=\"参数检查\"><a href=\"#参数检查\" class=\"headerlink\" title=\"参数检查\"></a>参数检查</h4><p>调用函数时，如果参数个数不对，Python解释器会自动检查出来，并抛出<code>TypeError</code>。</p>\n<p>但是如果参数类型不对，Python解释器就无法帮我们检查。试试<code>my_abs</code>和内置函数<code>`abs</code>的差别：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>my_abs(<span class=\"string\">&#x27;A&#x27;</span>)</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">    File <span class=\"string\">&quot;&lt;stdin&gt;&quot;</span>,line <span class=\"number\">1</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    File <span class=\"string\">&quot;&lt;stdin&gt;&quot;</span>,line <span class=\"number\">2</span>, <span class=\"keyword\">in</span> my_abs</span><br><span class=\"line\">TypeError:unorderable types: <span class=\"built_in\">str</span>() &gt;= <span class=\"built_in\">int</span>()</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">abs</span>(<span class=\"string\">&#x27;A&#x27;</span>)</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">&quot;&lt;stdin&gt;&quot;</span>, line <span class=\"number\">1</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">TypeError: bad operand <span class=\"built_in\">type</span> <span class=\"keyword\">for</span> <span class=\"built_in\">abs</span>(): <span class=\"string\">&#x27;str&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>当传入了不恰当的参数时，内置函数<code>abs</code>会检查出错误，而我们定义的<code>my_abs</code>没有参数检查，会导致<code>if</code>语句出错，出错信息和<code>abs</code>不一样。所以这个函数定义不够完善。</p>\n<p>让我们来修改一下<code>my_abs</code>的定义，对参数类型作检查，只允许整数和浮点数类型的参数。数据类型检查可以用内置函数<code>isinstance()</code>实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">my_abs</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(x,(<span class=\"built_in\">int</span>,<span class=\"built_in\">float</span>)):</span><br><span class=\"line\">        <span class=\"keyword\">raise</span> TypeError(<span class=\"string\">&#x27;bad operand type&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> x &gt;= <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -x</span><br></pre></td></tr></table></figure>\n<h2 id=\"函数式编程\"><a href=\"#函数式编程\" class=\"headerlink\" title=\"函数式编程\"></a>函数式编程</h2><h3 id=\"高阶函数\"><a href=\"#高阶函数\" class=\"headerlink\" title=\"高阶函数\"></a>高阶函数</h3><h4 id=\"map-reduce\"><a href=\"#map-reduce\" class=\"headerlink\" title=\"map/reduce\"></a>map/reduce</h4><p>什么是高阶函数？我们以是实际代码为例子，一步步升入概念。</p>\n<p><strong>变量可以指向函数</strong></p>\n<p>以python内置的求绝对值函数<code>abs()</code>为例，调用该函数用以下代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">abs</span>(-<span class=\"number\">10</span>)</span><br><span class=\"line\"><span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n<p>但是如果只写<code>abs</code>呢？</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">abs</span></span><br><span class=\"line\">&lt;built-<span class=\"keyword\">in</span> function <span class=\"built_in\">abs</span>&gt;</span><br></pre></td></tr></table></figure>\n<p>可见<code>abs(function)</code>是函数调用，而abs是函数本身。</p>\n<p>要获得函数调用的结果我们可以把函数赋值给变量：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;x = <span class=\"built_in\">abs</span>(-<span class=\"number\">10</span>)</span><br><span class=\"line\">&gt;&gt;&gt;x</span><br><span class=\"line\"><span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n<p>结论：函数本身因为可以赋值给变量，即：变量可以指向函数。</p>\n<p>如果一个变量指向了一个函数，那么可否通过改变量来调用这个函数？用代码验证以下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;f = <span class=\"built_in\">abs</span></span><br><span class=\"line\">&gt;&gt;&gt;f(-<span class=\"number\">10</span>)</span><br><span class=\"line\"><span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n<p>说明变量<code>f</code>现在已经指向了<code>abs</code>函数本身。直接调用<code>abs()</code>函数和调用<code>f</code>完全相同。</p>\n<p><strong>函数名也是变量</strong></p>\n<p>那么函数名是什么呢？函数名其实就是指向函数的变量！对于<code>abs</code>这个函数完全可以把<code>abs</code>看成变量，它指向一个可以计算绝对值的函数！如果把<code>abs</code>指向其他对象会有什么情况发生？</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">abs</span> = <span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">abs</span>(-<span class=\"number\">10</span>)</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">&quot;&lt;stdin&gt;&quot;</span>, line <span class=\"number\">1</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">TypeError: <span class=\"string\">&#x27;int&#x27;</span> <span class=\"built_in\">object</span> <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"built_in\">callable</span></span><br></pre></td></tr></table></figure>\n<p>把<code>abs</code>指向<code>10</code>后，就无法通过<code>abs(-10)</code>调用该函数了！因为<code>abs</code>这个变量已经不指向求绝对值函数而是指向一个整数<code>10</code>！</p>\n<p>当然实际代码绝对不能这么写，这里是为了说明函数名也是变量。要恢复<code>abs</code>函数，请重启Python交互环境。</p>\n<p>注：由于<code>abs</code>函数实际上是定义在<code>import builtins</code>模块中的，所以要让修改<code>abs</code>变量的指向在其它模块也生效，要用<code>import builtins; builtins.abs = 10</code>。</p>\n<h3 id=\"偏函数\"><a href=\"#偏函数\" class=\"headerlink\" title=\"偏函数\"></a>偏函数</h3><p>python的<code>functools</code>提供了偏函数功能（Partial function）。</p>\n<p>假设要转换大量的二进制字符串，每次都传入<code>int(x,base = 2)</code>非常麻烦，于是，我们想到可以定义一个函数<code>int2()</code>，默认把<code>base = 2</code>传进去：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">int2</span>(<span class=\"params\">x,base = <span class=\"number\">2</span></span>):</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"built_in\">int</span>(x,base)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#这样我们进行二进制转换就可以了</span></span><br><span class=\"line\">int2(<span class=\"number\">1000000</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Partial function</span></span><br><span class=\"line\">int2 = functools.partial(<span class=\"built_in\">int</span>,base = <span class=\"number\">2</span>)</span><br><span class=\"line\">int2(<span class=\"string\">&#x27;123456&#x27;</span>)</span><br><span class=\"line\"><span class=\"number\">64</span></span><br></pre></td></tr></table></figure>\n<p>Partial function作用就是，帮助我们把一个函数的某些参数固定住（也就是设置默认值），返回一个新的函数。</p>\n<p>创建偏函数时可以接受函数对象、<em>args、*</em>kw这3个参数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int2 = functools.partial(<span class=\"built_in\">int</span>,base = <span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>\n<p>实际上固定了<code>int()</code>函数的关键字<code>base</code>，也就是：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int2(<span class=\"string\">&#x27;10010&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>相当于：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kw = &#123;<span class=\"string\">&#x27;base&#x27;</span> : <span class=\"number\">2</span>&#125;</span><br><span class=\"line\"><span class=\"built_in\">int</span>(<span class=\"string\">&#x27;10010&#x27;</span> , **kw)</span><br></pre></td></tr></table></figure>\n<p>当传入：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">max2 = functools.partial(<span class=\"built_in\">max</span>,<span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure>\n<p>实际上会把10作为*args的一部分自动加到左边：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">max2(<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>)</span><br></pre></td></tr></table></figure>\n<p>相当于：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">args = (<span class=\"number\">10</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>)</span><br><span class=\"line\"><span class=\"built_in\">max</span>(*args)</span><br></pre></td></tr></table></figure>\n<p>结果为10。</p>\n<hr>\n<h2 id=\"模块\"><a href=\"#模块\" class=\"headerlink\" title=\"模块\"></a>模块</h2><p>为了编写可维护代码，我们把很多函数分组，分别放到不同的文件里，这样，每个文件包含的代码就相对较少。在python中一个.py文件就可以被称为一个模块（Module）。</p>\n<p>使用模块有什么好处？</p>\n<p>最大的好处是提高了代码的可维护性。其次，编写代码不必从0开始。</p>\n<p>其次，使用模块还可以避免函数名和变量名冲突。相同名字的函数名和变量名可以存在不同模块中。</p>\n<p>如果模块名冲突怎么办？Python又引入了按目录来组织模块的方法，称为包（Package）。</p>\n<p>每个包目录下都有一个<code>__init__.py</code>文件，这个文件是必须存在的，否则，python就会把这个文件当成普通目录而不是包。<code>__init__.py</code>可以是空文件，也可以有Python代码，因为<code>__init__.py</code>本身就是一个模块，而它的模块名就是<code>mycompany</code>。</p>\n<h3 id=\"使用模块\"><a href=\"#使用模块\" class=\"headerlink\" title=\"使用模块\"></a>使用模块</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#!/usr/bin/env python3</span></span><br><span class=\"line\"><span class=\"comment\"># -*- coding: utf-8 -*-</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27; a test module &#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">__author__ = <span class=\"string\">&#x27;Michael Liao&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test</span>():</span><br><span class=\"line\">    args = sys.argv</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(args)==<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Hello, world!&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> <span class=\"built_in\">len</span>(args)==<span class=\"number\">2</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Hello, %s!&#x27;</span> % args[<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Too many arguments!&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    test()</span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">当我们在命令行运行hello模块文件时，Python解释器把一个特殊变量__name__置为__main__，而如果在其他地方导入该hello模块时，if判断将失败，因此，这种if测试可以让一个模块通过命令行运行时执行一些额外的代码，最常见的就是运行测试。</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>代码的第一，二行是标准注释，第一行注释可以让这个<code>hellow.py</code>文件直接在Unix/Linux/Mac上运行，第二行注释表示了.py文件本身使用了UTF-8编码；</p>\n<p>第四行是一个字符串，表示模块的文档注释，任何模块的第一行字符串都被视为模块文档注释；</p>\n<p>第六行<code>__author__</code>变量把作者写进去。</p>\n<p>后面开始就是真正的代码部分。</p>\n<p>使用<code>sys</code>模块的第一步就是导入该模块：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br></pre></td></tr></table></figure>\n<p>导入了sys模块后，我们就有了变量sys指向该模块，利用<code>sys</code>这个变量，就可以访问<code>sys</code>模块所有的功能。</p>\n<p>sys有一个argv变量，用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是<code>.py</code>文件的名称，例如：</p>\n<p>运行<code>`python3 hello.py</code>获得的<code>sys.argv</code>就是<code>[&#39;hellow.py&#39;]</code></p>\n<p>运行<code>python3 hello.py Michael</code>获得的<code>sys.argv</code>就是<code>[&#39;hello.py&#39;, &#39;Michael&#39;]</code>。</p>\n<p>如果启动Python交互环境，再导入<code>hello</code>模块：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ python3</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">import</span> hello</span><br><span class=\"line\">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>\n<p>导入时，没有打印<code>hello world!</code>因为没有执行<code>test()</code>函数。</p>\n<p>调用<code>hello.test()</code>时，才能打印出<code>hello world!</code>:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;hello.test()</span><br><span class=\"line\">Hello,world!</span><br></pre></td></tr></table></figure>\n<h4 id=\"作用域\"><a href=\"#作用域\" class=\"headerlink\" title=\"作用域\"></a>作用域</h4><p>在一个模块中，我们可能会定义很多函数和变量，但有的函数我们希望给别人使用，有的仅仅在模块内部使用。在Python中我们是通过<code>_</code>前缀来实现的。</p>\n<p>正常函数和变量名是公开的（public），可以被直接引用。</p>\n<p>类似<code>__xx__</code>是特殊变量，可以被直接引用，但有特殊用途，比如上面的<code>__author__</code>，<code>__name__</code>就是特殊变量，<code>hello</code>模块定义的文档注释也可以用特殊变量<code>__doc__</code>访问，我们自己的变量一般不要用这种变量名。</p>\n<p>类似<code>_xxx</code>和<code>__xxx</code>这样的函数或变量就是非公开的（private），不应该被直接引用，比如<code>_abc</code>，<code>__abc</code>等；</p>\n<p>之所以我们说，private函数和变量“不应该”被直接引用，而不是“不能”被直接引用，是因为Python并没有一种方法可以完全限制访问private函数或变量，但是，从编程习惯上不应该引用private函数或变量。</p>\n<p>private函数或变量不应该被别人引用，那它们有什么用呢？请看例子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">_private_1</span>(<span class=\"params\">name</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&#x27;Hello, %s&#x27;</span> % name</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">_private_2</span>(<span class=\"params\">name</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&#x27;Hi, %s&#x27;</span> % name</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">greeting</span>(<span class=\"params\">name</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(name) &gt; <span class=\"number\">3</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> _private_1(name)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> _private_2(name)</span><br></pre></td></tr></table></figure>\n<p>我们在模块里公开<code>greeting()</code>函数，而把内部逻辑用private函数隐藏起来了，这样，调用<code>greeting()</code>函数不用关心内部的private函数细节，这也是一种非常有用的代码封装和抽象的方法，即：</p>\n<p>外部不需要引用的函数全部定义成private，只有外部需要引用的函数才定义为public。</p>\n<h2 id=\"面向对象编程（OOP）\"><a href=\"#面向对象编程（OOP）\" class=\"headerlink\" title=\"面向对象编程（OOP）\"></a>面向对象编程（OOP）</h2><p>OOP把对象作为基本单元，一个对象包含了数据和操作数据的函数。</p>\n<p>OOP把计算机程序视为一系列命令的集合，而每个对象都可以接受其他对象发过来的消息，并处理这些消息，计算机程序的执行就是一系列消息在对象之间传递。</p>\n<p>在Python中，所有的数据类型都可以视为对象，当然也可以自定义对象。自定义的对象数据类型就是面向对象中的类（Class）的概念。</p>\n<h3 id=\"类和实例\"><a href=\"#类和实例\" class=\"headerlink\" title=\"类和实例\"></a>类和实例</h3><p>OOP最重要概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，实例是根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但是各自的数据可能不同。</p>\n<p>仍以Student类为例，在Python中，定义类是通过<code>class</code>关键字：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n<p><code>calss</code>后面紧接着是类名，即<code>Student</code>类创建出<code>Student</code>实例，创建实例是通过类名+()实现的：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart = Student()</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart</span><br><span class=\"line\">&lt;__main__.Student <span class=\"built_in\">object</span> at <span class=\"number\">0x10a67a590</span>&gt;</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>Student</span><br><span class=\"line\">&lt;<span class=\"keyword\">class</span> <span class=\"string\">&#x27;__main__.Student&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>\n<p>可以看到，变量<code>bart</code>指向的就是一个<code>Student</code>的实例，后面的<code>0x10a67a590</code>是内存地址，每个object的地址都不一样，而<code>Student</code>本身则是一个类。</p>\n<p>可以自由地给一个实例变量绑定属性，比如，给实例<code>bart</code>绑定一个<code>name</code>属性：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart.name = <span class=\"string\">&#x27;Bart Simpson&#x27;</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart.name</span><br><span class=\"line\"><span class=\"string\">&#x27;Bart Simpson&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>由于类可以起到模板作用，因此可以在创建实例的时候，把一些我们任务必须绑定的属性强制填进去。通过定义一个特殊的<code>__init__</code>方法，创建实例的时候就把<code>name</code>，<code>score</code>等属性绑定上去：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,name,score</span>):</span><br><span class=\"line\">        self.name = name</span><br><span class=\"line\">        self.score = score</span><br></pre></td></tr></table></figure>\n<p>注意，<code>__init__</code>方法的第一个参数永远是<code>self</code>，表示创建的实例本身，因此，在<code>__init__</code>方法内部，就可以把各种属性绑定到<code>self</code>,因为<code>self</code>就指向创建的实例本身。</p>\n<p>有了<code>__init__</code>方法，在创建实例的时候，就不能传入空的参数了，必须传入与<code>__init__</code>方法相匹配的参数，但是self不需要传，Python解释器会自己把实例变量传进去。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;bart = Student(<span class=\"string\">&#x27;Bart Simpson&#x27;</span>,<span class=\"number\">59</span>)</span><br><span class=\"line\">&gt;&gt;&gt;bart.name</span><br><span class=\"line\"><span class=\"string\">&#x27;Bart Simpson&#x27;</span></span><br><span class=\"line\">&gt;&gt;&gt;bart.score</span><br><span class=\"line\"><span class=\"number\">59</span></span><br></pre></td></tr></table></figure>\n<p>和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量<code>self</code>，并且，调用时不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所有，你仍然可以用默认参数、可变参数、关键字参数、命名关键字参数。</p>\n<h4 id=\"数据封装\"><a href=\"#数据封装\" class=\"headerlink\" title=\"数据封装\"></a>数据封装</h4><p>OOP的一个重要特点就是数据封装。在上面的<code>Student</code>类中，每个实例就拥有各自的<code>name</code>和<code>score</code>这些数据。我们可以通过函数来访问这些数据，比如打印一个学生的成绩。但是<code>Student</code>实例本身就拥有这些数据，要访问这些数据，就没有必要从外面的函数去访问，可以直接在<code>Student</code>类的内部定义访问数据的函数，这样，就把“数据”给封装起来了。这些封装数据的函数是和<code>Student</code>类本身是关联起来的，我们称之为类的方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,name,score</span>):</span><br><span class=\"line\">        self.name = name</span><br><span class=\"line\">        self.score = score</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">print_score</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;%s : %s&#x27;</span> % (self.name, self.score))</span><br></pre></td></tr></table></figure>\n<p>要定义一个方法，除了第一个参数是<code>self</code>以外，其他和普通函数一样。要调用另一个方法，只需要在实例变量上直接调用，除了<code>self</code>不用传递，其他参数正常传入：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;bart.print_score()</span><br><span class=\"line\">Bart Simpson: <span class=\"number\">59</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"访问限制\"><a href=\"#访问限制\" class=\"headerlink\" title=\"访问限制\"></a>访问限制</h3><p>在class内部，可以有属性和方法，而外部代码可以通过直接调用实例变量的方法来操作数据，这样，就隐藏了内部的复杂逻辑。</p>\n<p>但是从<code>Student</code>类的定义来看，外部代码还是可以自由的修改一个实例的<code>name</code>、<code>scorre</code>属性。</p>\n<p>如果要让内部属性不被外部访问，可以把属性名称前加两个下划线<code>__</code>，在python中，实例的变量名如果以<code>__</code>开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，所以，我们把<code>Student</code>类改一改：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,name,score</span>):</span><br><span class=\"line\">        self.__name = name</span><br><span class=\"line\">        self.__score = score</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">print_score</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        prnint(<span class=\"string\">&#x27;%s %s&#x27;</span> % (self.__name, self.__score))</span><br></pre></td></tr></table></figure>\n<p>改完后，对外部代码来说没什么变动，但以及无法从外部访问<code>实例变量.__name</code>和<code>实例变量.__score</code>了</p>\n<p>但是如果外部代码要获取score怎么办？可以给<code>Student</code>类增加<code>set_score</code>方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">set_score</span>(<span class=\"params\">self,score</span>):</span><br><span class=\"line\">        self.__score = score</span><br></pre></td></tr></table></figure>\n<p>你也许会问，原先那种直接通过<code>bart.score = 99</code>也可以修改啊，为什么要定义一个方法大费周折？因为在方法中，可以对参数做检查，避免传入无效的参数：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">set_score</span>(<span class=\"params\">self, score</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"number\">0</span> &lt;= score &lt;= <span class=\"number\">100</span>:</span><br><span class=\"line\">            self.__score = score</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">raise</span> ValueError(<span class=\"string\">&#x27;bad score&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>而不能直接访问<code>__name</code>是因为Python解释器对外把<code>__name</code>变量改成了<code>_Student__name</code>，所以，仍然可以通过<code>_Student__name</code>来访问<code>__name</code>变量：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart._Student__name</span><br><span class=\"line\"><span class=\"string\">&#x27;Bart Simpson&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>但是强烈建议你不要这么干，因为不同版本的Python解释器可能会把<code>__name</code>改成不同的变量名。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart = Student(<span class=\"string\">&#x27;Bart Simpson&#x27;</span>, <span class=\"number\">59</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart.get_name()</span><br><span class=\"line\"><span class=\"string\">&#x27;Bart Simpson&#x27;</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart.__name = <span class=\"string\">&#x27;New Name&#x27;</span> <span class=\"comment\"># 设置__name变量！</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart.__name</span><br><span class=\"line\"><span class=\"string\">&#x27;New Name&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>表面上看，外部代码“成功”地设置了<code>__name</code>变量，但实际上这个<code>__name</code>变量和class内部的<code>__name</code>变量<em>不是</em>一个变量！内部的<code>__name</code>变量已经被Python解释器自动改成了<code>_Student__name</code>，而外部代码给<code>bart</code>新增了一个<code>__name</code>变量。不信试试：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart.get_name() <span class=\"comment\"># get_name()内部返回self.__name</span></span><br><span class=\"line\"><span class=\"string\">&#x27;Bart Simpson&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"继承和多态\"><a href=\"#继承和多态\" class=\"headerlink\" title=\"继承和多态\"></a>继承和多态</h3><p>在OOP程序设计中，当我们定义了一个class的时候，可以从某个现有的class继承，新的class称为子类（Subclass），而被继承的class称为基类，父类或超类（Base class，super class）。</p>\n<p>比如我们编写了一个名为<code>Animal</code>的class，有一个<code>run</code>方法可以直接打印：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Animal</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">run</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Animal is running……&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>当我们要编写dog和cat类时，就可以直接从<code>Animal</code>类继承：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Dog</span>(<span class=\"title class_ inherited__\">Animal</span>):</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Cat</span>(<span class=\"title class_ inherited__\">Animal</span>):</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n<p>对于<code>Dog</code>类来说，<code>Animal</code>就是它的父类，对于<code>Animal</code>类来说，<code>Dog</code>就是它的子类。<code>Cat</code>和<code>Dog</code>类似。</p>\n<p>继承有什么好处?最大的好处是，子类获得了父类的全部功能。由于<code>Animal</code>实现了<code>run()</code>方法，因此，<code>Dog</code>和<code>Cat</code>作为它的子类，什么事没干就自动拥有了<code>run()</code>方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dog = Dog()</span><br><span class=\"line\">dog.run</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#cat与dog的处理方式一致</span></span><br><span class=\"line\">cat = Cat()</span><br><span class=\"line\">cat.run</span><br></pre></td></tr></table></figure>\n<p>运行结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Animal <span class=\"keyword\">is</span> running...</span><br><span class=\"line\">Animal <span class=\"keyword\">is</span> running...</span><br></pre></td></tr></table></figure>\n<p>当然，也可以对子类增加一些方法，比如Dog类：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Dog</span>(<span class=\"title class_ inherited__\">Animal</span>):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">run</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Dog is running...&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">eat</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Eating meat...&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>继承的第二个好处需要我们对代码做一点改进。你看到了，无论是<code>Dog</code>还是<code>Cat</code>，它们<code>run()</code>的时候，显示的都是<code>Animal is running...</code>，符合逻辑的做法是分别显示<code>Dog is running...</code>和<code>Cat is running...</code>，因此，对<code>Dog</code>和<code>Cat</code>类改进如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Dog</span>(<span class=\"title class_ inherited__\">Animal</span>):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">run</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Dog is running...&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Cat</span>(<span class=\"title class_ inherited__\">Animal</span>):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">run</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Cat is running...&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>再次运行结果如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Dog <span class=\"keyword\">is</span> running...</span><br><span class=\"line\">Cat <span class=\"keyword\">is</span> running...</span><br></pre></td></tr></table></figure>\n<p>当子类和父类都存在相同的<code>run()</code>方法时，我们说，子类的<code>run</code>覆盖了父类的<code>run</code>，在代码运行的时候，总是会调用子类的<code>run</code>。这样我们就获得了继承的另一个好处：多态。</p>\n<p>要理解什么是多态，我们首先要对数据类型再作一点说明。当我们定义一个class的时候，我们实际上就定义了一种数据类型。</p>\n<p>判断一个变量是否是某个类型可以用<code>isinstance()</code>判断：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">isinstance</span>(a, <span class=\"built_in\">list</span>)</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">isinstance</span>(b, Animal)</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">isinstance</span>(c, Dog)</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br></pre></td></tr></table></figure>\n<p>看来<code>a</code>、<code>b</code>、<code>c</code>确实对应着<code>list</code>、<code>Animal</code>、<code>Dog</code>这3种类型。</p>\n<p>但是等等，试试：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">isinstance</span>(c, Animal)</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br></pre></td></tr></table></figure>\n<p>看来<code>c</code>不仅仅是<code>Dog</code>，<code>c</code>还是<code>Animal</code>！</p>\n<p>在继承关系中，如果一个数据类型是某个数据类型的子类，那它的数据类型也可以被看成是父类。但是反过来就不行：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;b = Animal()</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">isinstance</span>(b,Dog)</span><br><span class=\"line\"><span class=\"literal\">False</span></span><br></pre></td></tr></table></figure>\n<p>理解多态的好处，我们还需要编写一个函数，接受一个<code>Anmial</code>类型的变量：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">run_twice</span>(<span class=\"params\">animal</span>):</span><br><span class=\"line\">    animal.run()</span><br><span class=\"line\">    animal.run()</span><br></pre></td></tr></table></figure>\n<p>当我们传入<code>Animal</code>的实例时，<code>run_twice()</code>就打印出：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>run_twice(Animal())</span><br><span class=\"line\">Animal <span class=\"keyword\">is</span> running...</span><br><span class=\"line\">Animal <span class=\"keyword\">is</span> running...</span><br></pre></td></tr></table></figure>\n<p>传入<code>Dog</code>的实例时，<code>run_twice()</code>打印：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;run_twice(Dog())</span><br><span class=\"line\">Dog <span class=\"keyword\">is</span> runninng……</span><br><span class=\"line\">Dog <span class=\"keyword\">is</span> runninng……</span><br></pre></td></tr></table></figure>\n<p>现在我们再定义一个<code>Tortoise</code>类型，也从<code>Animal</code>派生：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Tortoise</span>(<span class=\"title class_ inherited__\">Animal</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">run</span>(<span class=\"params\">self</span>)</span><br><span class=\"line\">    \t<span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Tortoise is running slowwly…&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>当我们调用<code>run_twice()</code>时，传入<code>Tortoise</code>的实例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>run_twice(Tortoise())</span><br><span class=\"line\">Tortoise <span class=\"keyword\">is</span> running slowly...</span><br><span class=\"line\">Tortoise <span class=\"keyword\">is</span> running slowly...</span><br></pre></td></tr></table></figure>\n<p>你会发现，新增一个<code>Animal</code>的子类，不必对<code>run_twice</code>做任何修改，实际上，任何依赖<code>Animal</code>作为参数的函数或者方法都可以不加修改的正常运行，原因就在于多态。</p>\n<p>多态的好处是，当我们传入<code>Dog</code>，<code>Cat</code>，<code>Tortoise</code>……时，我们只需要接受<code>Anmial</code>类型就可以了，因为<code>Dog</code>，<code>Cat</code>，<code>Tortoise</code>……都是<code>Animal</code>类型，然后，按照<code>Animal</code>类型进行操作即可。由于<code>Anmial</code>类型有<code>run()</code>方法，因此，传入的任意类型，只要是<code>Aniaml</code>类或者是子类，就会自动调用实际类型的<code>run()</code>方法，这就是多态的意思。</p>\n<p>对于一个变量，我们只需要知道它是<code>Animal</code>类型，无需确切地知道它的子类型，就可以放心地调用<code>run()</code>方法，而具体调用的<code>run()</code>方法是作用在<code>Animal</code>、<code>Dog</code>、<code>Cat</code>还是<code>Tortoise</code>对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：调用方只管调用，不管细节，而当我们新增一种<code>Animal</code>的子类时，只要确保<code>run()</code>方法编写正确，不用管原来的代码是如何调用的。这就是著名的“开闭”原则：</p>\n<p>对扩展开放：允许新增<code>Animal</code>子类；</p>\n<p>对修改封闭：不需要修改依赖<code>Animal</code>类型的<code>run_twice()</code>等函数。</p>\n<h3 id=\"获取对象信息\"><a href=\"#获取对象信息\" class=\"headerlink\" title=\"获取对象信息\"></a>获取对象信息</h3><p>当我们拿到一个对象引用时，如何知道这个对象是什么类型，有那些方法呢？</p>\n<h4 id=\"使用type\"><a href=\"#使用type\" class=\"headerlink\" title=\"使用type()\"></a>使用<code>type()</code></h4><p>我们判断对象类型，使用<code>type</code>函数，基本类型都可以用<code>type</code>判断：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">type</span>(<span class=\"number\">123</span>)</span><br><span class=\"line\">&lt;<span class=\"keyword\">class</span> <span class=\"string\">&#x27;int&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>\n<p>如果一个对象指向函数或者类，也可以用<code>type()</code>判断。但是<code>type()</code>函数返回的是什么类型呢？它返回对应的Class类型。</p>\n<p>如果要判断一个对象是否是函数怎么办？可以使用<code>types</code>模块中定义的常量：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> types</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">def</span> <span class=\"title function_\">fn</span>():</span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"keyword\">pass</span></span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">type</span>(fn)==types.FunctionType</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">type</span>(<span class=\"built_in\">abs</span>)==types.BuiltinFunctionType</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">type</span>(<span class=\"keyword\">lambda</span> x: x)==types.LambdaType</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">type</span>((x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>)))==types.GeneratorType</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"使用dir\"><a href=\"#使用dir\" class=\"headerlink\" title=\"使用dir()\"></a>使用dir()</h4><p>如果要获得一个对象的所有属性和方法，可以使用<code>dir()</code>函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">dir</span>(<span class=\"string\">&#x27;ABC&#x27;</span>)</span><br><span class=\"line\">[<span class=\"string\">&#x27;__add__&#x27;</span>, <span class=\"string\">&#x27;__class__&#x27;</span>,..., <span class=\"string\">&#x27;__subclasshook__&#x27;</span>, <span class=\"string\">&#x27;capitalize&#x27;</span>, <span class=\"string\">&#x27;casefold&#x27;</span>,..., <span class=\"string\">&#x27;zfill&#x27;</span>]</span><br></pre></td></tr></table></figure>\n<p>紧接着，可以测试对象的属性：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">hasattr</span>(obj,<span class=\"string\">&#x27;x&#x27;</span>) <span class=\"comment\">#有x属性吗？</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">setattr</span>(obj,<span class=\"string\">&#x27;y&#x27;</span>) <span class=\"comment\">#设置x属性</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">getattr</span>(obj, <span class=\"string\">&#x27;x&#x27;</span>) <span class=\"comment\"># 获取属性&#x27;x&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>可以传入一个default参数，如果属性不存在，就返回默认值：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">getattr</span>(obj, <span class=\"string\">&#x27;z&#x27;</span>, <span class=\"number\">404</span>) <span class=\"comment\"># 获取属性&#x27;z&#x27;，如果不存在，返回默认值404</span></span><br><span class=\"line\"><span class=\"number\">404</span></span><br></pre></td></tr></table></figure>\n<p>也可以获得对象的方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">hasattr</span>(obj, <span class=\"string\">&#x27;power&#x27;</span>) <span class=\"comment\"># 有属性&#x27;power&#x27;吗？</span></span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">getattr</span>(obj, <span class=\"string\">&#x27;power&#x27;</span>) <span class=\"comment\"># 获取属性&#x27;power&#x27;</span></span><br><span class=\"line\">&lt;bound method MyObject.power of &lt;__main__.MyObject <span class=\"built_in\">object</span> at <span class=\"number\">0x10077a6a0</span>&gt;&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"面向对象高级编程\"><a href=\"#面向对象高级编程\" class=\"headerlink\" title=\"面向对象高级编程\"></a>面向对象高级编程</h2><h3 id=\"使用-slots\"><a href=\"#使用-slots\" class=\"headerlink\" title=\"使用__slots__\"></a>使用__slots__</h3><p>正常情况下，当我们定义了一个class，创建了一个clsaa的实例后，我们可以给该实例绑定任何实例和方法，这就是动态语言的灵活性。先定义class：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n<p>然后，尝试给实例绑定一个属性：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;s = Student</span><br><span class=\"line\">&gt;&gt;&gt;s.name = <span class=\"string\">&#x27;Michael&#x27;</span> <span class=\"comment\">#动态给实例绑定一个对象</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">print</span>(<span class=\"string\">&#x27;s.name&#x27;</span>)</span><br><span class=\"line\">Micheal</span><br></pre></td></tr></table></figure>\n<p>还可以尝试给实例绑定一个方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">def</span> <span class=\"title function_\">set_age</span>(<span class=\"params\">self,age</span>): <span class=\"comment\">#定义一个函数作为实例方法</span></span><br><span class=\"line\">    \tself.age=age</span><br><span class=\"line\">    </span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">from</span> <span class=\"built_in\">type</span> <span class=\"keyword\">import</span> MethodType</span><br><span class=\"line\">&gt;&gt;&gt;s.set_age = MethodeType(set_age, s) <span class=\"comment\">#给实例绑定一个方法</span></span><br><span class=\"line\">&gt;&gt;&gt;s.set_age(<span class=\"number\">25</span>) <span class=\"comment\">#调用实例方法</span></span><br><span class=\"line\">&gt;&gt;&gt;s.age <span class=\"comment\">#测试结果</span></span><br><span class=\"line\"><span class=\"number\">25</span></span><br></pre></td></tr></table></figure>\n<p>但是==给一个实例绑定的方法对另一个实例是不起作用的。==</p>\n<h2 id=\"进程和线程\"><a href=\"#进程和线程\" class=\"headerlink\" title=\"进程和线程\"></a>进程和线程</h2><h3 id=\"多进程\"><a href=\"#多进程\" class=\"headerlink\" title=\"多进程\"></a>多进程</h3><p>Unix/Linux操作系统提供了一个<code>fork</code>调用，普通的函数调用，调用一次返回一次，但是<code>fork</code>调用一次，返回两次，因为操作系统自动把当前进程（父进程）复制了一份（称为子进程），然后分别在父进程和子进程内返回。</p>\n<p>子进程永远返回<code>0</code>，而父进程返回子进程的ID，子进程只需要调用<code>getppid()</code>就可以拿到父进程的ID。</p>\n<p>Python的<code>os</code>模块封装了常见的系统调用，其中就包括<code>fork</code>，可以在Python程序中轻松创建子进程：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Process (%s) start...&#x27;</span> % os.getpid())</span><br><span class=\"line\"><span class=\"comment\"># Only works on Unix/Linux/Mac:</span></span><br><span class=\"line\">pid = os.fork()</span><br><span class=\"line\"><span class=\"keyword\">if</span> pid == <span class=\"number\">0</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;I am child process (%s) and my parent is %s.&#x27;</span> % (os.getpid(), os.getppid()))</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;I (%s) just created a child process (%s).&#x27;</span> % (os.getpid(), pid))</span><br></pre></td></tr></table></figure>\n<p>如果要启动大量子进程，可以用进程池的方法批量创建子进程：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> multiprocessing <span class=\"keyword\">import</span> Pool</span><br><span class=\"line\"><span class=\"keyword\">import</span> os, timme, random</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">long_time_task</span>(<span class=\"params\">name</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Run task %s (%s)...&#x27;</span> % (name, os.getpid()))</span><br><span class=\"line\">    start = time.time()</span><br><span class=\"line\">    time.sleep(random.random() * <span class=\"number\">3</span>)</span><br><span class=\"line\">    end = time.time()</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Task %s runs %0.2f seconds.&#x27;</span> % (name, (end - start)))</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Parent process %s.&#x27;</span> % os.getpid())</span><br><span class=\"line\">    p = Pool(<span class=\"number\">4</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">5</span>):</span><br><span class=\"line\">        p.apply_async(long_time_task, args=(i,))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Waiting for all subprocesses done...&#x27;</span>)</span><br><span class=\"line\">    p.close()</span><br><span class=\"line\">    p.join()</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;All subprocesses done.&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>代码解读：</p>\n<p>对<code>Pool</code>对象调用<code>join()</code>方法会等待所有子进程执行完毕，调用<code>join()</code>之前必须先调用<code>close()</code>，调用<code>close()</code>之后就不能继续添加新的<code>Process</code>了。</p>\n<p>例子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pool = Pool(<span class=\"number\">8</span>) <span class=\"comment\">#可以同时跑8个进程</span></span><br><span class=\"line\">\tpool.<span class=\"built_in\">map</span>(get_all, [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>)])</span><br><span class=\"line\">    pool.close</span><br><span class=\"line\">    pool.join()</span><br></pre></td></tr></table></figure>\n<p>这里的<code>pool.close()</code>是说关闭pool，使其不在接受新的（主进程）任务。</p>\n<p>这里的<code>pool.join()</code>是说：主进程阻塞后，让子进程继续运行完成，子进程运行完成后，再把主进程全部关掉。</p>\n<h4 id=\"子进程\"><a href=\"#子进程\" class=\"headerlink\" title=\"子进程\"></a>子进程</h4><p>很多时候，子进程并不是自生，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。</p>\n<p>==subprocess==模块可以让我们非常方便的启动一个子进程，然后控制其输入和输出。</p>\n<p>下面的例子演示了如何在Python代码中运行命令<code>nslookup www.python.org</code>，这和命令行的效果是一样的：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> subprocess</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;$ nslookup www.python.org&#x27;</span>)</span><br><span class=\"line\">r = subprocess.call([<span class=\"string\">&#x27;nslookup&#x27;</span>,<span class=\"string\">&#x27;www.python.org&#x27;</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Exit code:&#x27;</span>, r)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ nslookup www.python.org</span><br><span class=\"line\">Server:\t\t<span class=\"number\">192.168</span><span class=\"number\">.19</span><span class=\"number\">.4</span></span><br><span class=\"line\">Address:\t<span class=\"number\">192.168</span><span class=\"number\">.19</span><span class=\"number\">.4</span><span class=\"comment\">#53</span></span><br><span class=\"line\"></span><br><span class=\"line\">Non-authoritative answer:</span><br><span class=\"line\">www.python.org\tcanonical name = python.<span class=\"built_in\">map</span>.fastly.net.</span><br><span class=\"line\">Name:\tpython.<span class=\"built_in\">map</span>.fastly.net</span><br><span class=\"line\">Address: <span class=\"number\">199.27</span><span class=\"number\">.79</span><span class=\"number\">.223</span></span><br><span class=\"line\"></span><br><span class=\"line\">Exit code: <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"subprocess模块详解\"><a href=\"#subprocess模块详解\" class=\"headerlink\" title=\"subprocess模块详解\"></a>subprocess模块详解</h4><p><code>subprocess</code>模块是<strong>Python 2.4</strong>中新增的一个模块，它允许你生成新的进程，连接到它们的<strong>input/output/error</strong>管道，并获取它们的返回（状态）码。这个模块的目的在于替换几个旧的模块和方法，如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">os.system</span><br><span class=\"line\">os.spawn*</span><br></pre></td></tr></table></figure>\n<p><strong>1.subprocess模块中的常用函数</strong></p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>subprocess.run()</td>\n<td>py3.5中新增的函数。执行制定的命令，等待命令执行完毕后返回一个包含执行结果的CompletedProcess类的实例。</td>\n</tr>\n<tr>\n<td>subprocess.call()</td>\n<td>执行指定命令，返回命令执行状态，其功能类似于<code>os.system(cmd)</code></td>\n</tr>\n<tr>\n<td>subprocess.check_call()</td>\n<td>python2.5中新增的函数，执行制定的命令，如果执行成功则返回状态码，否则抛出异常。其功能等价于subprocess.run(…, check = True)</td>\n</tr>\n<tr>\n<td>subprocess.check_output()</td>\n<td>python 2.7中新增的函数。执行制定的命令，如果执行状态码为0则返回命令执行结果。都则抛出异常。</td>\n</tr>\n<tr>\n<td>subprocess.getoutput(cmd)</td>\n<td>接受字符串格式的命令，执行命令并返回执行结果，其功能类似于os.popen(cmd).reead()和commands.getoutput(cmd)</td>\n</tr>\n<tr>\n<td>subprocess.getstatusoutput(cmd)</td>\n<td>执行cmd命令，返回一个元组（命令执行状态，命令执行结果输出），其功能类似于commands,getstatusoutput()</td>\n</tr>\n</tbody>\n</table>\n</div>\n<blockquote>\n<p>说明：</p>\n<p>在Python 3.5之后的版本中，官方文档中提倡通过subprocess.run()函数替代其他函数来使用subproccess模块的功能；<br>在Python 3.5之前的版本中，我们可以通过subprocess.call()，subprocess.getoutput()等上面列出的其他函数来使用subprocess模块的功能；<br>subprocess.run()、subprocess.call()、subprocess.check_call()和subprocess.check_output()都是通过对subprocess.Popen的封装来实现的高级函数，因此如果我们需要更复杂功能时，可以通过subprocess.Popen来完成。<br>subprocess.getoutput()和subprocess.getstatusoutput()函数是来自Python 2.x的commands模块的两个遗留函数。它们隐式的调用系统shell，并且不保证其他函数所具有的安全性和异常处理的一致性。另外，它们从Python 3.3.4开始才支持Windows平台。</p>\n</blockquote>\n<p><strong>上面各函数的定义以及参数说明</strong></p>\n<p>函数参数列表</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">subprocess.run(args, *, stdin=<span class=\"literal\">None</span>, <span class=\"built_in\">input</span>=<span class=\"literal\">None</span>, stdout=<span class=\"literal\">None</span>, stderr=<span class=\"literal\">None</span>, shell=<span class=\"literal\">False</span>, timeout=<span class=\"literal\">None</span>, check=<span class=\"literal\">False</span>, universal_newlines=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">subprocess.call(args, *, stdin=<span class=\"literal\">None</span>, stdout=<span class=\"literal\">None</span>, stderr=<span class=\"literal\">None</span>, shell=<span class=\"literal\">False</span>, timeout=<span class=\"literal\">None</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">subprocess.check_call(args, *, stdin=<span class=\"literal\">None</span>, stdout=<span class=\"literal\">None</span>, stderr=<span class=\"literal\">None</span>, shell=<span class=\"literal\">False</span>, timeout=<span class=\"literal\">None</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">subprocess.check_output(args, *, stdin=<span class=\"literal\">None</span>, stderr=<span class=\"literal\">None</span>, shell=<span class=\"literal\">False</span>, universal_newlines=<span class=\"literal\">False</span>, timeout=<span class=\"literal\">None</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">subprocess.getstatusoutput(cmd)</span><br><span class=\"line\"></span><br><span class=\"line\">subprocess.getoutput(cmd)</span><br></pre></td></tr></table></figure>\n<p>参数说明：</p>\n<ul>\n<li><p>args：要执行的shell命令，默认应该是一个字符串序列，如[‘df’, ‘-Th’]，也可以是一个字符串，但要把shell的参数的值设置为True</p>\n</li>\n<li><p>shell：如果sehll为True，那么指定的命令将通过shell执行。</p>\n</li>\n<li><p>check：如果check参数的值是True，且执行命令的进程以非0状态码退出，则会抛出一个CalledProcessError的异常，且该异常会包含参数、退出状态码、以及stdout和stderr</p>\n</li>\n<li><p><code>stdout, stderr：input</code>： 该参数是传递给Popen.communicate()，通常该参数的值必须是一个字节序列，如果universal_newlines=True，则其值应该是一个字符串。</p>\n<ol>\n<li><p>run()函数默认不会捕获命令执行结果的正常输出和错误输出，如果我们向获取这些内容需要传递subprocess.PIPE，然后可以通过返回的CompletedProcess类实例的stdout和stderr属性或捕获相应的内容；</p>\n</li>\n<li><p>call()和check_call()函数返回的是命令执行的状态码，而不是CompletedProcess类实例，所以对于它们而言，stdout和stderr不适合赋值为subprocess.PIPE；<br>3</p>\n</li>\n<li>check_output()函数默认就会返回命令执行结果，所以不用设置stdout的值，如果我们希望在结果中捕获错误信息，可以执行stderr=subprocess.STDOUT。</li>\n</ol>\n</li>\n<li><p><code>universal_newlines</code>： 该参数影响的是输入与输出的数据格式，比如它的值默认为False，此时stdout和stderr的输出是字节序列；当该参数的值设置为True时，stdout和stderr的输出是字符串。</p>\n</li>\n</ul>\n<p><strong>3.subprocess.CompletedProcess类介绍</strong></p>\n<p>需要说明的是，<code>subprocess.run()</code>函数是Python3.5中新增一个高级函数，其返回值是一个<code>subprocess.CompletedProcess</code>类的实例，因此，subprocess,completedProcess类也是Python 3.5中才存在的。它表示的是一个以结束进程的状态信息。</p>\n<h4 id=\"subprocess-Popen介绍\"><a href=\"#subprocess-Popen介绍\" class=\"headerlink\" title=\"subprocess.Popen介绍\"></a>subprocess.Popen介绍</h4><p>该类用于在一个新的程序中执行一个子程序。前面我们提到过，上面介绍的这些函数艘是基于<code>subprocess.Popen</code>类实现的，通过使用这些被封装的高级函数可以很方便的完成一些常见需求。由于<code>subprocess</code>模块底层的进程创建和管理是有Popen类来处理的，因此，当我们无法通过上面哪些高级函数来实现一些不太常见的功能时就可以通过subprocess.Popen类提供灵活的api来完成。</p>\n<p>1.subprocess.Popen构造函数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">subprocess</span>.Popen(args, bufsize=-<span class=\"number\">1</span>, executable=<span class=\"literal\">None</span>, stdin=<span class=\"literal\">None</span>, stdout=<span class=\"literal\">None</span>, stderr=<span class=\"literal\">None</span>, </span><br><span class=\"line\">    preexec_fn=<span class=\"literal\">None</span>, close_fds=<span class=\"literal\">True</span>, shell=<span class=\"literal\">False</span>, cwd=<span class=\"literal\">None</span>, env=<span class=\"literal\">None</span>, universal_newlines=<span class=\"literal\">False</span>,</span><br><span class=\"line\">    startup_info=<span class=\"literal\">None</span>, creationflags=<span class=\"number\">0</span>, restore_signals=<span class=\"literal\">True</span>, start_new_session=<span class=\"literal\">False</span>, pass_fds=())</span><br></pre></td></tr></table></figure>\n<ul>\n<li>args： 要执行的shell命令，可以是字符串，也可以是命令各个参数组成的序列。当该参数的值是一个字符串时，该命令的解释过程是与平台相关的，因此通常建议将args参数作为一个序列传递。</li>\n<li>bufsize： 指定缓存策略，0表示不缓冲，1表示行缓冲，其他大于1的数字表示缓冲区大小，负数 表示使用系统默认缓冲策略。</li>\n<li>stdin, stdout, stderr： 分别表示程序标准输入、输出、错误句柄。</li>\n<li>preexec_fn： 用于指定一个将在子进程运行之前被调用的可执行对象，只在Unix平台下有效。</li>\n<li>close_fds： 如果该参数的值为True，则除了0,1和2之外的所有文件描述符都将会在子进程执行之前被关闭。</li>\n<li>shell： 该参数用于标识是否使用shell作为要执行的程序，如果shell值为True，则建议将args参数作为一个字符串传递而不要作为一个序列传递。</li>\n<li>cwd： 如果该参数值不是None，则该函数将会在执行这个子进程之前改变当前工作目录。</li>\n<li>env： 用于指定子进程的环境变量，如果env=None，那么子进程的环境变量将从父进程中继承。如果env!=None，它的值必须是一个映射对象。</li>\n<li>universal_newlines： 如果该参数值为True，则该文件对象的stdin，stdout和stderr将会作为文本流被打开，否则他们将会被作为二进制流被打开。</li>\n<li>startupinfo和creationflags： 这两个参数只在Windows下有效，它们将被传递给底层的CreateProcess()函数，用于设置子进程的一些属性，如主窗口的外观，进程优先级等。</li>\n</ul>\n<ol>\n<li>subprocess.Popen类的实例可调用的方法</li>\n</ol>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>方法</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Popen.poll()</td>\n<td>用于检查子进程（命令）是否已经执行结束，没结束返回None，结束后返回状态码。</td>\n</tr>\n<tr>\n<td>Popen.wait(timeout=None)</td>\n<td>等待子进程结束，并返回状态码；如果在timeout指定的秒数之后进程还没有结束，将会抛出一个TimeoutExpired异常。</td>\n</tr>\n<tr>\n<td>Popen.communicate(imput=None, timeout=None)</td>\n<td>该方法可用于来与程序进行交互，比如发送数据到stdin，从stdout和stderr读取数据，直到达到文件末尾。</td>\n</tr>\n<tr>\n<td>Popen.send_signal(signal)</td>\n<td>发送指定的信号给这个子进程</td>\n</tr>\n<tr>\n<td>Popen.terminate()</td>\n<td>停止这个子进程</td>\n</tr>\n<tr>\n<td>Popen.kill</td>\n<td>杀死该子进程</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h4 id=\"进程间通信\"><a href=\"#进程间通信\" class=\"headerlink\" title=\"进程间通信\"></a>进程间通信</h4><p><code>Process</code>之间肯定是需要通信的，操作系统提供了很多的机制来实现进程间的通信。Python的<code>nultiprocessing</code>模块包装了底层的机制，提供了<code>Queue</code>，<code>Pipes</code>等多种方式来交换数据。</p>\n<p>我们以<code>Queue</code>为例，在父进程中创建两个子进程，一个往<code>Queue</code>里写入数据，一个从<code>Queue</code>里读取数据：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> multiprocessing <span class=\"keyword\">import</span> Process, Queue</span><br><span class=\"line\"><span class=\"keyword\">import</span> os, time, rendom</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#写数据进程执行的代码</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">write</span>(<span class=\"params\">q</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Pricess to write: %s&#x27;</span> % os.getpid())</span><br><span class=\"line\">    <span class=\"keyword\">for</span> value <span class=\"keyword\">in</span> [<span class=\"string\">&#x27;A&#x27;</span>, <span class=\"string\">&#x27;B&#x27;</span>, <span class=\"string\">&#x27;C&#x27;</span>]:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Put %s to queue...&#x27;</span> % value)</span><br><span class=\"line\">        q.put(value)</span><br><span class=\"line\">        time.sleep(random.random())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#读数据进程执行的代码</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">read</span>(<span class=\"params\">q</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Process to readL %s&#x27;</span> % os.getpid)</span><br><span class=\"line\">\twrite <span class=\"literal\">True</span>:</span><br><span class=\"line\">        value = q.get(<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Get %s from queue.&#x27;</span> % value)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ = <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    <span class=\"comment\">#父进程创建的Queue，并传给各个子进程</span></span><br><span class=\"line\">    q = Queue()</span><br><span class=\"line\">    pw = Process(targe=write, args=(q,))</span><br><span class=\"line\">    pr = Process(target=read, args=(q,))</span><br><span class=\"line\">    <span class=\"comment\">#启动子进程pr,读取：</span></span><br><span class=\"line\">    pr.start()</span><br><span class=\"line\">    <span class=\"comment\">#等待pw结束</span></span><br><span class=\"line\">    pw.join()</span><br><span class=\"line\">    <span class=\"comment\">#pr进程里是四循环，无法等待其结束，只能强行终止：</span></span><br><span class=\"line\">    pr.terminate()</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"多线程\"><a href=\"#多线程\" class=\"headerlink\" title=\"多线程\"></a>多线程</h3><p>进程是由若干个线程组成的，一个进程至少有一个线程。</p>\n<p>由于线程的操作系统直接支持的执行单元，因此，高级语言通常都内置多线程的支持，Python也不例外，并且，Python的线程是真正的Posix Thread，而不是模拟出来的线程。</p>\n<p>Python的标准库提供了两个模块：<code>_thread</code>和<code>threading</code>，<code>_thread</code>是低级模块，<code>threading</code>是高级模块，对<code>_thread</code>进行了封装。绝大多数情况下，我们只需要使用<code>_threading</code>这个高级模块。</p>\n<p>启动一个线程就是把一个函数传入并创建<code>Thread</code>实例，然后调用<code>start()</code>开始执行：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> time, threading</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#新线程执行的代码：</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">loop</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;thread %s is runing...&#x27;</span> % threading.current_thread().name)</span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> n &lt; <span class=\"number\">5</span>:</span><br><span class=\"line\">        n = n+ <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;thread %s &gt;&gt;&gt; %s&#x27;</span> % (threading.current_thread ().name))</span><br><span class=\"line\">        time.sleep(<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;thread %s ended.&#x27;</span> % threading.current_thread().name)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;thread %s is running...&#x27;</span> % threading.current_thread().name)</span><br><span class=\"line\">t = threading.Thread(target=loop, name = <span class=\"string\">&#x27;LoopThread&#x27;</span>)</span><br><span class=\"line\">t.start()</span><br><span class=\"line\">t.join()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;thread %s ended.&#x27;</span> % threading.current_thread().name)</span><br></pre></td></tr></table></figure>\n<p>由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的<code>threading</code>模块有个<code>current_thread()</code>函数，它永远返回当前线程的实例。主线程实例的名字叫<code>MainThread</code>，子线程的名字在创建时指定，我们用<code>LoopThread</code>命名子线程。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为<code>Thread-1</code>，<code>Thread-2</code>……</p>\n<h4 id=\"Lock\"><a href=\"#Lock\" class=\"headerlink\" title=\"Lock\"></a>Lock</h4><p>多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，所有的变量都由所有的线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时修改一个变量，把内容给该乱了。</p>\n<p>所以我们希望创建一把锁，当某个程序开始执行一个线程时，我们说，该线程获得的锁，因此其他线程不能同时执行该线程，只能等待，，直到锁被释放后，获得该锁以后才能改。由于锁只有一个，无论多少个线程，同一时刻最多只有一个线程持有该锁，所以不会造成修改冲突。创建一个锁就是通过<code>threading.Lock()</code>来实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">balance = <span class=\"number\">0</span></span><br><span class=\"line\">lock = threading.Lock()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">run_thread</span>(<span class=\"params\">n</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10000</span>)</span><br><span class=\"line\">    <span class=\"comment\">#先要获取锁：</span></span><br><span class=\"line\">    lock.acquire()</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        change_it(n)</span><br><span class=\"line\">    <span class=\"keyword\">finally</span>:</span><br><span class=\"line\">        <span class=\"comment\">#改完了一定要释放锁</span></span><br><span class=\"line\">        lock.release()</span><br></pre></td></tr></table></figure>\n<p>当多个线程同时执行<code>lock.acquire()</code>时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。</p>\n<p>获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用<code>try...finally</code>来确保锁一定会被释放。</p>\n<p>锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。</p>\n<h4 id=\"多核CPU\"><a href=\"#多核CPU\" class=\"headerlink\" title=\"多核CPU\"></a>多核CPU</h4><p>如果你不幸拥有一个多核CPU，你肯定在想，多核应该可以同时执行多个线程。</p>\n<p>如果写一个死循环的话，会出现什么情况呢？</p>\n<p>打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。</p>\n<p>我们可以监控到一个死循环线程会100%占用一个CPU。</p>\n<p>如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。</p>\n<p>要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。</p>\n<p>试试用Python写个死循环：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> threading, multiprocessing</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">loop</span>():</span><br><span class=\"line\">    x = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">        x = x ^ <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(multiprocessing.cpu_count()):</span><br><span class=\"line\">    t = threading.Thread(target=loop)</span><br><span class=\"line\">    t.start()</span><br></pre></td></tr></table></figure>\n<p>启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。</p>\n<p>但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？</p>\n<p>因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。</p>\n<p>GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。</p>\n<p>所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。</p>\n<p>不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"Python\"><a href=\"#Python\" class=\"headerlink\" title=\"Python\"></a>Python</h1><p>Python的学习记录。</p>","more":"<h2 id=\"Python基础\"><a href=\"#Python基础\" class=\"headerlink\" title=\"Python基础\"></a>Python基础</h2><h3 id=\"字符编码\"><a href=\"#字符编码\" class=\"headerlink\" title=\"字符编码\"></a>字符编码</h3><p>计算机只能处理数字，如果要处理文本，就必须把文本转化成数字才能处理。最早的计算机在设计时采用8个比特(bit)作为一个字节(byte)，所以，一个字节能表示的最大的整数就是255（二进制11111111=十进制255）。如果要表示更大的整数就要采用更多的字节。</p>\n<p>为了不与ASCII编码冲突，中国制定了<code>GB2312</code>编码，用来把中文编码进去。日本把日文编到<code>shift_JIS</code>中等等。各国有各国的标准，就会发生冲突，结果是，在很多语言混合的文本中，显示出来会有乱码。</p>\n<p>因此，Unicode字符集应运而生。Unicode把所有语言都统一到一套编码里，这样就不会出现乱码问题了。</p>\n<p>ASCII编码是一个字节，而Unicode编码是两个字节。</p>\n<p>字母<code>A</code>用ASCII编码是十进制的<code>65</code>，二进制的<code>01000001</code>；</p>\n<p>字符<code>0</code>用ASCII编码是十进制的<code>48</code>，二进制的<code>00110000</code>，注意字符<code>&#39;0&#39;</code>和整数<code>0</code>是不同的；</p>\n<p>汉字<code>中</code>已经超出了ASCII编码的范围，用Unicode编码是十进制的<code>20013</code>，二进制的<code>01001110 00101101</code>。</p>\n<p>对于单个字符的编码，Python提供了<code>ord()</code>函数获取字符的整数表示，<code>chr()</code>函数把编码转化为对应字符。</p>\n<p>如果知道字符的整数编码，还可以用十六进制这么写str：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"string\">&#x27;\\u4e2d\\u6587&#x27;</span></span><br><span class=\"line\"><span class=\"string\">&#x27;中文&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>这两种写法完全等价。</p>\n<p>由于Python的字符串是<code>str</code>，所以在内存中以Unicode表示，一个字符对应若干个字节。如果要在网络上传输，或者要保存到磁盘上，就需要把<code>str</code>变为以字节为单位的<code>bytes</code>。</p>\n<p>Python对<code>bytes</code>类型的数据用带<code>b</code>前缀的单引号或双引号表示：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = <span class=\"string\">b&#x27;ABC&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>纯英文的<code>str</code>可以用<code>ASCII</code>编码为<code>bytes</code>，内容是一样的，含有中文的<code>str</code>可以用<code>UTF-8</code>编码为<code>bytes</code>。含有中文的<code>str</code>无法用<code>ASCII</code>编码，Python会报错。</p>\n<p>在<code>bytes</code>中，无法显示为ASCII字符的字节，用<code>\\x##</code>显示。</p>\n<p>反过来，如果我们从网络或磁盘上读取了字节流，那么读到的数据就是<code>bytes</code>要把<code>bytes</code>变为<code>str</code>，就需要用<code>decode()</code>方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"string\">b&#x27;ABC&#x27;</span>.decode(<span class=\"string\">&#x27;ascii&#x27;</span>)</span><br><span class=\"line\"><span class=\"string\">&#x27;ABC&#x27;</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"string\">b&#x27;\\xe4\\xb8\\xad\\xe6\\x96\\x87&#x27;</span>,decode(<span class=\"string\">&#x27;utf-8&#x27;</span>)</span><br><span class=\"line\"><span class=\"string\">&#x27;中文&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>如果<code>bytes</code>中包含无法解码的字节，<code>decode()</code>方法会报错。</p>\n<p>如果<code>bytes</code>中只有一小部分无效的字节，可以传入<code>errors=&#39;ignore&#39;</code>忽略错误的字节：</p>\n<p>由于Python源码是一个文本文件，所以，当你的源代码中包含了中文的时候，在保存源代码时，就需要务必指定保存UTF-8编码。当Python解释器读取源码时，为了让它按<code>UTF-8</code>编码读取，我们通常在文件开头写上这两行：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#!/usr/bin/env python3</span></span><br><span class=\"line\"><span class=\"comment\"># -*- coding: utf-8 -*-</span></span><br></pre></td></tr></table></figure>\n<p>第一行注释是为了告诉Linux/OS X 系统，这是一个Python可执行程序，Windows系统会忽略这个注释；</p>\n<p>第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，你在源代码中写入的中文输入可能会出现乱码。</p>\n<p>在Python中，采用的格式化字符串的方式是与C语言一致的，用<code>%</code>实现。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">占位符</th>\n<th style=\"text-align:left\">替换内容</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">%d</td>\n<td style=\"text-align:left\">整数</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">%f</td>\n<td style=\"text-align:left\">浮点数</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">%s</td>\n<td style=\"text-align:left\">字符串</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">%x</td>\n<td style=\"text-align:left\">十六进制整数</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>其中，格式化整数和浮点数还可以制定是否补0和整数与小数的为数：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;#2d-%02d&#x27;</span> % (<span class=\"number\">3</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;%.2f&#x27;</span> % <span class=\"number\">3.1415926</span>)</span><br></pre></td></tr></table></figure>\n<p>如果你不太确定应该用什么，<code>%s</code>永远起作用，它会把任何数据类型转换为字符串：</p>\n<p>有些时候，字符串里面的<code>%</code>是一个普通字符怎么办？这个时候就需要转义，用<code>%%</code>来表示一个<code>%</code>：</p>\n<h4 id=\"format\"><a href=\"#format\" class=\"headerlink\" title=\"format()\"></a>format()</h4><p>另一种格式化字符串的方法是使用字符串的<code>format()</code>方法，它会用传入的参数依次替换占为符<code>&#123;0&#125;</code>、<code>&#123;1&#125;</code>、……，不过这种方式写起来比%要麻烦的多：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"string\">&#x27;Hello, &#123;0&#125;, 成绩提升了 &#123;1:.1f&#125;%&#x27;</span>.<span class=\"built_in\">format</span>(<span class=\"string\">&#x27;小明&#x27;</span>, <span class=\"number\">17.125</span>)</span><br><span class=\"line\"><span class=\"string\">&#x27;Hello, 小明, 成绩提升了 17.1%&#x27;</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"f-string\"><a href=\"#f-string\" class=\"headerlink\" title=\"f-string\"></a><code>f-string</code></h4><p>最后一种格式化字符串的方法是使用以<code>f</code>开头的字符串，称之为<code>f-string</code>，它和普通字符串不同之处在于，字符串如果包含<code>&#123;xxx&#125;</code>，就会以对应变量替换：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>r = <span class=\"number\">2.5</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>s = <span class=\"number\">3.4</span> * r ** <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;The area of a child with radius <span class=\"subst\">&#123;r&#125;</span> is <span class=\"subst\">&#123;s:<span class=\"number\">.2</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>在上述代码中，<code>&#123;r&#125;</code>被变量<code>r</code>替换，<code>&#123;s:.2f&#125;</code>被变量<code>&#123;s&#125;</code>的值替换，并且<code>:</code>后面的<code>.2f</code>指定了格式化参数，因此，<code>&#123;s:.2f&#125;</code>的替换结果是<code>19.62</code>。</p>\n<h2 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h2><h3 id=\"定义函数\"><a href=\"#定义函数\" class=\"headerlink\" title=\"定义函数\"></a>定义函数</h3><p>在Python中，定义一个函数要使用<code>def</code>语句，依次写出函数名，括号，括号中的参数和冒号<code>:</code>，然后，在缩进块中编写函数体，函数的返回值用<code>return</code>语句返回。</p>\n<p>我们自定义一个求绝对值的<code>my_abs</code>函数为例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">my_abs</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> x &gt;= <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -x</span><br></pre></td></tr></table></figure>\n<p>请注意，函数体内部的语句在执行时，一旦执行到<code>return</code>时，函数就执行完毕，并将结果返回。因此，函数内部通过条件判断和循环可以实现非常复杂的逻辑。</p>\n<p>如果没有<code>return</code>语句，函数执行完毕后也会返回结果，只是结果为<code>None</code>。<code>return None</code>可以简写成<code>return</code>。</p>\n<p>在Python交互环境中定义函数时，注意Python会出现<code>...</code>的提示。函数定义结束后需要按两次回车重新回到<code>&gt;&gt;&gt;</code>提示符下。</p>\n<p>如果你已经把<code>my_abs()</code>的函数定义保存为<code>abstest.py</code>文件了，那么，可以在该文件的当前目录下启动Python解释器，用<code>from abstest import my_abs</code>来导入<code>my_abs()</code>函数，注意<code>abstest</code>是文件名（不含<code>.py</code>扩展名）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">from</span> abstest <span class=\"keyword\">import</span> my_abs</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>my_abs(-<span class=\"number\">9</span>)</span><br><span class=\"line\"><span class=\"number\">9</span></span><br><span class=\"line\">&gt;&gt;&gt;_</span><br></pre></td></tr></table></figure>\n<h4 id=\"空函数\"><a href=\"#空函数\" class=\"headerlink\" title=\"空函数\"></a>空函数</h4><p>如果想定义一个什么事也不做的空函数，可以用<code>pass</code>语句：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">nop</span>():</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n<p>一个空函数，缺少了<code>pass</code>，代码运行就会有语法错误。</p>\n<h4 id=\"参数检查\"><a href=\"#参数检查\" class=\"headerlink\" title=\"参数检查\"></a>参数检查</h4><p>调用函数时，如果参数个数不对，Python解释器会自动检查出来，并抛出<code>TypeError</code>。</p>\n<p>但是如果参数类型不对，Python解释器就无法帮我们检查。试试<code>my_abs</code>和内置函数<code>`abs</code>的差别：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>my_abs(<span class=\"string\">&#x27;A&#x27;</span>)</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">    File <span class=\"string\">&quot;&lt;stdin&gt;&quot;</span>,line <span class=\"number\">1</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    File <span class=\"string\">&quot;&lt;stdin&gt;&quot;</span>,line <span class=\"number\">2</span>, <span class=\"keyword\">in</span> my_abs</span><br><span class=\"line\">TypeError:unorderable types: <span class=\"built_in\">str</span>() &gt;= <span class=\"built_in\">int</span>()</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">abs</span>(<span class=\"string\">&#x27;A&#x27;</span>)</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">&quot;&lt;stdin&gt;&quot;</span>, line <span class=\"number\">1</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">TypeError: bad operand <span class=\"built_in\">type</span> <span class=\"keyword\">for</span> <span class=\"built_in\">abs</span>(): <span class=\"string\">&#x27;str&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>当传入了不恰当的参数时，内置函数<code>abs</code>会检查出错误，而我们定义的<code>my_abs</code>没有参数检查，会导致<code>if</code>语句出错，出错信息和<code>abs</code>不一样。所以这个函数定义不够完善。</p>\n<p>让我们来修改一下<code>my_abs</code>的定义，对参数类型作检查，只允许整数和浮点数类型的参数。数据类型检查可以用内置函数<code>isinstance()</code>实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">my_abs</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(x,(<span class=\"built_in\">int</span>,<span class=\"built_in\">float</span>)):</span><br><span class=\"line\">        <span class=\"keyword\">raise</span> TypeError(<span class=\"string\">&#x27;bad operand type&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> x &gt;= <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -x</span><br></pre></td></tr></table></figure>\n<h2 id=\"函数式编程\"><a href=\"#函数式编程\" class=\"headerlink\" title=\"函数式编程\"></a>函数式编程</h2><h3 id=\"高阶函数\"><a href=\"#高阶函数\" class=\"headerlink\" title=\"高阶函数\"></a>高阶函数</h3><h4 id=\"map-reduce\"><a href=\"#map-reduce\" class=\"headerlink\" title=\"map/reduce\"></a>map/reduce</h4><p>什么是高阶函数？我们以是实际代码为例子，一步步升入概念。</p>\n<p><strong>变量可以指向函数</strong></p>\n<p>以python内置的求绝对值函数<code>abs()</code>为例，调用该函数用以下代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">abs</span>(-<span class=\"number\">10</span>)</span><br><span class=\"line\"><span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n<p>但是如果只写<code>abs</code>呢？</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">abs</span></span><br><span class=\"line\">&lt;built-<span class=\"keyword\">in</span> function <span class=\"built_in\">abs</span>&gt;</span><br></pre></td></tr></table></figure>\n<p>可见<code>abs(function)</code>是函数调用，而abs是函数本身。</p>\n<p>要获得函数调用的结果我们可以把函数赋值给变量：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;x = <span class=\"built_in\">abs</span>(-<span class=\"number\">10</span>)</span><br><span class=\"line\">&gt;&gt;&gt;x</span><br><span class=\"line\"><span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n<p>结论：函数本身因为可以赋值给变量，即：变量可以指向函数。</p>\n<p>如果一个变量指向了一个函数，那么可否通过改变量来调用这个函数？用代码验证以下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;f = <span class=\"built_in\">abs</span></span><br><span class=\"line\">&gt;&gt;&gt;f(-<span class=\"number\">10</span>)</span><br><span class=\"line\"><span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n<p>说明变量<code>f</code>现在已经指向了<code>abs</code>函数本身。直接调用<code>abs()</code>函数和调用<code>f</code>完全相同。</p>\n<p><strong>函数名也是变量</strong></p>\n<p>那么函数名是什么呢？函数名其实就是指向函数的变量！对于<code>abs</code>这个函数完全可以把<code>abs</code>看成变量，它指向一个可以计算绝对值的函数！如果把<code>abs</code>指向其他对象会有什么情况发生？</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">abs</span> = <span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">abs</span>(-<span class=\"number\">10</span>)</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">&quot;&lt;stdin&gt;&quot;</span>, line <span class=\"number\">1</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">TypeError: <span class=\"string\">&#x27;int&#x27;</span> <span class=\"built_in\">object</span> <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"built_in\">callable</span></span><br></pre></td></tr></table></figure>\n<p>把<code>abs</code>指向<code>10</code>后，就无法通过<code>abs(-10)</code>调用该函数了！因为<code>abs</code>这个变量已经不指向求绝对值函数而是指向一个整数<code>10</code>！</p>\n<p>当然实际代码绝对不能这么写，这里是为了说明函数名也是变量。要恢复<code>abs</code>函数，请重启Python交互环境。</p>\n<p>注：由于<code>abs</code>函数实际上是定义在<code>import builtins</code>模块中的，所以要让修改<code>abs</code>变量的指向在其它模块也生效，要用<code>import builtins; builtins.abs = 10</code>。</p>\n<h3 id=\"偏函数\"><a href=\"#偏函数\" class=\"headerlink\" title=\"偏函数\"></a>偏函数</h3><p>python的<code>functools</code>提供了偏函数功能（Partial function）。</p>\n<p>假设要转换大量的二进制字符串，每次都传入<code>int(x,base = 2)</code>非常麻烦，于是，我们想到可以定义一个函数<code>int2()</code>，默认把<code>base = 2</code>传进去：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">int2</span>(<span class=\"params\">x,base = <span class=\"number\">2</span></span>):</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"built_in\">int</span>(x,base)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#这样我们进行二进制转换就可以了</span></span><br><span class=\"line\">int2(<span class=\"number\">1000000</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Partial function</span></span><br><span class=\"line\">int2 = functools.partial(<span class=\"built_in\">int</span>,base = <span class=\"number\">2</span>)</span><br><span class=\"line\">int2(<span class=\"string\">&#x27;123456&#x27;</span>)</span><br><span class=\"line\"><span class=\"number\">64</span></span><br></pre></td></tr></table></figure>\n<p>Partial function作用就是，帮助我们把一个函数的某些参数固定住（也就是设置默认值），返回一个新的函数。</p>\n<p>创建偏函数时可以接受函数对象、<em>args、*</em>kw这3个参数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int2 = functools.partial(<span class=\"built_in\">int</span>,base = <span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>\n<p>实际上固定了<code>int()</code>函数的关键字<code>base</code>，也就是：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int2(<span class=\"string\">&#x27;10010&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>相当于：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kw = &#123;<span class=\"string\">&#x27;base&#x27;</span> : <span class=\"number\">2</span>&#125;</span><br><span class=\"line\"><span class=\"built_in\">int</span>(<span class=\"string\">&#x27;10010&#x27;</span> , **kw)</span><br></pre></td></tr></table></figure>\n<p>当传入：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">max2 = functools.partial(<span class=\"built_in\">max</span>,<span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure>\n<p>实际上会把10作为*args的一部分自动加到左边：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">max2(<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>)</span><br></pre></td></tr></table></figure>\n<p>相当于：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">args = (<span class=\"number\">10</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>)</span><br><span class=\"line\"><span class=\"built_in\">max</span>(*args)</span><br></pre></td></tr></table></figure>\n<p>结果为10。</p>\n<hr>\n<h2 id=\"模块\"><a href=\"#模块\" class=\"headerlink\" title=\"模块\"></a>模块</h2><p>为了编写可维护代码，我们把很多函数分组，分别放到不同的文件里，这样，每个文件包含的代码就相对较少。在python中一个.py文件就可以被称为一个模块（Module）。</p>\n<p>使用模块有什么好处？</p>\n<p>最大的好处是提高了代码的可维护性。其次，编写代码不必从0开始。</p>\n<p>其次，使用模块还可以避免函数名和变量名冲突。相同名字的函数名和变量名可以存在不同模块中。</p>\n<p>如果模块名冲突怎么办？Python又引入了按目录来组织模块的方法，称为包（Package）。</p>\n<p>每个包目录下都有一个<code>__init__.py</code>文件，这个文件是必须存在的，否则，python就会把这个文件当成普通目录而不是包。<code>__init__.py</code>可以是空文件，也可以有Python代码，因为<code>__init__.py</code>本身就是一个模块，而它的模块名就是<code>mycompany</code>。</p>\n<h3 id=\"使用模块\"><a href=\"#使用模块\" class=\"headerlink\" title=\"使用模块\"></a>使用模块</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#!/usr/bin/env python3</span></span><br><span class=\"line\"><span class=\"comment\"># -*- coding: utf-8 -*-</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27; a test module &#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">__author__ = <span class=\"string\">&#x27;Michael Liao&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">test</span>():</span><br><span class=\"line\">    args = sys.argv</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(args)==<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Hello, world!&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> <span class=\"built_in\">len</span>(args)==<span class=\"number\">2</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Hello, %s!&#x27;</span> % args[<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Too many arguments!&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    test()</span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">当我们在命令行运行hello模块文件时，Python解释器把一个特殊变量__name__置为__main__，而如果在其他地方导入该hello模块时，if判断将失败，因此，这种if测试可以让一个模块通过命令行运行时执行一些额外的代码，最常见的就是运行测试。</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>代码的第一，二行是标准注释，第一行注释可以让这个<code>hellow.py</code>文件直接在Unix/Linux/Mac上运行，第二行注释表示了.py文件本身使用了UTF-8编码；</p>\n<p>第四行是一个字符串，表示模块的文档注释，任何模块的第一行字符串都被视为模块文档注释；</p>\n<p>第六行<code>__author__</code>变量把作者写进去。</p>\n<p>后面开始就是真正的代码部分。</p>\n<p>使用<code>sys</code>模块的第一步就是导入该模块：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br></pre></td></tr></table></figure>\n<p>导入了sys模块后，我们就有了变量sys指向该模块，利用<code>sys</code>这个变量，就可以访问<code>sys</code>模块所有的功能。</p>\n<p>sys有一个argv变量，用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是<code>.py</code>文件的名称，例如：</p>\n<p>运行<code>`python3 hello.py</code>获得的<code>sys.argv</code>就是<code>[&#39;hellow.py&#39;]</code></p>\n<p>运行<code>python3 hello.py Michael</code>获得的<code>sys.argv</code>就是<code>[&#39;hello.py&#39;, &#39;Michael&#39;]</code>。</p>\n<p>如果启动Python交互环境，再导入<code>hello</code>模块：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ python3</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">import</span> hello</span><br><span class=\"line\">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>\n<p>导入时，没有打印<code>hello world!</code>因为没有执行<code>test()</code>函数。</p>\n<p>调用<code>hello.test()</code>时，才能打印出<code>hello world!</code>:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;hello.test()</span><br><span class=\"line\">Hello,world!</span><br></pre></td></tr></table></figure>\n<h4 id=\"作用域\"><a href=\"#作用域\" class=\"headerlink\" title=\"作用域\"></a>作用域</h4><p>在一个模块中，我们可能会定义很多函数和变量，但有的函数我们希望给别人使用，有的仅仅在模块内部使用。在Python中我们是通过<code>_</code>前缀来实现的。</p>\n<p>正常函数和变量名是公开的（public），可以被直接引用。</p>\n<p>类似<code>__xx__</code>是特殊变量，可以被直接引用，但有特殊用途，比如上面的<code>__author__</code>，<code>__name__</code>就是特殊变量，<code>hello</code>模块定义的文档注释也可以用特殊变量<code>__doc__</code>访问，我们自己的变量一般不要用这种变量名。</p>\n<p>类似<code>_xxx</code>和<code>__xxx</code>这样的函数或变量就是非公开的（private），不应该被直接引用，比如<code>_abc</code>，<code>__abc</code>等；</p>\n<p>之所以我们说，private函数和变量“不应该”被直接引用，而不是“不能”被直接引用，是因为Python并没有一种方法可以完全限制访问private函数或变量，但是，从编程习惯上不应该引用private函数或变量。</p>\n<p>private函数或变量不应该被别人引用，那它们有什么用呢？请看例子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">_private_1</span>(<span class=\"params\">name</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&#x27;Hello, %s&#x27;</span> % name</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">_private_2</span>(<span class=\"params\">name</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&#x27;Hi, %s&#x27;</span> % name</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">greeting</span>(<span class=\"params\">name</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(name) &gt; <span class=\"number\">3</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> _private_1(name)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> _private_2(name)</span><br></pre></td></tr></table></figure>\n<p>我们在模块里公开<code>greeting()</code>函数，而把内部逻辑用private函数隐藏起来了，这样，调用<code>greeting()</code>函数不用关心内部的private函数细节，这也是一种非常有用的代码封装和抽象的方法，即：</p>\n<p>外部不需要引用的函数全部定义成private，只有外部需要引用的函数才定义为public。</p>\n<h2 id=\"面向对象编程（OOP）\"><a href=\"#面向对象编程（OOP）\" class=\"headerlink\" title=\"面向对象编程（OOP）\"></a>面向对象编程（OOP）</h2><p>OOP把对象作为基本单元，一个对象包含了数据和操作数据的函数。</p>\n<p>OOP把计算机程序视为一系列命令的集合，而每个对象都可以接受其他对象发过来的消息，并处理这些消息，计算机程序的执行就是一系列消息在对象之间传递。</p>\n<p>在Python中，所有的数据类型都可以视为对象，当然也可以自定义对象。自定义的对象数据类型就是面向对象中的类（Class）的概念。</p>\n<h3 id=\"类和实例\"><a href=\"#类和实例\" class=\"headerlink\" title=\"类和实例\"></a>类和实例</h3><p>OOP最重要概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，实例是根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但是各自的数据可能不同。</p>\n<p>仍以Student类为例，在Python中，定义类是通过<code>class</code>关键字：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n<p><code>calss</code>后面紧接着是类名，即<code>Student</code>类创建出<code>Student</code>实例，创建实例是通过类名+()实现的：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart = Student()</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart</span><br><span class=\"line\">&lt;__main__.Student <span class=\"built_in\">object</span> at <span class=\"number\">0x10a67a590</span>&gt;</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>Student</span><br><span class=\"line\">&lt;<span class=\"keyword\">class</span> <span class=\"string\">&#x27;__main__.Student&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>\n<p>可以看到，变量<code>bart</code>指向的就是一个<code>Student</code>的实例，后面的<code>0x10a67a590</code>是内存地址，每个object的地址都不一样，而<code>Student</code>本身则是一个类。</p>\n<p>可以自由地给一个实例变量绑定属性，比如，给实例<code>bart</code>绑定一个<code>name</code>属性：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart.name = <span class=\"string\">&#x27;Bart Simpson&#x27;</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart.name</span><br><span class=\"line\"><span class=\"string\">&#x27;Bart Simpson&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>由于类可以起到模板作用，因此可以在创建实例的时候，把一些我们任务必须绑定的属性强制填进去。通过定义一个特殊的<code>__init__</code>方法，创建实例的时候就把<code>name</code>，<code>score</code>等属性绑定上去：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,name,score</span>):</span><br><span class=\"line\">        self.name = name</span><br><span class=\"line\">        self.score = score</span><br></pre></td></tr></table></figure>\n<p>注意，<code>__init__</code>方法的第一个参数永远是<code>self</code>，表示创建的实例本身，因此，在<code>__init__</code>方法内部，就可以把各种属性绑定到<code>self</code>,因为<code>self</code>就指向创建的实例本身。</p>\n<p>有了<code>__init__</code>方法，在创建实例的时候，就不能传入空的参数了，必须传入与<code>__init__</code>方法相匹配的参数，但是self不需要传，Python解释器会自己把实例变量传进去。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;bart = Student(<span class=\"string\">&#x27;Bart Simpson&#x27;</span>,<span class=\"number\">59</span>)</span><br><span class=\"line\">&gt;&gt;&gt;bart.name</span><br><span class=\"line\"><span class=\"string\">&#x27;Bart Simpson&#x27;</span></span><br><span class=\"line\">&gt;&gt;&gt;bart.score</span><br><span class=\"line\"><span class=\"number\">59</span></span><br></pre></td></tr></table></figure>\n<p>和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量<code>self</code>，并且，调用时不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所有，你仍然可以用默认参数、可变参数、关键字参数、命名关键字参数。</p>\n<h4 id=\"数据封装\"><a href=\"#数据封装\" class=\"headerlink\" title=\"数据封装\"></a>数据封装</h4><p>OOP的一个重要特点就是数据封装。在上面的<code>Student</code>类中，每个实例就拥有各自的<code>name</code>和<code>score</code>这些数据。我们可以通过函数来访问这些数据，比如打印一个学生的成绩。但是<code>Student</code>实例本身就拥有这些数据，要访问这些数据，就没有必要从外面的函数去访问，可以直接在<code>Student</code>类的内部定义访问数据的函数，这样，就把“数据”给封装起来了。这些封装数据的函数是和<code>Student</code>类本身是关联起来的，我们称之为类的方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,name,score</span>):</span><br><span class=\"line\">        self.name = name</span><br><span class=\"line\">        self.score = score</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">print_score</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;%s : %s&#x27;</span> % (self.name, self.score))</span><br></pre></td></tr></table></figure>\n<p>要定义一个方法，除了第一个参数是<code>self</code>以外，其他和普通函数一样。要调用另一个方法，只需要在实例变量上直接调用，除了<code>self</code>不用传递，其他参数正常传入：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;bart.print_score()</span><br><span class=\"line\">Bart Simpson: <span class=\"number\">59</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"访问限制\"><a href=\"#访问限制\" class=\"headerlink\" title=\"访问限制\"></a>访问限制</h3><p>在class内部，可以有属性和方法，而外部代码可以通过直接调用实例变量的方法来操作数据，这样，就隐藏了内部的复杂逻辑。</p>\n<p>但是从<code>Student</code>类的定义来看，外部代码还是可以自由的修改一个实例的<code>name</code>、<code>scorre</code>属性。</p>\n<p>如果要让内部属性不被外部访问，可以把属性名称前加两个下划线<code>__</code>，在python中，实例的变量名如果以<code>__</code>开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，所以，我们把<code>Student</code>类改一改：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,name,score</span>):</span><br><span class=\"line\">        self.__name = name</span><br><span class=\"line\">        self.__score = score</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">print_score</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        prnint(<span class=\"string\">&#x27;%s %s&#x27;</span> % (self.__name, self.__score))</span><br></pre></td></tr></table></figure>\n<p>改完后，对外部代码来说没什么变动，但以及无法从外部访问<code>实例变量.__name</code>和<code>实例变量.__score</code>了</p>\n<p>但是如果外部代码要获取score怎么办？可以给<code>Student</code>类增加<code>set_score</code>方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">set_score</span>(<span class=\"params\">self,score</span>):</span><br><span class=\"line\">        self.__score = score</span><br></pre></td></tr></table></figure>\n<p>你也许会问，原先那种直接通过<code>bart.score = 99</code>也可以修改啊，为什么要定义一个方法大费周折？因为在方法中，可以对参数做检查，避免传入无效的参数：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">set_score</span>(<span class=\"params\">self, score</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"number\">0</span> &lt;= score &lt;= <span class=\"number\">100</span>:</span><br><span class=\"line\">            self.__score = score</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">raise</span> ValueError(<span class=\"string\">&#x27;bad score&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>而不能直接访问<code>__name</code>是因为Python解释器对外把<code>__name</code>变量改成了<code>_Student__name</code>，所以，仍然可以通过<code>_Student__name</code>来访问<code>__name</code>变量：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart._Student__name</span><br><span class=\"line\"><span class=\"string\">&#x27;Bart Simpson&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>但是强烈建议你不要这么干，因为不同版本的Python解释器可能会把<code>__name</code>改成不同的变量名。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart = Student(<span class=\"string\">&#x27;Bart Simpson&#x27;</span>, <span class=\"number\">59</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart.get_name()</span><br><span class=\"line\"><span class=\"string\">&#x27;Bart Simpson&#x27;</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart.__name = <span class=\"string\">&#x27;New Name&#x27;</span> <span class=\"comment\"># 设置__name变量！</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart.__name</span><br><span class=\"line\"><span class=\"string\">&#x27;New Name&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>表面上看，外部代码“成功”地设置了<code>__name</code>变量，但实际上这个<code>__name</code>变量和class内部的<code>__name</code>变量<em>不是</em>一个变量！内部的<code>__name</code>变量已经被Python解释器自动改成了<code>_Student__name</code>，而外部代码给<code>bart</code>新增了一个<code>__name</code>变量。不信试试：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bart.get_name() <span class=\"comment\"># get_name()内部返回self.__name</span></span><br><span class=\"line\"><span class=\"string\">&#x27;Bart Simpson&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"继承和多态\"><a href=\"#继承和多态\" class=\"headerlink\" title=\"继承和多态\"></a>继承和多态</h3><p>在OOP程序设计中，当我们定义了一个class的时候，可以从某个现有的class继承，新的class称为子类（Subclass），而被继承的class称为基类，父类或超类（Base class，super class）。</p>\n<p>比如我们编写了一个名为<code>Animal</code>的class，有一个<code>run</code>方法可以直接打印：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Animal</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">run</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Animal is running……&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>当我们要编写dog和cat类时，就可以直接从<code>Animal</code>类继承：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Dog</span>(<span class=\"title class_ inherited__\">Animal</span>):</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Cat</span>(<span class=\"title class_ inherited__\">Animal</span>):</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n<p>对于<code>Dog</code>类来说，<code>Animal</code>就是它的父类，对于<code>Animal</code>类来说，<code>Dog</code>就是它的子类。<code>Cat</code>和<code>Dog</code>类似。</p>\n<p>继承有什么好处?最大的好处是，子类获得了父类的全部功能。由于<code>Animal</code>实现了<code>run()</code>方法，因此，<code>Dog</code>和<code>Cat</code>作为它的子类，什么事没干就自动拥有了<code>run()</code>方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dog = Dog()</span><br><span class=\"line\">dog.run</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#cat与dog的处理方式一致</span></span><br><span class=\"line\">cat = Cat()</span><br><span class=\"line\">cat.run</span><br></pre></td></tr></table></figure>\n<p>运行结果如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Animal <span class=\"keyword\">is</span> running...</span><br><span class=\"line\">Animal <span class=\"keyword\">is</span> running...</span><br></pre></td></tr></table></figure>\n<p>当然，也可以对子类增加一些方法，比如Dog类：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Dog</span>(<span class=\"title class_ inherited__\">Animal</span>):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">run</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Dog is running...&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">eat</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Eating meat...&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>继承的第二个好处需要我们对代码做一点改进。你看到了，无论是<code>Dog</code>还是<code>Cat</code>，它们<code>run()</code>的时候，显示的都是<code>Animal is running...</code>，符合逻辑的做法是分别显示<code>Dog is running...</code>和<code>Cat is running...</code>，因此，对<code>Dog</code>和<code>Cat</code>类改进如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Dog</span>(<span class=\"title class_ inherited__\">Animal</span>):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">run</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Dog is running...&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Cat</span>(<span class=\"title class_ inherited__\">Animal</span>):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">run</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Cat is running...&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>再次运行结果如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Dog <span class=\"keyword\">is</span> running...</span><br><span class=\"line\">Cat <span class=\"keyword\">is</span> running...</span><br></pre></td></tr></table></figure>\n<p>当子类和父类都存在相同的<code>run()</code>方法时，我们说，子类的<code>run</code>覆盖了父类的<code>run</code>，在代码运行的时候，总是会调用子类的<code>run</code>。这样我们就获得了继承的另一个好处：多态。</p>\n<p>要理解什么是多态，我们首先要对数据类型再作一点说明。当我们定义一个class的时候，我们实际上就定义了一种数据类型。</p>\n<p>判断一个变量是否是某个类型可以用<code>isinstance()</code>判断：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">isinstance</span>(a, <span class=\"built_in\">list</span>)</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">isinstance</span>(b, Animal)</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">isinstance</span>(c, Dog)</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br></pre></td></tr></table></figure>\n<p>看来<code>a</code>、<code>b</code>、<code>c</code>确实对应着<code>list</code>、<code>Animal</code>、<code>Dog</code>这3种类型。</p>\n<p>但是等等，试试：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">isinstance</span>(c, Animal)</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br></pre></td></tr></table></figure>\n<p>看来<code>c</code>不仅仅是<code>Dog</code>，<code>c</code>还是<code>Animal</code>！</p>\n<p>在继承关系中，如果一个数据类型是某个数据类型的子类，那它的数据类型也可以被看成是父类。但是反过来就不行：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;b = Animal()</span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">isinstance</span>(b,Dog)</span><br><span class=\"line\"><span class=\"literal\">False</span></span><br></pre></td></tr></table></figure>\n<p>理解多态的好处，我们还需要编写一个函数，接受一个<code>Anmial</code>类型的变量：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">run_twice</span>(<span class=\"params\">animal</span>):</span><br><span class=\"line\">    animal.run()</span><br><span class=\"line\">    animal.run()</span><br></pre></td></tr></table></figure>\n<p>当我们传入<code>Animal</code>的实例时，<code>run_twice()</code>就打印出：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>run_twice(Animal())</span><br><span class=\"line\">Animal <span class=\"keyword\">is</span> running...</span><br><span class=\"line\">Animal <span class=\"keyword\">is</span> running...</span><br></pre></td></tr></table></figure>\n<p>传入<code>Dog</code>的实例时，<code>run_twice()</code>打印：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;run_twice(Dog())</span><br><span class=\"line\">Dog <span class=\"keyword\">is</span> runninng……</span><br><span class=\"line\">Dog <span class=\"keyword\">is</span> runninng……</span><br></pre></td></tr></table></figure>\n<p>现在我们再定义一个<code>Tortoise</code>类型，也从<code>Animal</code>派生：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Tortoise</span>(<span class=\"title class_ inherited__\">Animal</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">run</span>(<span class=\"params\">self</span>)</span><br><span class=\"line\">    \t<span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Tortoise is running slowwly…&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>当我们调用<code>run_twice()</code>时，传入<code>Tortoise</code>的实例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>run_twice(Tortoise())</span><br><span class=\"line\">Tortoise <span class=\"keyword\">is</span> running slowly...</span><br><span class=\"line\">Tortoise <span class=\"keyword\">is</span> running slowly...</span><br></pre></td></tr></table></figure>\n<p>你会发现，新增一个<code>Animal</code>的子类，不必对<code>run_twice</code>做任何修改，实际上，任何依赖<code>Animal</code>作为参数的函数或者方法都可以不加修改的正常运行，原因就在于多态。</p>\n<p>多态的好处是，当我们传入<code>Dog</code>，<code>Cat</code>，<code>Tortoise</code>……时，我们只需要接受<code>Anmial</code>类型就可以了，因为<code>Dog</code>，<code>Cat</code>，<code>Tortoise</code>……都是<code>Animal</code>类型，然后，按照<code>Animal</code>类型进行操作即可。由于<code>Anmial</code>类型有<code>run()</code>方法，因此，传入的任意类型，只要是<code>Aniaml</code>类或者是子类，就会自动调用实际类型的<code>run()</code>方法，这就是多态的意思。</p>\n<p>对于一个变量，我们只需要知道它是<code>Animal</code>类型，无需确切地知道它的子类型，就可以放心地调用<code>run()</code>方法，而具体调用的<code>run()</code>方法是作用在<code>Animal</code>、<code>Dog</code>、<code>Cat</code>还是<code>Tortoise</code>对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：调用方只管调用，不管细节，而当我们新增一种<code>Animal</code>的子类时，只要确保<code>run()</code>方法编写正确，不用管原来的代码是如何调用的。这就是著名的“开闭”原则：</p>\n<p>对扩展开放：允许新增<code>Animal</code>子类；</p>\n<p>对修改封闭：不需要修改依赖<code>Animal</code>类型的<code>run_twice()</code>等函数。</p>\n<h3 id=\"获取对象信息\"><a href=\"#获取对象信息\" class=\"headerlink\" title=\"获取对象信息\"></a>获取对象信息</h3><p>当我们拿到一个对象引用时，如何知道这个对象是什么类型，有那些方法呢？</p>\n<h4 id=\"使用type\"><a href=\"#使用type\" class=\"headerlink\" title=\"使用type()\"></a>使用<code>type()</code></h4><p>我们判断对象类型，使用<code>type</code>函数，基本类型都可以用<code>type</code>判断：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">type</span>(<span class=\"number\">123</span>)</span><br><span class=\"line\">&lt;<span class=\"keyword\">class</span> <span class=\"string\">&#x27;int&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>\n<p>如果一个对象指向函数或者类，也可以用<code>type()</code>判断。但是<code>type()</code>函数返回的是什么类型呢？它返回对应的Class类型。</p>\n<p>如果要判断一个对象是否是函数怎么办？可以使用<code>types</code>模块中定义的常量：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> types</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">def</span> <span class=\"title function_\">fn</span>():</span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"keyword\">pass</span></span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">type</span>(fn)==types.FunctionType</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">type</span>(<span class=\"built_in\">abs</span>)==types.BuiltinFunctionType</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">type</span>(<span class=\"keyword\">lambda</span> x: x)==types.LambdaType</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">type</span>((x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>)))==types.GeneratorType</span><br><span class=\"line\"><span class=\"literal\">True</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"使用dir\"><a href=\"#使用dir\" class=\"headerlink\" title=\"使用dir()\"></a>使用dir()</h4><p>如果要获得一个对象的所有属性和方法，可以使用<code>dir()</code>函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">dir</span>(<span class=\"string\">&#x27;ABC&#x27;</span>)</span><br><span class=\"line\">[<span class=\"string\">&#x27;__add__&#x27;</span>, <span class=\"string\">&#x27;__class__&#x27;</span>,..., <span class=\"string\">&#x27;__subclasshook__&#x27;</span>, <span class=\"string\">&#x27;capitalize&#x27;</span>, <span class=\"string\">&#x27;casefold&#x27;</span>,..., <span class=\"string\">&#x27;zfill&#x27;</span>]</span><br></pre></td></tr></table></figure>\n<p>紧接着，可以测试对象的属性：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">hasattr</span>(obj,<span class=\"string\">&#x27;x&#x27;</span>) <span class=\"comment\">#有x属性吗？</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">setattr</span>(obj,<span class=\"string\">&#x27;y&#x27;</span>) <span class=\"comment\">#设置x属性</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">getattr</span>(obj, <span class=\"string\">&#x27;x&#x27;</span>) <span class=\"comment\"># 获取属性&#x27;x&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>可以传入一个default参数，如果属性不存在，就返回默认值：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">getattr</span>(obj, <span class=\"string\">&#x27;z&#x27;</span>, <span class=\"number\">404</span>) <span class=\"comment\"># 获取属性&#x27;z&#x27;，如果不存在，返回默认值404</span></span><br><span class=\"line\"><span class=\"number\">404</span></span><br></pre></td></tr></table></figure>\n<p>也可以获得对象的方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">hasattr</span>(obj, <span class=\"string\">&#x27;power&#x27;</span>) <span class=\"comment\"># 有属性&#x27;power&#x27;吗？</span></span><br><span class=\"line\"><span class=\"literal\">True</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">getattr</span>(obj, <span class=\"string\">&#x27;power&#x27;</span>) <span class=\"comment\"># 获取属性&#x27;power&#x27;</span></span><br><span class=\"line\">&lt;bound method MyObject.power of &lt;__main__.MyObject <span class=\"built_in\">object</span> at <span class=\"number\">0x10077a6a0</span>&gt;&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"面向对象高级编程\"><a href=\"#面向对象高级编程\" class=\"headerlink\" title=\"面向对象高级编程\"></a>面向对象高级编程</h2><h3 id=\"使用-slots\"><a href=\"#使用-slots\" class=\"headerlink\" title=\"使用__slots__\"></a>使用__slots__</h3><p>正常情况下，当我们定义了一个class，创建了一个clsaa的实例后，我们可以给该实例绑定任何实例和方法，这就是动态语言的灵活性。先定义class：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Student</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n<p>然后，尝试给实例绑定一个属性：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;s = Student</span><br><span class=\"line\">&gt;&gt;&gt;s.name = <span class=\"string\">&#x27;Michael&#x27;</span> <span class=\"comment\">#动态给实例绑定一个对象</span></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"built_in\">print</span>(<span class=\"string\">&#x27;s.name&#x27;</span>)</span><br><span class=\"line\">Micheal</span><br></pre></td></tr></table></figure>\n<p>还可以尝试给实例绑定一个方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">def</span> <span class=\"title function_\">set_age</span>(<span class=\"params\">self,age</span>): <span class=\"comment\">#定义一个函数作为实例方法</span></span><br><span class=\"line\">    \tself.age=age</span><br><span class=\"line\">    </span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"keyword\">from</span> <span class=\"built_in\">type</span> <span class=\"keyword\">import</span> MethodType</span><br><span class=\"line\">&gt;&gt;&gt;s.set_age = MethodeType(set_age, s) <span class=\"comment\">#给实例绑定一个方法</span></span><br><span class=\"line\">&gt;&gt;&gt;s.set_age(<span class=\"number\">25</span>) <span class=\"comment\">#调用实例方法</span></span><br><span class=\"line\">&gt;&gt;&gt;s.age <span class=\"comment\">#测试结果</span></span><br><span class=\"line\"><span class=\"number\">25</span></span><br></pre></td></tr></table></figure>\n<p>但是==给一个实例绑定的方法对另一个实例是不起作用的。==</p>\n<h2 id=\"进程和线程\"><a href=\"#进程和线程\" class=\"headerlink\" title=\"进程和线程\"></a>进程和线程</h2><h3 id=\"多进程\"><a href=\"#多进程\" class=\"headerlink\" title=\"多进程\"></a>多进程</h3><p>Unix/Linux操作系统提供了一个<code>fork</code>调用，普通的函数调用，调用一次返回一次，但是<code>fork</code>调用一次，返回两次，因为操作系统自动把当前进程（父进程）复制了一份（称为子进程），然后分别在父进程和子进程内返回。</p>\n<p>子进程永远返回<code>0</code>，而父进程返回子进程的ID，子进程只需要调用<code>getppid()</code>就可以拿到父进程的ID。</p>\n<p>Python的<code>os</code>模块封装了常见的系统调用，其中就包括<code>fork</code>，可以在Python程序中轻松创建子进程：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Process (%s) start...&#x27;</span> % os.getpid())</span><br><span class=\"line\"><span class=\"comment\"># Only works on Unix/Linux/Mac:</span></span><br><span class=\"line\">pid = os.fork()</span><br><span class=\"line\"><span class=\"keyword\">if</span> pid == <span class=\"number\">0</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;I am child process (%s) and my parent is %s.&#x27;</span> % (os.getpid(), os.getppid()))</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;I (%s) just created a child process (%s).&#x27;</span> % (os.getpid(), pid))</span><br></pre></td></tr></table></figure>\n<p>如果要启动大量子进程，可以用进程池的方法批量创建子进程：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> multiprocessing <span class=\"keyword\">import</span> Pool</span><br><span class=\"line\"><span class=\"keyword\">import</span> os, timme, random</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">long_time_task</span>(<span class=\"params\">name</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Run task %s (%s)...&#x27;</span> % (name, os.getpid()))</span><br><span class=\"line\">    start = time.time()</span><br><span class=\"line\">    time.sleep(random.random() * <span class=\"number\">3</span>)</span><br><span class=\"line\">    end = time.time()</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Task %s runs %0.2f seconds.&#x27;</span> % (name, (end - start)))</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Parent process %s.&#x27;</span> % os.getpid())</span><br><span class=\"line\">    p = Pool(<span class=\"number\">4</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">5</span>):</span><br><span class=\"line\">        p.apply_async(long_time_task, args=(i,))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Waiting for all subprocesses done...&#x27;</span>)</span><br><span class=\"line\">    p.close()</span><br><span class=\"line\">    p.join()</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;All subprocesses done.&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>代码解读：</p>\n<p>对<code>Pool</code>对象调用<code>join()</code>方法会等待所有子进程执行完毕，调用<code>join()</code>之前必须先调用<code>close()</code>，调用<code>close()</code>之后就不能继续添加新的<code>Process</code>了。</p>\n<p>例子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pool = Pool(<span class=\"number\">8</span>) <span class=\"comment\">#可以同时跑8个进程</span></span><br><span class=\"line\">\tpool.<span class=\"built_in\">map</span>(get_all, [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>)])</span><br><span class=\"line\">    pool.close</span><br><span class=\"line\">    pool.join()</span><br></pre></td></tr></table></figure>\n<p>这里的<code>pool.close()</code>是说关闭pool，使其不在接受新的（主进程）任务。</p>\n<p>这里的<code>pool.join()</code>是说：主进程阻塞后，让子进程继续运行完成，子进程运行完成后，再把主进程全部关掉。</p>\n<h4 id=\"子进程\"><a href=\"#子进程\" class=\"headerlink\" title=\"子进程\"></a>子进程</h4><p>很多时候，子进程并不是自生，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。</p>\n<p>==subprocess==模块可以让我们非常方便的启动一个子进程，然后控制其输入和输出。</p>\n<p>下面的例子演示了如何在Python代码中运行命令<code>nslookup www.python.org</code>，这和命令行的效果是一样的：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> subprocess</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;$ nslookup www.python.org&#x27;</span>)</span><br><span class=\"line\">r = subprocess.call([<span class=\"string\">&#x27;nslookup&#x27;</span>,<span class=\"string\">&#x27;www.python.org&#x27;</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Exit code:&#x27;</span>, r)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ nslookup www.python.org</span><br><span class=\"line\">Server:\t\t<span class=\"number\">192.168</span><span class=\"number\">.19</span><span class=\"number\">.4</span></span><br><span class=\"line\">Address:\t<span class=\"number\">192.168</span><span class=\"number\">.19</span><span class=\"number\">.4</span><span class=\"comment\">#53</span></span><br><span class=\"line\"></span><br><span class=\"line\">Non-authoritative answer:</span><br><span class=\"line\">www.python.org\tcanonical name = python.<span class=\"built_in\">map</span>.fastly.net.</span><br><span class=\"line\">Name:\tpython.<span class=\"built_in\">map</span>.fastly.net</span><br><span class=\"line\">Address: <span class=\"number\">199.27</span><span class=\"number\">.79</span><span class=\"number\">.223</span></span><br><span class=\"line\"></span><br><span class=\"line\">Exit code: <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"subprocess模块详解\"><a href=\"#subprocess模块详解\" class=\"headerlink\" title=\"subprocess模块详解\"></a>subprocess模块详解</h4><p><code>subprocess</code>模块是<strong>Python 2.4</strong>中新增的一个模块，它允许你生成新的进程，连接到它们的<strong>input/output/error</strong>管道，并获取它们的返回（状态）码。这个模块的目的在于替换几个旧的模块和方法，如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">os.system</span><br><span class=\"line\">os.spawn*</span><br></pre></td></tr></table></figure>\n<p><strong>1.subprocess模块中的常用函数</strong></p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>subprocess.run()</td>\n<td>py3.5中新增的函数。执行制定的命令，等待命令执行完毕后返回一个包含执行结果的CompletedProcess类的实例。</td>\n</tr>\n<tr>\n<td>subprocess.call()</td>\n<td>执行指定命令，返回命令执行状态，其功能类似于<code>os.system(cmd)</code></td>\n</tr>\n<tr>\n<td>subprocess.check_call()</td>\n<td>python2.5中新增的函数，执行制定的命令，如果执行成功则返回状态码，否则抛出异常。其功能等价于subprocess.run(…, check = True)</td>\n</tr>\n<tr>\n<td>subprocess.check_output()</td>\n<td>python 2.7中新增的函数。执行制定的命令，如果执行状态码为0则返回命令执行结果。都则抛出异常。</td>\n</tr>\n<tr>\n<td>subprocess.getoutput(cmd)</td>\n<td>接受字符串格式的命令，执行命令并返回执行结果，其功能类似于os.popen(cmd).reead()和commands.getoutput(cmd)</td>\n</tr>\n<tr>\n<td>subprocess.getstatusoutput(cmd)</td>\n<td>执行cmd命令，返回一个元组（命令执行状态，命令执行结果输出），其功能类似于commands,getstatusoutput()</td>\n</tr>\n</tbody>\n</table>\n</div>\n<blockquote>\n<p>说明：</p>\n<p>在Python 3.5之后的版本中，官方文档中提倡通过subprocess.run()函数替代其他函数来使用subproccess模块的功能；<br>在Python 3.5之前的版本中，我们可以通过subprocess.call()，subprocess.getoutput()等上面列出的其他函数来使用subprocess模块的功能；<br>subprocess.run()、subprocess.call()、subprocess.check_call()和subprocess.check_output()都是通过对subprocess.Popen的封装来实现的高级函数，因此如果我们需要更复杂功能时，可以通过subprocess.Popen来完成。<br>subprocess.getoutput()和subprocess.getstatusoutput()函数是来自Python 2.x的commands模块的两个遗留函数。它们隐式的调用系统shell，并且不保证其他函数所具有的安全性和异常处理的一致性。另外，它们从Python 3.3.4开始才支持Windows平台。</p>\n</blockquote>\n<p><strong>上面各函数的定义以及参数说明</strong></p>\n<p>函数参数列表</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">subprocess.run(args, *, stdin=<span class=\"literal\">None</span>, <span class=\"built_in\">input</span>=<span class=\"literal\">None</span>, stdout=<span class=\"literal\">None</span>, stderr=<span class=\"literal\">None</span>, shell=<span class=\"literal\">False</span>, timeout=<span class=\"literal\">None</span>, check=<span class=\"literal\">False</span>, universal_newlines=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">subprocess.call(args, *, stdin=<span class=\"literal\">None</span>, stdout=<span class=\"literal\">None</span>, stderr=<span class=\"literal\">None</span>, shell=<span class=\"literal\">False</span>, timeout=<span class=\"literal\">None</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">subprocess.check_call(args, *, stdin=<span class=\"literal\">None</span>, stdout=<span class=\"literal\">None</span>, stderr=<span class=\"literal\">None</span>, shell=<span class=\"literal\">False</span>, timeout=<span class=\"literal\">None</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">subprocess.check_output(args, *, stdin=<span class=\"literal\">None</span>, stderr=<span class=\"literal\">None</span>, shell=<span class=\"literal\">False</span>, universal_newlines=<span class=\"literal\">False</span>, timeout=<span class=\"literal\">None</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">subprocess.getstatusoutput(cmd)</span><br><span class=\"line\"></span><br><span class=\"line\">subprocess.getoutput(cmd)</span><br></pre></td></tr></table></figure>\n<p>参数说明：</p>\n<ul>\n<li><p>args：要执行的shell命令，默认应该是一个字符串序列，如[‘df’, ‘-Th’]，也可以是一个字符串，但要把shell的参数的值设置为True</p>\n</li>\n<li><p>shell：如果sehll为True，那么指定的命令将通过shell执行。</p>\n</li>\n<li><p>check：如果check参数的值是True，且执行命令的进程以非0状态码退出，则会抛出一个CalledProcessError的异常，且该异常会包含参数、退出状态码、以及stdout和stderr</p>\n</li>\n<li><p><code>stdout, stderr：input</code>： 该参数是传递给Popen.communicate()，通常该参数的值必须是一个字节序列，如果universal_newlines=True，则其值应该是一个字符串。</p>\n<ol>\n<li><p>run()函数默认不会捕获命令执行结果的正常输出和错误输出，如果我们向获取这些内容需要传递subprocess.PIPE，然后可以通过返回的CompletedProcess类实例的stdout和stderr属性或捕获相应的内容；</p>\n</li>\n<li><p>call()和check_call()函数返回的是命令执行的状态码，而不是CompletedProcess类实例，所以对于它们而言，stdout和stderr不适合赋值为subprocess.PIPE；<br>3</p>\n</li>\n<li>check_output()函数默认就会返回命令执行结果，所以不用设置stdout的值，如果我们希望在结果中捕获错误信息，可以执行stderr=subprocess.STDOUT。</li>\n</ol>\n</li>\n<li><p><code>universal_newlines</code>： 该参数影响的是输入与输出的数据格式，比如它的值默认为False，此时stdout和stderr的输出是字节序列；当该参数的值设置为True时，stdout和stderr的输出是字符串。</p>\n</li>\n</ul>\n<p><strong>3.subprocess.CompletedProcess类介绍</strong></p>\n<p>需要说明的是，<code>subprocess.run()</code>函数是Python3.5中新增一个高级函数，其返回值是一个<code>subprocess.CompletedProcess</code>类的实例，因此，subprocess,completedProcess类也是Python 3.5中才存在的。它表示的是一个以结束进程的状态信息。</p>\n<h4 id=\"subprocess-Popen介绍\"><a href=\"#subprocess-Popen介绍\" class=\"headerlink\" title=\"subprocess.Popen介绍\"></a>subprocess.Popen介绍</h4><p>该类用于在一个新的程序中执行一个子程序。前面我们提到过，上面介绍的这些函数艘是基于<code>subprocess.Popen</code>类实现的，通过使用这些被封装的高级函数可以很方便的完成一些常见需求。由于<code>subprocess</code>模块底层的进程创建和管理是有Popen类来处理的，因此，当我们无法通过上面哪些高级函数来实现一些不太常见的功能时就可以通过subprocess.Popen类提供灵活的api来完成。</p>\n<p>1.subprocess.Popen构造函数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">subprocess</span>.Popen(args, bufsize=-<span class=\"number\">1</span>, executable=<span class=\"literal\">None</span>, stdin=<span class=\"literal\">None</span>, stdout=<span class=\"literal\">None</span>, stderr=<span class=\"literal\">None</span>, </span><br><span class=\"line\">    preexec_fn=<span class=\"literal\">None</span>, close_fds=<span class=\"literal\">True</span>, shell=<span class=\"literal\">False</span>, cwd=<span class=\"literal\">None</span>, env=<span class=\"literal\">None</span>, universal_newlines=<span class=\"literal\">False</span>,</span><br><span class=\"line\">    startup_info=<span class=\"literal\">None</span>, creationflags=<span class=\"number\">0</span>, restore_signals=<span class=\"literal\">True</span>, start_new_session=<span class=\"literal\">False</span>, pass_fds=())</span><br></pre></td></tr></table></figure>\n<ul>\n<li>args： 要执行的shell命令，可以是字符串，也可以是命令各个参数组成的序列。当该参数的值是一个字符串时，该命令的解释过程是与平台相关的，因此通常建议将args参数作为一个序列传递。</li>\n<li>bufsize： 指定缓存策略，0表示不缓冲，1表示行缓冲，其他大于1的数字表示缓冲区大小，负数 表示使用系统默认缓冲策略。</li>\n<li>stdin, stdout, stderr： 分别表示程序标准输入、输出、错误句柄。</li>\n<li>preexec_fn： 用于指定一个将在子进程运行之前被调用的可执行对象，只在Unix平台下有效。</li>\n<li>close_fds： 如果该参数的值为True，则除了0,1和2之外的所有文件描述符都将会在子进程执行之前被关闭。</li>\n<li>shell： 该参数用于标识是否使用shell作为要执行的程序，如果shell值为True，则建议将args参数作为一个字符串传递而不要作为一个序列传递。</li>\n<li>cwd： 如果该参数值不是None，则该函数将会在执行这个子进程之前改变当前工作目录。</li>\n<li>env： 用于指定子进程的环境变量，如果env=None，那么子进程的环境变量将从父进程中继承。如果env!=None，它的值必须是一个映射对象。</li>\n<li>universal_newlines： 如果该参数值为True，则该文件对象的stdin，stdout和stderr将会作为文本流被打开，否则他们将会被作为二进制流被打开。</li>\n<li>startupinfo和creationflags： 这两个参数只在Windows下有效，它们将被传递给底层的CreateProcess()函数，用于设置子进程的一些属性，如主窗口的外观，进程优先级等。</li>\n</ul>\n<ol>\n<li>subprocess.Popen类的实例可调用的方法</li>\n</ol>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>方法</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Popen.poll()</td>\n<td>用于检查子进程（命令）是否已经执行结束，没结束返回None，结束后返回状态码。</td>\n</tr>\n<tr>\n<td>Popen.wait(timeout=None)</td>\n<td>等待子进程结束，并返回状态码；如果在timeout指定的秒数之后进程还没有结束，将会抛出一个TimeoutExpired异常。</td>\n</tr>\n<tr>\n<td>Popen.communicate(imput=None, timeout=None)</td>\n<td>该方法可用于来与程序进行交互，比如发送数据到stdin，从stdout和stderr读取数据，直到达到文件末尾。</td>\n</tr>\n<tr>\n<td>Popen.send_signal(signal)</td>\n<td>发送指定的信号给这个子进程</td>\n</tr>\n<tr>\n<td>Popen.terminate()</td>\n<td>停止这个子进程</td>\n</tr>\n<tr>\n<td>Popen.kill</td>\n<td>杀死该子进程</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h4 id=\"进程间通信\"><a href=\"#进程间通信\" class=\"headerlink\" title=\"进程间通信\"></a>进程间通信</h4><p><code>Process</code>之间肯定是需要通信的，操作系统提供了很多的机制来实现进程间的通信。Python的<code>nultiprocessing</code>模块包装了底层的机制，提供了<code>Queue</code>，<code>Pipes</code>等多种方式来交换数据。</p>\n<p>我们以<code>Queue</code>为例，在父进程中创建两个子进程，一个往<code>Queue</code>里写入数据，一个从<code>Queue</code>里读取数据：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> multiprocessing <span class=\"keyword\">import</span> Process, Queue</span><br><span class=\"line\"><span class=\"keyword\">import</span> os, time, rendom</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#写数据进程执行的代码</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">write</span>(<span class=\"params\">q</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Pricess to write: %s&#x27;</span> % os.getpid())</span><br><span class=\"line\">    <span class=\"keyword\">for</span> value <span class=\"keyword\">in</span> [<span class=\"string\">&#x27;A&#x27;</span>, <span class=\"string\">&#x27;B&#x27;</span>, <span class=\"string\">&#x27;C&#x27;</span>]:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Put %s to queue...&#x27;</span> % value)</span><br><span class=\"line\">        q.put(value)</span><br><span class=\"line\">        time.sleep(random.random())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#读数据进程执行的代码</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">read</span>(<span class=\"params\">q</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Process to readL %s&#x27;</span> % os.getpid)</span><br><span class=\"line\">\twrite <span class=\"literal\">True</span>:</span><br><span class=\"line\">        value = q.get(<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Get %s from queue.&#x27;</span> % value)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ = <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    <span class=\"comment\">#父进程创建的Queue，并传给各个子进程</span></span><br><span class=\"line\">    q = Queue()</span><br><span class=\"line\">    pw = Process(targe=write, args=(q,))</span><br><span class=\"line\">    pr = Process(target=read, args=(q,))</span><br><span class=\"line\">    <span class=\"comment\">#启动子进程pr,读取：</span></span><br><span class=\"line\">    pr.start()</span><br><span class=\"line\">    <span class=\"comment\">#等待pw结束</span></span><br><span class=\"line\">    pw.join()</span><br><span class=\"line\">    <span class=\"comment\">#pr进程里是四循环，无法等待其结束，只能强行终止：</span></span><br><span class=\"line\">    pr.terminate()</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"多线程\"><a href=\"#多线程\" class=\"headerlink\" title=\"多线程\"></a>多线程</h3><p>进程是由若干个线程组成的，一个进程至少有一个线程。</p>\n<p>由于线程的操作系统直接支持的执行单元，因此，高级语言通常都内置多线程的支持，Python也不例外，并且，Python的线程是真正的Posix Thread，而不是模拟出来的线程。</p>\n<p>Python的标准库提供了两个模块：<code>_thread</code>和<code>threading</code>，<code>_thread</code>是低级模块，<code>threading</code>是高级模块，对<code>_thread</code>进行了封装。绝大多数情况下，我们只需要使用<code>_threading</code>这个高级模块。</p>\n<p>启动一个线程就是把一个函数传入并创建<code>Thread</code>实例，然后调用<code>start()</code>开始执行：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> time, threading</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#新线程执行的代码：</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">loop</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;thread %s is runing...&#x27;</span> % threading.current_thread().name)</span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> n &lt; <span class=\"number\">5</span>:</span><br><span class=\"line\">        n = n+ <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;thread %s &gt;&gt;&gt; %s&#x27;</span> % (threading.current_thread ().name))</span><br><span class=\"line\">        time.sleep(<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;thread %s ended.&#x27;</span> % threading.current_thread().name)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;thread %s is running...&#x27;</span> % threading.current_thread().name)</span><br><span class=\"line\">t = threading.Thread(target=loop, name = <span class=\"string\">&#x27;LoopThread&#x27;</span>)</span><br><span class=\"line\">t.start()</span><br><span class=\"line\">t.join()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;thread %s ended.&#x27;</span> % threading.current_thread().name)</span><br></pre></td></tr></table></figure>\n<p>由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的<code>threading</code>模块有个<code>current_thread()</code>函数，它永远返回当前线程的实例。主线程实例的名字叫<code>MainThread</code>，子线程的名字在创建时指定，我们用<code>LoopThread</code>命名子线程。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为<code>Thread-1</code>，<code>Thread-2</code>……</p>\n<h4 id=\"Lock\"><a href=\"#Lock\" class=\"headerlink\" title=\"Lock\"></a>Lock</h4><p>多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，所有的变量都由所有的线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时修改一个变量，把内容给该乱了。</p>\n<p>所以我们希望创建一把锁，当某个程序开始执行一个线程时，我们说，该线程获得的锁，因此其他线程不能同时执行该线程，只能等待，，直到锁被释放后，获得该锁以后才能改。由于锁只有一个，无论多少个线程，同一时刻最多只有一个线程持有该锁，所以不会造成修改冲突。创建一个锁就是通过<code>threading.Lock()</code>来实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">balance = <span class=\"number\">0</span></span><br><span class=\"line\">lock = threading.Lock()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">run_thread</span>(<span class=\"params\">n</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10000</span>)</span><br><span class=\"line\">    <span class=\"comment\">#先要获取锁：</span></span><br><span class=\"line\">    lock.acquire()</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        change_it(n)</span><br><span class=\"line\">    <span class=\"keyword\">finally</span>:</span><br><span class=\"line\">        <span class=\"comment\">#改完了一定要释放锁</span></span><br><span class=\"line\">        lock.release()</span><br></pre></td></tr></table></figure>\n<p>当多个线程同时执行<code>lock.acquire()</code>时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。</p>\n<p>获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用<code>try...finally</code>来确保锁一定会被释放。</p>\n<p>锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。</p>\n<h4 id=\"多核CPU\"><a href=\"#多核CPU\" class=\"headerlink\" title=\"多核CPU\"></a>多核CPU</h4><p>如果你不幸拥有一个多核CPU，你肯定在想，多核应该可以同时执行多个线程。</p>\n<p>如果写一个死循环的话，会出现什么情况呢？</p>\n<p>打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。</p>\n<p>我们可以监控到一个死循环线程会100%占用一个CPU。</p>\n<p>如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。</p>\n<p>要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。</p>\n<p>试试用Python写个死循环：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> threading, multiprocessing</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">loop</span>():</span><br><span class=\"line\">    x = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">        x = x ^ <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(multiprocessing.cpu_count()):</span><br><span class=\"line\">    t = threading.Thread(target=loop)</span><br><span class=\"line\">    t.start()</span><br></pre></td></tr></table></figure>\n<p>启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。</p>\n<p>但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？</p>\n<p>因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。</p>\n<p>GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。</p>\n<p>所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。</p>\n<p>不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。</p>"},{"title":"softmax回归","date":"2023-08-07T15:19:01.000Z","mathjax":true,"_content":"## softmax  回归\n\nsoftmax 回归实际上是分类问题。\n\n\n\n### 预备知识\n\nsigmoid函数，softmax函数，极大似然估计，交叉熵函数，one-hot编码。\n\n<!--more-->\n\n--------\n\n### 1 sigmoid函数\n\n#### 1.1定义\n\nsigmoid函数是一个在生物学中常见的函数，也称为S型生长曲线。Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0，1之间。\n\n#### 1.2公式\n\n$$\nS(x) = \\frac{1} {1+e^{-x} } = \\frac{e^x} {e^x + 1}\n$$\n\n其对x的导数可以用自身表示：\n$$\nS^{'}(x) = \\frac{e^{-x} }{ {(1+e^{-x})}^2} = S(x)(1 - S(x))\n$$\n\n--------------------------------------\n\n\n\n\n\n\n\n### 2 softmax函数\n\n#### 2.1 定义\n\n在数学，尤其是概率论和相关领域中，Softmax 函数，或称归一化函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量z的\"压缩\"到另一个K维实向量$\\sigma(z)$中，使得每一个元素的范围都在$(0,1)$之间，并且所有的元素和为1。\n\n#### 2.2 公式\n\n$$\n\\sigma(z)_j = \\frac{e^{z_j} } {\\sum^K_{k=1} } \\quad for j=1,...,k\n$$\n\n在多项逻辑回归和线性分析中，函数的输入是从K个不同的线性函数得到的结果，而样本向量$x$属于第$j$个分类的概率为：\n$$\nP(y=j|x) = \\frac{e^{x^1w_j} } {\\sum^K_{k=1} }e^{x^Tw_k}\n$$\n这可以被视作K个线性函数$x- > x^T w_1,..., x^T w_k $ Softmax函数的复合。\n\n-------------------------------\n\n\n\n### 3.  极大似然估计\n\n#### 3.1 似然函数\n\n相信大家已经掌握条件概率函数。那么我们假设一个条件概率函数为$P(\\theta_k|x)$，其中，$\\theta_k$已知，我们希望通过已知的$\\theta_k$求出未知的变量$x$。\n\n那么应该如何理解似然函数呢？\n\n我们先给出似然函数的定义式：\n$$\nL(\\theta|x) = P(x|\\theta)\n$$\n\n\n这时$x$是已知的，而$\\theta$作为模型参数是未知的。\n\n#### 3.2 最大似然函数\n\n最大似然函数的思想在于，对于给定的观测数据$x$，我们希望从所有$\\theta_1,\\theta_2,...,\\theta_n$中找出能最大化概率函数的参数$\\theta_x$即可：\n$$\nL(\\theta_x|x)=P(x|\\theta_x) >= L(\\theta|x)=P(x|\\theta) \\quad \\theta=\\theta_1,...\\theta_n\n$$\n那么在实际运算中，我们将代估计的参数$\\theta$看成是变量，通过$\\theta$变量计算出概率函数$P(x|\\theta)$，并找到能使得概率函数取得最大化的参数($\\theta$)即可。\n$$\n\\theta_x = arg \\mathop{max}_{\\theta} p(x|\\theta)\n$$\n这一步骤通过求导数得到导数为0来解。\n\n#### 3. 3 离散随机变量的最大似然估计\n\n离散型随机变量$X$的分布律为$P\\{A=x\\}=p(x;\\theta)$，$A_1,A_2,...,A_n$为来自$A$的样本，$x_1,x_2,...,x_n$为样本对应的观测值，$\\theta$为待估计参数。\n\n在参数$\\theta$下，分布函数随机取到$x_1,...,x_n$的概率是\n$$\nP(x|\\theta) = \\prod \\limits_{i=1}^N P(x_i;\\theta)\n$$\n我们的目标是找到使$P(x|\\theta)$最大化的参数$\\theta$。\n\n求解最大值，通常是求导等于0：\n$$\n\\frac{d}{d\\theta} L(\\theta|x) = 0\n$$\n 由于$L(\\theta|x)$通常是由累乘的形式，我们借助对数函数来简化问题：\n$$\n\\frac{d}{d \\theta} L(\\theta|x) = 0\n$$\n上式也通常被称作**对数似然方程**。如果$\\theta$包含多个$\\theta_1,...\\theta_k$可以对每个$\\theta$求偏导来连立方程组。\n\n#### 3.4 连续型随机变量的最大似然估计\n\n连续型随机变量$X$的概率密度为$f(x;\\theta)$，设$X_1,...,X_n$为样本，相应的观察值为$x_1,x_2,...,x_n$。\n\n与离散型随机变量类似，构造似然函数为：\n$$\nL(x|\\theta) = p(x;\\theta) = \\prod \\limits_{i=1}^n f(x_i;\\theta)dx\n$$\n由于$\\prod limits_{i=1}^n dx$不随参数变化，故我们选择忽略，似然函数变为：\n$$\nL(x|\\theta) = \\prod \\limits_{i=1}^N P(x_i;\\theta)\n$$\n\n$$\n\n$$\n\n\n\n-------------------------------------------------------------\n\n### 4 交叉熵函数\n\n#### 4.1 熵\n\n信息论中熵的概念首次被香农提出，目的是寻找一种高效/无损地编码信息的方法：以编码后数据的平均长度来衡量高效性，平均长度越小越高效；同时还需满足“无损”的条件，即编码后不能有原始信息的丢失。这样，香农提出了熵的定义：无损编码事件信息的最小平均编码长度。\n\n**直接计算熵**\n\n假设一个事件有八中可能性，且各个状态等可能性，即可能性都是12.5%，也就是1/8，那么我们需要多少为来编码这八个值呢？答案是$2^3$，也就是三位。我们不能减少任何1位，因为那样会造成歧义，同样我们也不要多于3位来编码8个可能的值。用归纳法来看假设有N种状态，每种状态是等可能的，那么每种状态的可能性为$P=\\frac{1}{N}$，那么我们用以下公式计算编码位数：\n$$\nlog_2 N = - log_2 \\frac{1} {N} = -log_2P\n$$\n那么计算平均最小长度\n$$\nEntropy =- \\sum_i P(i) log_2 P(i)\n$$\n其中P(i)是第i个信息状态的可能性。相当于，熵=编码长度*可能性。\n\n如果熵比较大(即平均编码长度较长)，意味着这一信息有较多的可能状态，相应的每个状态的可能性比较低；因此每当来了一个新的信息，我们很难对其作出准确预测，即有着比较大的混乱程度/不确定性/不可预测性。\n\n\n\n#### 4.2 交叉熵\n\n对于离散型随机变量，熵的公式可以表示为\n$$\n-\\sum_i P(i)log_2P(i)\n$$\n对于连续型随机变量，熵的公式可以表示为：\n$$\n- \\int P(x) log_2 P(x) dx\n$$\n\n\n那么我们现在有真实的概率分布$P$，以及预测的概率分布$Q$。\n\nj假设计算离散型变量的交叉熵，在计算交叉熵时，我们可以采用以下公式，即使用$P$计算平均编码长度，使用$Q$计算实际编码长度：\n$$\nCrossEntropy = - \\sum_i P(x_i) logQ(x_i)\n$$\n\n\n假设计算连续型变量的交叉熵，在计算交叉熵时，我们可以采用以下公式，即使用$P$计算平均编码长度，使用$Q$计算实际编码长度：\n$$\nCrossEntorpy = - \\int P(x)logQ(x)dx\n$$\n\n\n\n\n\n\n--------------------------\n\n\n\n### 5 softmax 回归\n\n使用热独编码，分量和类别一样多。类别对应的分量设置为1，其他所有分量设置为0。假设标签$y$是一个三维向量其中$(1,0,0)$对应猫，$(0,1,0)$对应鸡，$(0,0,1)$对应于狗。我们需要和输出一样多的仿射函数。\n$$\n\\begin{array}{1}\no_1 = x_1 \\omega_11 + x_2 \\omega_12 + x_3 \\omega_13 + x_4 \\omega_14 + b_1 \\\\\no_2 = x_1 \\omega_21 + x_2 \\omega_22 + x_3 \\omega_23 + x_4 \\omega_24 + b_2 \\\\\no_3 = x_3 \\omega_31 + x_2 \\omega_32 + x_3 \\omega_33 + x_4 \\omega_34 + b_3 \\\\\n\\end{array}\n\\tag{1}\n$$\n为了更简洁表述，我们用向量形式来描述$o = Wx + b$。\n\n现在我们将优化参数以最大化观测数据的概率。我们希望模型的输出$y_j$可以视为属于$j$类的概率，然后选择具有最大输出值的类别$argmax_j y_j$作为我们的预测。例如，$y_1,y_2,y_3$分别是$0.1,0.8,0.1$那么我们可以判断其类别为”鸡“。\n\n要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。此外我们需要训练一个目标函数来激励模型精准的估计概率。\n\n由于softmax函数能够将为规范化的预测变为非负数且总和为1，同时让模型保持可导的性质。我们可以将输出$o$输入进softmax函数：\n$$\n\\hat{y} = softmax(o) \\quad 其中 \\quad \\hat{y_j} = \\frac{e^{o_j} } {\\sum_k e^{k}} = \\frac{e^{o_j} } {e^1 + e^2 + ... + e^k} \\tag{2}\n$$\n在预测过程中，我们仍然可以用下式来选择最有可能的类别。\n$$\n\\mathop{argmax}\\limits_j \\quad \\hat{y_j} = \\mathop{argmax} \\limits_j \\quad o_j \\tag{3}\n$$\n\n#### 损失函数\n\n假设给出数据集${X,Y}$有$n$个样本，其中索引$i$的样本有特征向量$x^{(i)}$和独热标签向量$y^{(i)}$组成。我们可以将估计值和真实值进行比较：\n$$\nP(Y|X) = \\prod \\limits_{i=1}^n P(y_i|x_i) \\tag{4}\n$$\n根据最大似然估计，我们最大化$P(Y|X)$，相当于最小化负对数似然：\n$$\n- logP(Y|X) = \\sum \\limits_{i=1}^n - log P(y^{(i)}|x^{(i)}) = \\sum \\limits_{i=1}^{n} l(y^{(i)},\\hat{y^{(i)} }) \\tag{5}\n$$\n对于任何标签$y$和预测模型$\\hat{y}$，损失函数为：\n$$\nl(y,\\hat{y}) = -\\sum \\limits_{i=1}^q y_i log \\hat{y_i} \\tag{6}\n$$\n我们可以将$(2)$带入$(6)$中去，利用softmax定义我们得到：\n$$\n\\begin{array}\n{1} l(y,\\hat{y}) = - \\sum \\limits^q_{j = 1} y_i log\\frac{exp(o_j)}{\\sum_{k=1}^q exp(o_k)}\n\\\\\n=\\sum_{j=1}^q y_i log \\sum_{k=1}^q exp(o_k) - \\sum \\limits^q_{j=1}y_i o_j\n\\\\\n=log \\sum \\limits_{k=1}^q exp(o_k) - \\sum_{j=1}^q y_j  o_j\n\\end{array}\n$$\n对$o_j$求偏导，我们得到：\n$$\n\\partial_{o_j} l(y,\\hat{y}) = \\frac{exp(o_j)}{\\sum\\limits_{k=1}^q exp(o_k)} -y_i = softmax(o)_j - y_i \\tag{7}\n$$\n","source":"_posts/softmax回归.md","raw":"---\ntitle: softmax回归\ndate: 2023-08-07 23:19:01\ntags:\ncategories:\n- deep learning\nmathjax: true\n---\n## softmax  回归\n\nsoftmax 回归实际上是分类问题。\n\n\n\n### 预备知识\n\nsigmoid函数，softmax函数，极大似然估计，交叉熵函数，one-hot编码。\n\n<!--more-->\n\n--------\n\n### 1 sigmoid函数\n\n#### 1.1定义\n\nsigmoid函数是一个在生物学中常见的函数，也称为S型生长曲线。Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0，1之间。\n\n#### 1.2公式\n\n$$\nS(x) = \\frac{1} {1+e^{-x} } = \\frac{e^x} {e^x + 1}\n$$\n\n其对x的导数可以用自身表示：\n$$\nS^{'}(x) = \\frac{e^{-x} }{ {(1+e^{-x})}^2} = S(x)(1 - S(x))\n$$\n\n--------------------------------------\n\n\n\n\n\n\n\n### 2 softmax函数\n\n#### 2.1 定义\n\n在数学，尤其是概率论和相关领域中，Softmax 函数，或称归一化函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量z的\"压缩\"到另一个K维实向量$\\sigma(z)$中，使得每一个元素的范围都在$(0,1)$之间，并且所有的元素和为1。\n\n#### 2.2 公式\n\n$$\n\\sigma(z)_j = \\frac{e^{z_j} } {\\sum^K_{k=1} } \\quad for j=1,...,k\n$$\n\n在多项逻辑回归和线性分析中，函数的输入是从K个不同的线性函数得到的结果，而样本向量$x$属于第$j$个分类的概率为：\n$$\nP(y=j|x) = \\frac{e^{x^1w_j} } {\\sum^K_{k=1} }e^{x^Tw_k}\n$$\n这可以被视作K个线性函数$x- > x^T w_1,..., x^T w_k $ Softmax函数的复合。\n\n-------------------------------\n\n\n\n### 3.  极大似然估计\n\n#### 3.1 似然函数\n\n相信大家已经掌握条件概率函数。那么我们假设一个条件概率函数为$P(\\theta_k|x)$，其中，$\\theta_k$已知，我们希望通过已知的$\\theta_k$求出未知的变量$x$。\n\n那么应该如何理解似然函数呢？\n\n我们先给出似然函数的定义式：\n$$\nL(\\theta|x) = P(x|\\theta)\n$$\n\n\n这时$x$是已知的，而$\\theta$作为模型参数是未知的。\n\n#### 3.2 最大似然函数\n\n最大似然函数的思想在于，对于给定的观测数据$x$，我们希望从所有$\\theta_1,\\theta_2,...,\\theta_n$中找出能最大化概率函数的参数$\\theta_x$即可：\n$$\nL(\\theta_x|x)=P(x|\\theta_x) >= L(\\theta|x)=P(x|\\theta) \\quad \\theta=\\theta_1,...\\theta_n\n$$\n那么在实际运算中，我们将代估计的参数$\\theta$看成是变量，通过$\\theta$变量计算出概率函数$P(x|\\theta)$，并找到能使得概率函数取得最大化的参数($\\theta$)即可。\n$$\n\\theta_x = arg \\mathop{max}_{\\theta} p(x|\\theta)\n$$\n这一步骤通过求导数得到导数为0来解。\n\n#### 3. 3 离散随机变量的最大似然估计\n\n离散型随机变量$X$的分布律为$P\\{A=x\\}=p(x;\\theta)$，$A_1,A_2,...,A_n$为来自$A$的样本，$x_1,x_2,...,x_n$为样本对应的观测值，$\\theta$为待估计参数。\n\n在参数$\\theta$下，分布函数随机取到$x_1,...,x_n$的概率是\n$$\nP(x|\\theta) = \\prod \\limits_{i=1}^N P(x_i;\\theta)\n$$\n我们的目标是找到使$P(x|\\theta)$最大化的参数$\\theta$。\n\n求解最大值，通常是求导等于0：\n$$\n\\frac{d}{d\\theta} L(\\theta|x) = 0\n$$\n 由于$L(\\theta|x)$通常是由累乘的形式，我们借助对数函数来简化问题：\n$$\n\\frac{d}{d \\theta} L(\\theta|x) = 0\n$$\n上式也通常被称作**对数似然方程**。如果$\\theta$包含多个$\\theta_1,...\\theta_k$可以对每个$\\theta$求偏导来连立方程组。\n\n#### 3.4 连续型随机变量的最大似然估计\n\n连续型随机变量$X$的概率密度为$f(x;\\theta)$，设$X_1,...,X_n$为样本，相应的观察值为$x_1,x_2,...,x_n$。\n\n与离散型随机变量类似，构造似然函数为：\n$$\nL(x|\\theta) = p(x;\\theta) = \\prod \\limits_{i=1}^n f(x_i;\\theta)dx\n$$\n由于$\\prod limits_{i=1}^n dx$不随参数变化，故我们选择忽略，似然函数变为：\n$$\nL(x|\\theta) = \\prod \\limits_{i=1}^N P(x_i;\\theta)\n$$\n\n$$\n\n$$\n\n\n\n-------------------------------------------------------------\n\n### 4 交叉熵函数\n\n#### 4.1 熵\n\n信息论中熵的概念首次被香农提出，目的是寻找一种高效/无损地编码信息的方法：以编码后数据的平均长度来衡量高效性，平均长度越小越高效；同时还需满足“无损”的条件，即编码后不能有原始信息的丢失。这样，香农提出了熵的定义：无损编码事件信息的最小平均编码长度。\n\n**直接计算熵**\n\n假设一个事件有八中可能性，且各个状态等可能性，即可能性都是12.5%，也就是1/8，那么我们需要多少为来编码这八个值呢？答案是$2^3$，也就是三位。我们不能减少任何1位，因为那样会造成歧义，同样我们也不要多于3位来编码8个可能的值。用归纳法来看假设有N种状态，每种状态是等可能的，那么每种状态的可能性为$P=\\frac{1}{N}$，那么我们用以下公式计算编码位数：\n$$\nlog_2 N = - log_2 \\frac{1} {N} = -log_2P\n$$\n那么计算平均最小长度\n$$\nEntropy =- \\sum_i P(i) log_2 P(i)\n$$\n其中P(i)是第i个信息状态的可能性。相当于，熵=编码长度*可能性。\n\n如果熵比较大(即平均编码长度较长)，意味着这一信息有较多的可能状态，相应的每个状态的可能性比较低；因此每当来了一个新的信息，我们很难对其作出准确预测，即有着比较大的混乱程度/不确定性/不可预测性。\n\n\n\n#### 4.2 交叉熵\n\n对于离散型随机变量，熵的公式可以表示为\n$$\n-\\sum_i P(i)log_2P(i)\n$$\n对于连续型随机变量，熵的公式可以表示为：\n$$\n- \\int P(x) log_2 P(x) dx\n$$\n\n\n那么我们现在有真实的概率分布$P$，以及预测的概率分布$Q$。\n\nj假设计算离散型变量的交叉熵，在计算交叉熵时，我们可以采用以下公式，即使用$P$计算平均编码长度，使用$Q$计算实际编码长度：\n$$\nCrossEntropy = - \\sum_i P(x_i) logQ(x_i)\n$$\n\n\n假设计算连续型变量的交叉熵，在计算交叉熵时，我们可以采用以下公式，即使用$P$计算平均编码长度，使用$Q$计算实际编码长度：\n$$\nCrossEntorpy = - \\int P(x)logQ(x)dx\n$$\n\n\n\n\n\n\n--------------------------\n\n\n\n### 5 softmax 回归\n\n使用热独编码，分量和类别一样多。类别对应的分量设置为1，其他所有分量设置为0。假设标签$y$是一个三维向量其中$(1,0,0)$对应猫，$(0,1,0)$对应鸡，$(0,0,1)$对应于狗。我们需要和输出一样多的仿射函数。\n$$\n\\begin{array}{1}\no_1 = x_1 \\omega_11 + x_2 \\omega_12 + x_3 \\omega_13 + x_4 \\omega_14 + b_1 \\\\\no_2 = x_1 \\omega_21 + x_2 \\omega_22 + x_3 \\omega_23 + x_4 \\omega_24 + b_2 \\\\\no_3 = x_3 \\omega_31 + x_2 \\omega_32 + x_3 \\omega_33 + x_4 \\omega_34 + b_3 \\\\\n\\end{array}\n\\tag{1}\n$$\n为了更简洁表述，我们用向量形式来描述$o = Wx + b$。\n\n现在我们将优化参数以最大化观测数据的概率。我们希望模型的输出$y_j$可以视为属于$j$类的概率，然后选择具有最大输出值的类别$argmax_j y_j$作为我们的预测。例如，$y_1,y_2,y_3$分别是$0.1,0.8,0.1$那么我们可以判断其类别为”鸡“。\n\n要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。此外我们需要训练一个目标函数来激励模型精准的估计概率。\n\n由于softmax函数能够将为规范化的预测变为非负数且总和为1，同时让模型保持可导的性质。我们可以将输出$o$输入进softmax函数：\n$$\n\\hat{y} = softmax(o) \\quad 其中 \\quad \\hat{y_j} = \\frac{e^{o_j} } {\\sum_k e^{k}} = \\frac{e^{o_j} } {e^1 + e^2 + ... + e^k} \\tag{2}\n$$\n在预测过程中，我们仍然可以用下式来选择最有可能的类别。\n$$\n\\mathop{argmax}\\limits_j \\quad \\hat{y_j} = \\mathop{argmax} \\limits_j \\quad o_j \\tag{3}\n$$\n\n#### 损失函数\n\n假设给出数据集${X,Y}$有$n$个样本，其中索引$i$的样本有特征向量$x^{(i)}$和独热标签向量$y^{(i)}$组成。我们可以将估计值和真实值进行比较：\n$$\nP(Y|X) = \\prod \\limits_{i=1}^n P(y_i|x_i) \\tag{4}\n$$\n根据最大似然估计，我们最大化$P(Y|X)$，相当于最小化负对数似然：\n$$\n- logP(Y|X) = \\sum \\limits_{i=1}^n - log P(y^{(i)}|x^{(i)}) = \\sum \\limits_{i=1}^{n} l(y^{(i)},\\hat{y^{(i)} }) \\tag{5}\n$$\n对于任何标签$y$和预测模型$\\hat{y}$，损失函数为：\n$$\nl(y,\\hat{y}) = -\\sum \\limits_{i=1}^q y_i log \\hat{y_i} \\tag{6}\n$$\n我们可以将$(2)$带入$(6)$中去，利用softmax定义我们得到：\n$$\n\\begin{array}\n{1} l(y,\\hat{y}) = - \\sum \\limits^q_{j = 1} y_i log\\frac{exp(o_j)}{\\sum_{k=1}^q exp(o_k)}\n\\\\\n=\\sum_{j=1}^q y_i log \\sum_{k=1}^q exp(o_k) - \\sum \\limits^q_{j=1}y_i o_j\n\\\\\n=log \\sum \\limits_{k=1}^q exp(o_k) - \\sum_{j=1}^q y_j  o_j\n\\end{array}\n$$\n对$o_j$求偏导，我们得到：\n$$\n\\partial_{o_j} l(y,\\hat{y}) = \\frac{exp(o_j)}{\\sum\\limits_{k=1}^q exp(o_k)} -y_i = softmax(o)_j - y_i \\tag{7}\n$$\n","slug":"softmax回归","published":1,"updated":"2023-09-15T07:13:19.298Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clmnlyz6r000amyqbccaeg5fd","content":"<h2 id=\"softmax-回归\"><a href=\"#softmax-回归\" class=\"headerlink\" title=\"softmax  回归\"></a>softmax  回归</h2><p>softmax 回归实际上是分类问题。</p>\n<h3 id=\"预备知识\"><a href=\"#预备知识\" class=\"headerlink\" title=\"预备知识\"></a>预备知识</h3><p>sigmoid函数，softmax函数，极大似然估计，交叉熵函数，one-hot编码。</p>\n<span id=\"more\"></span>\n<hr>\n<h3 id=\"1-sigmoid函数\"><a href=\"#1-sigmoid函数\" class=\"headerlink\" title=\"1 sigmoid函数\"></a>1 sigmoid函数</h3><h4 id=\"1-1定义\"><a href=\"#1-1定义\" class=\"headerlink\" title=\"1.1定义\"></a>1.1定义</h4><p>sigmoid函数是一个在生物学中常见的函数，也称为S型生长曲线。Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0，1之间。</p>\n<h4 id=\"1-2公式\"><a href=\"#1-2公式\" class=\"headerlink\" title=\"1.2公式\"></a>1.2公式</h4><script type=\"math/tex; mode=display\">\nS(x) = \\frac{1} {1+e^{-x} } = \\frac{e^x} {e^x + 1}</script><p>其对x的导数可以用自身表示：</p>\n<script type=\"math/tex; mode=display\">\nS^{'}(x) = \\frac{e^{-x} }{ {(1+e^{-x})}^2} = S(x)(1 - S(x))</script><hr>\n<h3 id=\"2-softmax函数\"><a href=\"#2-softmax函数\" class=\"headerlink\" title=\"2 softmax函数\"></a>2 softmax函数</h3><h4 id=\"2-1-定义\"><a href=\"#2-1-定义\" class=\"headerlink\" title=\"2.1 定义\"></a>2.1 定义</h4><p>在数学，尤其是概率论和相关领域中，Softmax 函数，或称归一化函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量z的”压缩”到另一个K维实向量$\\sigma(z)$中，使得每一个元素的范围都在$(0,1)$之间，并且所有的元素和为1。</p>\n<h4 id=\"2-2-公式\"><a href=\"#2-2-公式\" class=\"headerlink\" title=\"2.2 公式\"></a>2.2 公式</h4><script type=\"math/tex; mode=display\">\n\\sigma(z)_j = \\frac{e^{z_j} } {\\sum^K_{k=1} } \\quad for j=1,...,k</script><p>在多项逻辑回归和线性分析中，函数的输入是从K个不同的线性函数得到的结果，而样本向量$x$属于第$j$个分类的概率为：</p>\n<script type=\"math/tex; mode=display\">\nP(y=j|x) = \\frac{e^{x^1w_j} } {\\sum^K_{k=1} }e^{x^Tw_k}</script><p>这可以被视作K个线性函数$x- &gt; x^T w_1,…, x^T w_k $ Softmax函数的复合。</p>\n<hr>\n<h3 id=\"3-极大似然估计\"><a href=\"#3-极大似然估计\" class=\"headerlink\" title=\"3.  极大似然估计\"></a>3.  极大似然估计</h3><h4 id=\"3-1-似然函数\"><a href=\"#3-1-似然函数\" class=\"headerlink\" title=\"3.1 似然函数\"></a>3.1 似然函数</h4><p>相信大家已经掌握条件概率函数。那么我们假设一个条件概率函数为$P(\\theta_k|x)$，其中，$\\theta_k$已知，我们希望通过已知的$\\theta_k$求出未知的变量$x$。</p>\n<p>那么应该如何理解似然函数呢？</p>\n<p>我们先给出似然函数的定义式：</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta|x) = P(x|\\theta)</script><p>这时$x$是已知的，而$\\theta$作为模型参数是未知的。</p>\n<h4 id=\"3-2-最大似然函数\"><a href=\"#3-2-最大似然函数\" class=\"headerlink\" title=\"3.2 最大似然函数\"></a>3.2 最大似然函数</h4><p>最大似然函数的思想在于，对于给定的观测数据$x$，我们希望从所有$\\theta_1,\\theta_2,…,\\theta_n$中找出能最大化概率函数的参数$\\theta_x$即可：</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta_x|x)=P(x|\\theta_x) >= L(\\theta|x)=P(x|\\theta) \\quad \\theta=\\theta_1,...\\theta_n</script><p>那么在实际运算中，我们将代估计的参数$\\theta$看成是变量，通过$\\theta$变量计算出概率函数$P(x|\\theta)$，并找到能使得概率函数取得最大化的参数($\\theta$)即可。</p>\n<script type=\"math/tex; mode=display\">\n\\theta_x = arg \\mathop{max}_{\\theta} p(x|\\theta)</script><p>这一步骤通过求导数得到导数为0来解。</p>\n<h4 id=\"3-3-离散随机变量的最大似然估计\"><a href=\"#3-3-离散随机变量的最大似然估计\" class=\"headerlink\" title=\"3. 3 离散随机变量的最大似然估计\"></a>3. 3 离散随机变量的最大似然估计</h4><p>离散型随机变量$X$的分布律为$P\\{A=x\\}=p(x;\\theta)$，$A_1,A_2,…,A_n$为来自$A$的样本，$x_1,x_2,…,x_n$为样本对应的观测值，$\\theta$为待估计参数。</p>\n<p>在参数$\\theta$下，分布函数随机取到$x_1,…,x_n$的概率是</p>\n<script type=\"math/tex; mode=display\">\nP(x|\\theta) = \\prod \\limits_{i=1}^N P(x_i;\\theta)</script><p>我们的目标是找到使$P(x|\\theta)$最大化的参数$\\theta$。</p>\n<p>求解最大值，通常是求导等于0：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{d}{d\\theta} L(\\theta|x) = 0</script><p> 由于$L(\\theta|x)$通常是由累乘的形式，我们借助对数函数来简化问题：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{d}{d \\theta} L(\\theta|x) = 0</script><p>上式也通常被称作<strong>对数似然方程</strong>。如果$\\theta$包含多个$\\theta_1,…\\theta_k$可以对每个$\\theta$求偏导来连立方程组。</p>\n<h4 id=\"3-4-连续型随机变量的最大似然估计\"><a href=\"#3-4-连续型随机变量的最大似然估计\" class=\"headerlink\" title=\"3.4 连续型随机变量的最大似然估计\"></a>3.4 连续型随机变量的最大似然估计</h4><p>连续型随机变量$X$的概率密度为$f(x;\\theta)$，设$X_1,…,X_n$为样本，相应的观察值为$x_1,x_2,…,x_n$。</p>\n<p>与离散型随机变量类似，构造似然函数为：</p>\n<script type=\"math/tex; mode=display\">\nL(x|\\theta) = p(x;\\theta) = \\prod \\limits_{i=1}^n f(x_i;\\theta)dx</script><p>由于$\\prod limits_{i=1}^n dx$不随参数变化，故我们选择忽略，似然函数变为：</p>\n<script type=\"math/tex; mode=display\">\nL(x|\\theta) = \\prod \\limits_{i=1}^N P(x_i;\\theta)</script><script type=\"math/tex; mode=display\">\n</script><hr>\n<h3 id=\"4-交叉熵函数\"><a href=\"#4-交叉熵函数\" class=\"headerlink\" title=\"4 交叉熵函数\"></a>4 交叉熵函数</h3><h4 id=\"4-1-熵\"><a href=\"#4-1-熵\" class=\"headerlink\" title=\"4.1 熵\"></a>4.1 熵</h4><p>信息论中熵的概念首次被香农提出，目的是寻找一种高效/无损地编码信息的方法：以编码后数据的平均长度来衡量高效性，平均长度越小越高效；同时还需满足“无损”的条件，即编码后不能有原始信息的丢失。这样，香农提出了熵的定义：无损编码事件信息的最小平均编码长度。</p>\n<p><strong>直接计算熵</strong></p>\n<p>假设一个事件有八中可能性，且各个状态等可能性，即可能性都是12.5%，也就是1/8，那么我们需要多少为来编码这八个值呢？答案是$2^3$，也就是三位。我们不能减少任何1位，因为那样会造成歧义，同样我们也不要多于3位来编码8个可能的值。用归纳法来看假设有N种状态，每种状态是等可能的，那么每种状态的可能性为$P=\\frac{1}{N}$，那么我们用以下公式计算编码位数：</p>\n<script type=\"math/tex; mode=display\">\nlog_2 N = - log_2 \\frac{1} {N} = -log_2P</script><p>那么计算平均最小长度</p>\n<script type=\"math/tex; mode=display\">\nEntropy =- \\sum_i P(i) log_2 P(i)</script><p>其中P(i)是第i个信息状态的可能性。相当于，熵=编码长度*可能性。</p>\n<p>如果熵比较大(即平均编码长度较长)，意味着这一信息有较多的可能状态，相应的每个状态的可能性比较低；因此每当来了一个新的信息，我们很难对其作出准确预测，即有着比较大的混乱程度/不确定性/不可预测性。</p>\n<h4 id=\"4-2-交叉熵\"><a href=\"#4-2-交叉熵\" class=\"headerlink\" title=\"4.2 交叉熵\"></a>4.2 交叉熵</h4><p>对于离散型随机变量，熵的公式可以表示为</p>\n<script type=\"math/tex; mode=display\">\n-\\sum_i P(i)log_2P(i)</script><p>对于连续型随机变量，熵的公式可以表示为：</p>\n<script type=\"math/tex; mode=display\">\n- \\int P(x) log_2 P(x) dx</script><p>那么我们现在有真实的概率分布$P$，以及预测的概率分布$Q$。</p>\n<p>j假设计算离散型变量的交叉熵，在计算交叉熵时，我们可以采用以下公式，即使用$P$计算平均编码长度，使用$Q$计算实际编码长度：</p>\n<script type=\"math/tex; mode=display\">\nCrossEntropy = - \\sum_i P(x_i) logQ(x_i)</script><p>假设计算连续型变量的交叉熵，在计算交叉熵时，我们可以采用以下公式，即使用$P$计算平均编码长度，使用$Q$计算实际编码长度：</p>\n<script type=\"math/tex; mode=display\">\nCrossEntorpy = - \\int P(x)logQ(x)dx</script><hr>\n<h3 id=\"5-softmax-回归\"><a href=\"#5-softmax-回归\" class=\"headerlink\" title=\"5 softmax 回归\"></a>5 softmax 回归</h3><p>使用热独编码，分量和类别一样多。类别对应的分量设置为1，其他所有分量设置为0。假设标签$y$是一个三维向量其中$(1,0,0)$对应猫，$(0,1,0)$对应鸡，$(0,0,1)$对应于狗。我们需要和输出一样多的仿射函数。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{1}\no_1 = x_1 \\omega_11 + x_2 \\omega_12 + x_3 \\omega_13 + x_4 \\omega_14 + b_1 \\\\\no_2 = x_1 \\omega_21 + x_2 \\omega_22 + x_3 \\omega_23 + x_4 \\omega_24 + b_2 \\\\\no_3 = x_3 \\omega_31 + x_2 \\omega_32 + x_3 \\omega_33 + x_4 \\omega_34 + b_3 \\\\\n\\end{array}\n\\tag{1}</script><p>为了更简洁表述，我们用向量形式来描述$o = Wx + b$。</p>\n<p>现在我们将优化参数以最大化观测数据的概率。我们希望模型的输出$y_j$可以视为属于$j$类的概率，然后选择具有最大输出值的类别$argmax_j y_j$作为我们的预测。例如，$y_1,y_2,y_3$分别是$0.1,0.8,0.1$那么我们可以判断其类别为”鸡“。</p>\n<p>要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。此外我们需要训练一个目标函数来激励模型精准的估计概率。</p>\n<p>由于softmax函数能够将为规范化的预测变为非负数且总和为1，同时让模型保持可导的性质。我们可以将输出$o$输入进softmax函数：</p>\n<script type=\"math/tex; mode=display\">\n\\hat{y} = softmax(o) \\quad 其中 \\quad \\hat{y_j} = \\frac{e^{o_j} } {\\sum_k e^{k}} = \\frac{e^{o_j} } {e^1 + e^2 + ... + e^k} \\tag{2}</script><p>在预测过程中，我们仍然可以用下式来选择最有可能的类别。</p>\n<script type=\"math/tex; mode=display\">\n\\mathop{argmax}\\limits_j \\quad \\hat{y_j} = \\mathop{argmax} \\limits_j \\quad o_j \\tag{3}</script><h4 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h4><p>假设给出数据集${X,Y}$有$n$个样本，其中索引$i$的样本有特征向量$x^{(i)}$和独热标签向量$y^{(i)}$组成。我们可以将估计值和真实值进行比较：</p>\n<script type=\"math/tex; mode=display\">\nP(Y|X) = \\prod \\limits_{i=1}^n P(y_i|x_i) \\tag{4}</script><p>根据最大似然估计，我们最大化$P(Y|X)$，相当于最小化负对数似然：</p>\n<script type=\"math/tex; mode=display\">\n- logP(Y|X) = \\sum \\limits_{i=1}^n - log P(y^{(i)}|x^{(i)}) = \\sum \\limits_{i=1}^{n} l(y^{(i)},\\hat{y^{(i)} }) \\tag{5}</script><p>对于任何标签$y$和预测模型$\\hat{y}$，损失函数为：</p>\n<script type=\"math/tex; mode=display\">\nl(y,\\hat{y}) = -\\sum \\limits_{i=1}^q y_i log \\hat{y_i} \\tag{6}</script><p>我们可以将$(2)$带入$(6)$中去，利用softmax定义我们得到：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}\n{1} l(y,\\hat{y}) = - \\sum \\limits^q_{j = 1} y_i log\\frac{exp(o_j)}{\\sum_{k=1}^q exp(o_k)}\n\\\\\n=\\sum_{j=1}^q y_i log \\sum_{k=1}^q exp(o_k) - \\sum \\limits^q_{j=1}y_i o_j\n\\\\\n=log \\sum \\limits_{k=1}^q exp(o_k) - \\sum_{j=1}^q y_j  o_j\n\\end{array}</script><p>对$o_j$求偏导，我们得到：</p>\n<script type=\"math/tex; mode=display\">\n\\partial_{o_j} l(y,\\hat{y}) = \\frac{exp(o_j)}{\\sum\\limits_{k=1}^q exp(o_k)} -y_i = softmax(o)_j - y_i \\tag{7}</script>","site":{"data":{}},"excerpt":"<h2 id=\"softmax-回归\"><a href=\"#softmax-回归\" class=\"headerlink\" title=\"softmax  回归\"></a>softmax  回归</h2><p>softmax 回归实际上是分类问题。</p>\n<h3 id=\"预备知识\"><a href=\"#预备知识\" class=\"headerlink\" title=\"预备知识\"></a>预备知识</h3><p>sigmoid函数，softmax函数，极大似然估计，交叉熵函数，one-hot编码。</p>","more":"<hr>\n<h3 id=\"1-sigmoid函数\"><a href=\"#1-sigmoid函数\" class=\"headerlink\" title=\"1 sigmoid函数\"></a>1 sigmoid函数</h3><h4 id=\"1-1定义\"><a href=\"#1-1定义\" class=\"headerlink\" title=\"1.1定义\"></a>1.1定义</h4><p>sigmoid函数是一个在生物学中常见的函数，也称为S型生长曲线。Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0，1之间。</p>\n<h4 id=\"1-2公式\"><a href=\"#1-2公式\" class=\"headerlink\" title=\"1.2公式\"></a>1.2公式</h4><script type=\"math/tex; mode=display\">\nS(x) = \\frac{1} {1+e^{-x} } = \\frac{e^x} {e^x + 1}</script><p>其对x的导数可以用自身表示：</p>\n<script type=\"math/tex; mode=display\">\nS^{'}(x) = \\frac{e^{-x} }{ {(1+e^{-x})}^2} = S(x)(1 - S(x))</script><hr>\n<h3 id=\"2-softmax函数\"><a href=\"#2-softmax函数\" class=\"headerlink\" title=\"2 softmax函数\"></a>2 softmax函数</h3><h4 id=\"2-1-定义\"><a href=\"#2-1-定义\" class=\"headerlink\" title=\"2.1 定义\"></a>2.1 定义</h4><p>在数学，尤其是概率论和相关领域中，Softmax 函数，或称归一化函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量z的”压缩”到另一个K维实向量$\\sigma(z)$中，使得每一个元素的范围都在$(0,1)$之间，并且所有的元素和为1。</p>\n<h4 id=\"2-2-公式\"><a href=\"#2-2-公式\" class=\"headerlink\" title=\"2.2 公式\"></a>2.2 公式</h4><script type=\"math/tex; mode=display\">\n\\sigma(z)_j = \\frac{e^{z_j} } {\\sum^K_{k=1} } \\quad for j=1,...,k</script><p>在多项逻辑回归和线性分析中，函数的输入是从K个不同的线性函数得到的结果，而样本向量$x$属于第$j$个分类的概率为：</p>\n<script type=\"math/tex; mode=display\">\nP(y=j|x) = \\frac{e^{x^1w_j} } {\\sum^K_{k=1} }e^{x^Tw_k}</script><p>这可以被视作K个线性函数$x- &gt; x^T w_1,…, x^T w_k $ Softmax函数的复合。</p>\n<hr>\n<h3 id=\"3-极大似然估计\"><a href=\"#3-极大似然估计\" class=\"headerlink\" title=\"3.  极大似然估计\"></a>3.  极大似然估计</h3><h4 id=\"3-1-似然函数\"><a href=\"#3-1-似然函数\" class=\"headerlink\" title=\"3.1 似然函数\"></a>3.1 似然函数</h4><p>相信大家已经掌握条件概率函数。那么我们假设一个条件概率函数为$P(\\theta_k|x)$，其中，$\\theta_k$已知，我们希望通过已知的$\\theta_k$求出未知的变量$x$。</p>\n<p>那么应该如何理解似然函数呢？</p>\n<p>我们先给出似然函数的定义式：</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta|x) = P(x|\\theta)</script><p>这时$x$是已知的，而$\\theta$作为模型参数是未知的。</p>\n<h4 id=\"3-2-最大似然函数\"><a href=\"#3-2-最大似然函数\" class=\"headerlink\" title=\"3.2 最大似然函数\"></a>3.2 最大似然函数</h4><p>最大似然函数的思想在于，对于给定的观测数据$x$，我们希望从所有$\\theta_1,\\theta_2,…,\\theta_n$中找出能最大化概率函数的参数$\\theta_x$即可：</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta_x|x)=P(x|\\theta_x) >= L(\\theta|x)=P(x|\\theta) \\quad \\theta=\\theta_1,...\\theta_n</script><p>那么在实际运算中，我们将代估计的参数$\\theta$看成是变量，通过$\\theta$变量计算出概率函数$P(x|\\theta)$，并找到能使得概率函数取得最大化的参数($\\theta$)即可。</p>\n<script type=\"math/tex; mode=display\">\n\\theta_x = arg \\mathop{max}_{\\theta} p(x|\\theta)</script><p>这一步骤通过求导数得到导数为0来解。</p>\n<h4 id=\"3-3-离散随机变量的最大似然估计\"><a href=\"#3-3-离散随机变量的最大似然估计\" class=\"headerlink\" title=\"3. 3 离散随机变量的最大似然估计\"></a>3. 3 离散随机变量的最大似然估计</h4><p>离散型随机变量$X$的分布律为$P\\{A=x\\}=p(x;\\theta)$，$A_1,A_2,…,A_n$为来自$A$的样本，$x_1,x_2,…,x_n$为样本对应的观测值，$\\theta$为待估计参数。</p>\n<p>在参数$\\theta$下，分布函数随机取到$x_1,…,x_n$的概率是</p>\n<script type=\"math/tex; mode=display\">\nP(x|\\theta) = \\prod \\limits_{i=1}^N P(x_i;\\theta)</script><p>我们的目标是找到使$P(x|\\theta)$最大化的参数$\\theta$。</p>\n<p>求解最大值，通常是求导等于0：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{d}{d\\theta} L(\\theta|x) = 0</script><p> 由于$L(\\theta|x)$通常是由累乘的形式，我们借助对数函数来简化问题：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{d}{d \\theta} L(\\theta|x) = 0</script><p>上式也通常被称作<strong>对数似然方程</strong>。如果$\\theta$包含多个$\\theta_1,…\\theta_k$可以对每个$\\theta$求偏导来连立方程组。</p>\n<h4 id=\"3-4-连续型随机变量的最大似然估计\"><a href=\"#3-4-连续型随机变量的最大似然估计\" class=\"headerlink\" title=\"3.4 连续型随机变量的最大似然估计\"></a>3.4 连续型随机变量的最大似然估计</h4><p>连续型随机变量$X$的概率密度为$f(x;\\theta)$，设$X_1,…,X_n$为样本，相应的观察值为$x_1,x_2,…,x_n$。</p>\n<p>与离散型随机变量类似，构造似然函数为：</p>\n<script type=\"math/tex; mode=display\">\nL(x|\\theta) = p(x;\\theta) = \\prod \\limits_{i=1}^n f(x_i;\\theta)dx</script><p>由于$\\prod limits_{i=1}^n dx$不随参数变化，故我们选择忽略，似然函数变为：</p>\n<script type=\"math/tex; mode=display\">\nL(x|\\theta) = \\prod \\limits_{i=1}^N P(x_i;\\theta)</script><script type=\"math/tex; mode=display\">\n</script><hr>\n<h3 id=\"4-交叉熵函数\"><a href=\"#4-交叉熵函数\" class=\"headerlink\" title=\"4 交叉熵函数\"></a>4 交叉熵函数</h3><h4 id=\"4-1-熵\"><a href=\"#4-1-熵\" class=\"headerlink\" title=\"4.1 熵\"></a>4.1 熵</h4><p>信息论中熵的概念首次被香农提出，目的是寻找一种高效/无损地编码信息的方法：以编码后数据的平均长度来衡量高效性，平均长度越小越高效；同时还需满足“无损”的条件，即编码后不能有原始信息的丢失。这样，香农提出了熵的定义：无损编码事件信息的最小平均编码长度。</p>\n<p><strong>直接计算熵</strong></p>\n<p>假设一个事件有八中可能性，且各个状态等可能性，即可能性都是12.5%，也就是1/8，那么我们需要多少为来编码这八个值呢？答案是$2^3$，也就是三位。我们不能减少任何1位，因为那样会造成歧义，同样我们也不要多于3位来编码8个可能的值。用归纳法来看假设有N种状态，每种状态是等可能的，那么每种状态的可能性为$P=\\frac{1}{N}$，那么我们用以下公式计算编码位数：</p>\n<script type=\"math/tex; mode=display\">\nlog_2 N = - log_2 \\frac{1} {N} = -log_2P</script><p>那么计算平均最小长度</p>\n<script type=\"math/tex; mode=display\">\nEntropy =- \\sum_i P(i) log_2 P(i)</script><p>其中P(i)是第i个信息状态的可能性。相当于，熵=编码长度*可能性。</p>\n<p>如果熵比较大(即平均编码长度较长)，意味着这一信息有较多的可能状态，相应的每个状态的可能性比较低；因此每当来了一个新的信息，我们很难对其作出准确预测，即有着比较大的混乱程度/不确定性/不可预测性。</p>\n<h4 id=\"4-2-交叉熵\"><a href=\"#4-2-交叉熵\" class=\"headerlink\" title=\"4.2 交叉熵\"></a>4.2 交叉熵</h4><p>对于离散型随机变量，熵的公式可以表示为</p>\n<script type=\"math/tex; mode=display\">\n-\\sum_i P(i)log_2P(i)</script><p>对于连续型随机变量，熵的公式可以表示为：</p>\n<script type=\"math/tex; mode=display\">\n- \\int P(x) log_2 P(x) dx</script><p>那么我们现在有真实的概率分布$P$，以及预测的概率分布$Q$。</p>\n<p>j假设计算离散型变量的交叉熵，在计算交叉熵时，我们可以采用以下公式，即使用$P$计算平均编码长度，使用$Q$计算实际编码长度：</p>\n<script type=\"math/tex; mode=display\">\nCrossEntropy = - \\sum_i P(x_i) logQ(x_i)</script><p>假设计算连续型变量的交叉熵，在计算交叉熵时，我们可以采用以下公式，即使用$P$计算平均编码长度，使用$Q$计算实际编码长度：</p>\n<script type=\"math/tex; mode=display\">\nCrossEntorpy = - \\int P(x)logQ(x)dx</script><hr>\n<h3 id=\"5-softmax-回归\"><a href=\"#5-softmax-回归\" class=\"headerlink\" title=\"5 softmax 回归\"></a>5 softmax 回归</h3><p>使用热独编码，分量和类别一样多。类别对应的分量设置为1，其他所有分量设置为0。假设标签$y$是一个三维向量其中$(1,0,0)$对应猫，$(0,1,0)$对应鸡，$(0,0,1)$对应于狗。我们需要和输出一样多的仿射函数。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}{1}\no_1 = x_1 \\omega_11 + x_2 \\omega_12 + x_3 \\omega_13 + x_4 \\omega_14 + b_1 \\\\\no_2 = x_1 \\omega_21 + x_2 \\omega_22 + x_3 \\omega_23 + x_4 \\omega_24 + b_2 \\\\\no_3 = x_3 \\omega_31 + x_2 \\omega_32 + x_3 \\omega_33 + x_4 \\omega_34 + b_3 \\\\\n\\end{array}\n\\tag{1}</script><p>为了更简洁表述，我们用向量形式来描述$o = Wx + b$。</p>\n<p>现在我们将优化参数以最大化观测数据的概率。我们希望模型的输出$y_j$可以视为属于$j$类的概率，然后选择具有最大输出值的类别$argmax_j y_j$作为我们的预测。例如，$y_1,y_2,y_3$分别是$0.1,0.8,0.1$那么我们可以判断其类别为”鸡“。</p>\n<p>要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。此外我们需要训练一个目标函数来激励模型精准的估计概率。</p>\n<p>由于softmax函数能够将为规范化的预测变为非负数且总和为1，同时让模型保持可导的性质。我们可以将输出$o$输入进softmax函数：</p>\n<script type=\"math/tex; mode=display\">\n\\hat{y} = softmax(o) \\quad 其中 \\quad \\hat{y_j} = \\frac{e^{o_j} } {\\sum_k e^{k}} = \\frac{e^{o_j} } {e^1 + e^2 + ... + e^k} \\tag{2}</script><p>在预测过程中，我们仍然可以用下式来选择最有可能的类别。</p>\n<script type=\"math/tex; mode=display\">\n\\mathop{argmax}\\limits_j \\quad \\hat{y_j} = \\mathop{argmax} \\limits_j \\quad o_j \\tag{3}</script><h4 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h4><p>假设给出数据集${X,Y}$有$n$个样本，其中索引$i$的样本有特征向量$x^{(i)}$和独热标签向量$y^{(i)}$组成。我们可以将估计值和真实值进行比较：</p>\n<script type=\"math/tex; mode=display\">\nP(Y|X) = \\prod \\limits_{i=1}^n P(y_i|x_i) \\tag{4}</script><p>根据最大似然估计，我们最大化$P(Y|X)$，相当于最小化负对数似然：</p>\n<script type=\"math/tex; mode=display\">\n- logP(Y|X) = \\sum \\limits_{i=1}^n - log P(y^{(i)}|x^{(i)}) = \\sum \\limits_{i=1}^{n} l(y^{(i)},\\hat{y^{(i)} }) \\tag{5}</script><p>对于任何标签$y$和预测模型$\\hat{y}$，损失函数为：</p>\n<script type=\"math/tex; mode=display\">\nl(y,\\hat{y}) = -\\sum \\limits_{i=1}^q y_i log \\hat{y_i} \\tag{6}</script><p>我们可以将$(2)$带入$(6)$中去，利用softmax定义我们得到：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{array}\n{1} l(y,\\hat{y}) = - \\sum \\limits^q_{j = 1} y_i log\\frac{exp(o_j)}{\\sum_{k=1}^q exp(o_k)}\n\\\\\n=\\sum_{j=1}^q y_i log \\sum_{k=1}^q exp(o_k) - \\sum \\limits^q_{j=1}y_i o_j\n\\\\\n=log \\sum \\limits_{k=1}^q exp(o_k) - \\sum_{j=1}^q y_j  o_j\n\\end{array}</script><p>对$o_j$求偏导，我们得到：</p>\n<script type=\"math/tex; mode=display\">\n\\partial_{o_j} l(y,\\hat{y}) = \\frac{exp(o_j)}{\\sum\\limits_{k=1}^q exp(o_k)} -y_i = softmax(o)_j - y_i \\tag{7}</script>"}],"PostAsset":[],"PostCategory":[{"post_id":"clmnlyz6m0001myqbffcif0wn","category_id":"clmnlyz6o0004myqbcrno9q5t","_id":"clmnlyz6s000dmyqb107s37gy"},{"post_id":"clmnlyz6n0003myqb9tgkg2bc","category_id":"clmnlyz6r000bmyqbar1317uo","_id":"clmnlyz6s000gmyqbbzj298tq"},{"post_id":"clmnlyz6p0006myqb2t36fa66","category_id":"clmnlyz6s000emyqb72hc0f8k","_id":"clmnlyz6s000imyqbhzcg628z"},{"post_id":"clmnlyz6q0008myqbds0b87kl","category_id":"clmnlyz6s000hmyqbglgg6mpe","_id":"clmnlyz6t000kmyqbabh05nk0"},{"post_id":"clmnlyz6r000amyqbccaeg5fd","category_id":"clmnlyz6r000bmyqbar1317uo","_id":"clmnlyz6t000lmyqbf3hu7a3r"}],"PostTag":[{"post_id":"clmnlyz6q0008myqbds0b87kl","tag_id":"clmnlyz6r000cmyqbesfhbi1u","_id":"clmnlyz6s000fmyqbed1jhes4"}],"Tag":[{"name":"python","_id":"clmnlyz6r000cmyqbesfhbi1u"}]}}