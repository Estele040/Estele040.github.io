---
title: softmax回归
date: 2023-08-07 23:19:01
tags:
categories:
- deep learning
mathjax: true
---
## softmax  回归

softmax 回归实际上是分类问题。



### 预备知识

sigmoid函数，softmax函数，极大似然估计，交叉熵函数，one-hot编码。

<!--more-->

--------

### 1 sigmoid函数

#### 1.1定义

sigmoid函数是一个在生物学中常见的函数，也称为S型生长曲线。Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0，1之间。

#### 1.2公式

$$
S(x) = \frac{1} {1+e^{-x} } = \frac{e^x} {e^x + 1}
$$

其对x的导数可以用自身表示：
$$
S^{'}(x) = \frac{e^{-x} }{ {(1+e^{-x})}^2} = S(x)(1 - S(x))
$$

--------------------------------------







### 2 softmax函数

#### 2.1 定义

在数学，尤其是概率论和相关领域中，Softmax 函数，或称归一化函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量z的"压缩"到另一个K维实向量$\sigma(z)$中，使得每一个元素的范围都在$(0,1)$之间，并且所有的元素和为1。

#### 2.2 公式

$$
\sigma(z)_j = \frac{e^{z_j} } {\sum^K_{k=1} } \quad for j=1,...,k
$$

在多项逻辑回归和线性分析中，函数的输入是从K个不同的线性函数得到的结果，而样本向量$x$属于第$j$个分类的概率为：
$$
P(y=j|x) = \frac{e^{x^1w_j} } {\sum^K_{k=1} }e^{x^Tw_k}
$$
这可以被视作K个线性函数$x- > x^T w_1,..., x^T w_k $ Softmax函数的复合。

-------------------------------



### 3.  极大似然估计

#### 3.1 似然函数

相信大家已经掌握条件概率函数。那么我们假设一个条件概率函数为$P(\theta_k|x)$，其中，$\theta_k$已知，我们希望通过已知的$\theta_k$求出未知的变量$x$。

那么应该如何理解似然函数呢？

我们先给出似然函数的定义式：
$$
L(\theta|x) = P(x|\theta)
$$


这时$x$是已知的，而$\theta$作为模型参数是未知的。

#### 3.2 最大似然函数

最大似然函数的思想在于，对于给定的观测数据$x$，我们希望从所有$\theta_1,\theta_2,...,\theta_n$中找出能最大化概率函数的参数$\theta_x$即可：
$$
L(\theta_x|x)=P(x|\theta_x) >= L(\theta|x)=P(x|\theta) \quad \theta=\theta_1,...\theta_n
$$
那么在实际运算中，我们将代估计的参数$\theta$看成是变量，通过$\theta$变量计算出概率函数$P(x|\theta)$，并找到能使得概率函数取得最大化的参数($\theta$)即可。
$$
\theta_x = arg \mathop{max}_{\theta} p(x|\theta)
$$
这一步骤通过求导数得到导数为0来解。

#### 3. 3 离散随机变量的最大似然估计

离散型随机变量$X$的分布律为$P\{A=x\}=p(x;\theta)$，$A_1,A_2,...,A_n$为来自$A$的样本，$x_1,x_2,...,x_n$为样本对应的观测值，$\theta$为待估计参数。

在参数$\theta$下，分布函数随机取到$x_1,...,x_n$的概率是
$$
P(x|\theta) = \prod \limits_{i=1}^N P(x_i;\theta)
$$
我们的目标是找到使$P(x|\theta)$最大化的参数$\theta$。

求解最大值，通常是求导等于0：
$$
\frac{d}{d\theta} L(\theta|x) = 0
$$
 由于$L(\theta|x)$通常是由累乘的形式，我们借助对数函数来简化问题：
$$
\frac{d}{d \theta} L(\theta|x) = 0
$$
上式也通常被称作**对数似然方程**。如果$\theta$包含多个$\theta_1,...\theta_k$可以对每个$\theta$求偏导来连立方程组。

#### 3.4 连续型随机变量的最大似然估计

连续型随机变量$X$的概率密度为$f(x;\theta)$，设$X_1,...,X_n$为样本，相应的观察值为$x_1,x_2,...,x_n$。

与离散型随机变量类似，构造似然函数为：
$$
L(x|\theta) = p(x;\theta) = \prod \limits_{i=1}^n f(x_i;\theta)dx
$$
由于$\prod limits_{i=1}^n dx$不随参数变化，故我们选择忽略，似然函数变为：
$$
L(x|\theta) = \prod \limits_{i=1}^N P(x_i;\theta)
$$

$$

$$



-------------------------------------------------------------

### 4 交叉熵函数

#### 4.1 熵

信息论中熵的概念首次被香农提出，目的是寻找一种高效/无损地编码信息的方法：以编码后数据的平均长度来衡量高效性，平均长度越小越高效；同时还需满足“无损”的条件，即编码后不能有原始信息的丢失。这样，香农提出了熵的定义：无损编码事件信息的最小平均编码长度。

**直接计算熵**

假设一个事件有八中可能性，且各个状态等可能性，即可能性都是12.5%，也就是1/8，那么我们需要多少为来编码这八个值呢？答案是$2^3$，也就是三位。我们不能减少任何1位，因为那样会造成歧义，同样我们也不要多于3位来编码8个可能的值。用归纳法来看假设有N种状态，每种状态是等可能的，那么每种状态的可能性为$P=\frac{1}{N}$，那么我们用以下公式计算编码位数：
$$
log_2 N = - log_2 \frac{1} {N} = -log_2P
$$
那么计算平均最小长度
$$
Entropy =- \sum_i P(i) log_2 P(i)
$$
其中P(i)是第i个信息状态的可能性。相当于，熵=编码长度*可能性。

如果熵比较大(即平均编码长度较长)，意味着这一信息有较多的可能状态，相应的每个状态的可能性比较低；因此每当来了一个新的信息，我们很难对其作出准确预测，即有着比较大的混乱程度/不确定性/不可预测性。



#### 4.2 交叉熵

对于离散型随机变量，熵的公式可以表示为
$$
-\sum_i P(i)log_2P(i)
$$
对于连续型随机变量，熵的公式可以表示为：
$$
- \int P(x) log_2 P(x) dx
$$


那么我们现在有真实的概率分布$P$，以及预测的概率分布$Q$。

j假设计算离散型变量的交叉熵，在计算交叉熵时，我们可以采用以下公式，即使用$P$计算平均编码长度，使用$Q$计算实际编码长度：
$$
CrossEntropy = - \sum_i P(x_i) logQ(x_i)
$$


假设计算连续型变量的交叉熵，在计算交叉熵时，我们可以采用以下公式，即使用$P$计算平均编码长度，使用$Q$计算实际编码长度：
$$
CrossEntorpy = - \int P(x)logQ(x)dx
$$






--------------------------



### 5 softmax 回归

使用热独编码，分量和类别一样多。类别对应的分量设置为1，其他所有分量设置为0。假设标签$y$是一个三维向量其中$(1,0,0)$对应猫，$(0,1,0)$对应鸡，$(0,0,1)$对应于狗。我们需要和输出一样多的仿射函数。
$$
\begin{array}{1}
o_1 = x_1 \omega_11 + x_2 \omega_12 + x_3 \omega_13 + x_4 \omega_14 + b_1 \\
o_2 = x_1 \omega_21 + x_2 \omega_22 + x_3 \omega_23 + x_4 \omega_24 + b_2 \\
o_3 = x_3 \omega_31 + x_2 \omega_32 + x_3 \omega_33 + x_4 \omega_34 + b_3 \\
\end{array}
\tag{1}
$$
为了更简洁表述，我们用向量形式来描述$o = Wx + b$。

现在我们将优化参数以最大化观测数据的概率。我们希望模型的输出$y_j$可以视为属于$j$类的概率，然后选择具有最大输出值的类别$argmax_j y_j$作为我们的预测。例如，$y_1,y_2,y_3$分别是$0.1,0.8,0.1$那么我们可以判断其类别为”鸡“。

要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。此外我们需要训练一个目标函数来激励模型精准的估计概率。

由于softmax函数能够将为规范化的预测变为非负数且总和为1，同时让模型保持可导的性质。我们可以将输出$o$输入进softmax函数：
$$
\hat{y} = softmax(o) \quad 其中 \quad \hat{y_j} = \frac{e^{o_j} } {\sum_k e^{k}} = \frac{e^{o_j} } {e^1 + e^2 + ... + e^k} \tag{2}
$$
在预测过程中，我们仍然可以用下式来选择最有可能的类别。
$$
\mathop{argmax}\limits_j \quad \hat{y_j} = \mathop{argmax} \limits_j \quad o_j \tag{3}
$$

#### 损失函数

假设给出数据集${X,Y}$有$n$个样本，其中索引$i$的样本有特征向量$x^{(i)}$和独热标签向量$y^{(i)}$组成。我们可以将估计值和真实值进行比较：
$$
P(Y|X) = \prod \limits_{i=1}^n P(y_i|x_i) \tag{4}
$$
根据最大似然估计，我们最大化$P(Y|X)$，相当于最小化负对数似然：
$$
- logP(Y|X) = \sum \limits_{i=1}^n - log P(y^{(i)}|x^{(i)}) = \sum \limits_{i=1}^{n} l(y^{(i)},\hat{y^{(i)} }) \tag{5}
$$
对于任何标签$y$和预测模型$\hat{y}$，损失函数为：
$$
l(y,\hat{y}) = -\sum \limits_{i=1}^q y_i log \hat{y_i} \tag{6}
$$
我们可以将$(2)$带入$(6)$中去，利用softmax定义我们得到：
$$
\begin{array}
{1} l(y,\hat{y}) = - \sum \limits^q_{j = 1} y_i log\frac{exp(o_j)}{\sum_{k=1}^q exp(o_k)}
\\
=\sum_{j=1}^q y_i log \sum_{k=1}^q exp(o_k) - \sum \limits^q_{j=1}y_i o_j
\\
=log \sum \limits_{k=1}^q exp(o_k) - \sum_{j=1}^q y_j  o_j
\end{array}
$$
对$o_j$求偏导，我们得到：
$$
\partial_{o_j} l(y,\hat{y}) = \frac{exp(o_j)}{\sum\limits_{k=1}^q exp(o_k)} -y_i = softmax(o)_j - y_i \tag{7}
$$
